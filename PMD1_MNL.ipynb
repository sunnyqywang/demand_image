{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dc2bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"models/\")\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from setup import *\n",
    "from dataloader import SurveyDataset\n",
    "import mnl\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a011f12",
   "metadata": {},
   "source": [
    "# Load Model Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ab8e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'SAE'\n",
    "load_model_name = 'Autoencoder'\n",
    "load_model_file = 'sae'\n",
    "zoomlevel = 'zoom13'\n",
    "output_dim = 1\n",
    "model_run_date = '22020901'\n",
    "model_code = 'M1_D1'\n",
    "\n",
    "variable_names = ['active','auto','mas','pt', 'trpgen']\n",
    "\n",
    "demo_variables = ['tot_population','pct25_34yrs','pct35_50yrs','pctover65yrs',\n",
    "         'pctwhite_alone','pct_nonwhite','pctblack_alone',\n",
    "         'pct_col_grad','avg_tt_to_work','inc_per_capita']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eec4bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proj_dir+\"latent_space/\"+model_type+\"_\"+zoomlevel+\"_\"+str(output_dim**2*2048)+\"_\"+\n",
    "                       model_run_date+\".pkl\", \"rb\") as f:\n",
    "    encoder_output = pkl.load(f)\n",
    "    im = pkl.load(f)\n",
    "    ct = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af7a682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Embeddings\n",
    "unique_ct = list(set(ct))\n",
    "unique_ct.sort()\n",
    "ct = np.array(ct)\n",
    "aggregate_embeddings = []\n",
    "for i in unique_ct:\n",
    "    aggregate_embeddings.append(np.mean(encoder_output[ct == i], axis=0))\n",
    "aggregate_embeddings = np.array(aggregate_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31691fe",
   "metadata": {},
   "source": [
    "# Load Trip Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cedeaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = pd.read_csv(data_dir+\"trips.csv\")\n",
    "n_alts = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9133c15f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    0.713060\n",
      "1    0.132001\n",
      "4    0.111893\n",
      "3    0.043046\n",
      "Name: mode, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(tp['mode'].value_counts()/len(tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed76f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp['tract_1'] = tp['state_fips_1'].astype(str) + '_' + tp['county_fips_1'].astype(str)+ '_' + tp['tract_fips_1'].astype(str)\n",
    "tp['tract_2'] = tp['state_fips_2'].astype(str) + '_' + tp['county_fips_2'].astype(str)+ '_' + tp['tract_fips_2'].astype(str)\n",
    "\n",
    "tp['morning'] = (tp['dep_hour'] > 6) & (tp['dep_hour'] < 10)\n",
    "tp['afternoon'] = (tp['dep_hour'] > 15) & (tp['dep_hour'] < 19)\n",
    "tp['morning'] = tp['morning'].astype(int)\n",
    "tp['afternoon'] = tp['afternoon'].astype(int)\n",
    "\n",
    "tp['const'] = 1\n",
    "\n",
    "def normalize_features(df, cols):\n",
    "    for c in cols:\n",
    "        df[c] = df[c]/df[c].max()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0165e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ct = np.array(unique_ct)\n",
    "\n",
    "x_embed = []\n",
    "trip_filter = []\n",
    "for t1, t2 in zip(tp['tract_1'], tp['tract_2']):\n",
    "    if sum(unique_ct == t1) == 1 and sum(unique_ct == t2) == 1:\n",
    "        x_embed.append(np.hstack((aggregate_embeddings[unique_ct == t1], aggregate_embeddings[unique_ct == t2])).flatten())\n",
    "        trip_filter.append(True)\n",
    "    else:\n",
    "        trip_filter.append(False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9657533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1337"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57141937",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_filter = np.array(trip_filter)\n",
    "x_embed = np.array(x_embed)\n",
    "x_trip = tp[['morning','afternoon','companion', 'distance', \n",
    "         'from_home', 'to_home', 'purp_work', 'purp_school', 'purp_errand', 'purp_recreation', \n",
    "         'ontime_important', '12_18yrs', '18_25yrs', '25_55yrs', '55+yrs', 'no_age', \n",
    "         'disability', 'educ_col', 'educ_grad', \n",
    "         'race_white', 'race_black', 'race_asian', \n",
    "         'male', 'female', \n",
    "         'emply_park', 'emply_transit', 'emply_veh', 'emply_wfh', 'emply_flex', 'emply_hours', \n",
    "         'license', 'person_trips', 'person_transit', 'person_freq_transit', \n",
    "         'hh_inc_0_30', 'hh_inc_30_60', 'hh_inc_60_100', 'hh_inc_100_150', 'hh_inc_150', \n",
    "         'avg_pr_veh', 'home_own', 'home_house', 'home_condo']].to_numpy()[trip_filter]\n",
    "\n",
    "x = np.concatenate([x_trip, x_embed], axis=1)\n",
    "\n",
    "y = tp['mode'].astype(int).to_numpy() - 1\n",
    "y = y[trip_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9301b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "267ed279",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = SurveyDataset(torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "trainloader = DataLoader(trainset, batch_size=256, shuffle=True)\n",
    "\n",
    "testset = SurveyDataset(torch.tensor(x_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "testloader = DataLoader(testset, batch_size=len(testset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0132c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75248, 4139)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b4c4444",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:   0] Train loss: 0.4810 accuracy: 0.831\n",
      "[epoch:   0] Test loss: 0.3669 accuracy: 0.876\n",
      "[epoch:   3] Train loss: 0.3494 accuracy: 0.879\n",
      "[epoch:   3] Test loss: 0.3405 accuracy: 0.884\n",
      "[epoch:   6] Train loss: 0.3362 accuracy: 0.883\n",
      "[epoch:   6] Test loss: 0.3310 accuracy: 0.887\n",
      "[epoch:   9] Train loss: 0.3287 accuracy: 0.885\n",
      "[epoch:   9] Test loss: 0.3192 accuracy: 0.890\n",
      "[epoch:  12] Train loss: 0.3306 accuracy: 0.883\n",
      "[epoch:  12] Test loss: 0.3419 accuracy: 0.884\n",
      "[epoch:  15] Train loss: 0.3263 accuracy: 0.886\n",
      "[epoch:  15] Test loss: 0.3193 accuracy: 0.889\n",
      "[epoch:  18] Train loss: 0.3245 accuracy: 0.886\n",
      "[epoch:  18] Test loss: 0.3318 accuracy: 0.886\n",
      "[epoch:  21] Train loss: 0.3198 accuracy: 0.888\n",
      "[epoch:  21] Test loss: 0.3394 accuracy: 0.883\n",
      "[epoch:  24] Train loss: 0.3153 accuracy: 0.889\n",
      "[epoch:  24] Test loss: 0.3150 accuracy: 0.891\n",
      "[epoch:  27] Train loss: 0.3223 accuracy: 0.887\n",
      "[epoch:  27] Test loss: 0.3301 accuracy: 0.885\n",
      "Diverging. stop.\n",
      "[epoch:   0] Train loss: 0.4715 accuracy: 0.836\n",
      "[epoch:   0] Test loss: 0.3770 accuracy: 0.872\n",
      "[epoch:   3] Train loss: 0.3536 accuracy: 0.877\n",
      "[epoch:   3] Test loss: 0.3410 accuracy: 0.882\n",
      "[epoch:   6] Train loss: 0.3505 accuracy: 0.876\n",
      "[epoch:   6] Test loss: 0.3568 accuracy: 0.880\n",
      "[epoch:   9] Train loss: 0.3411 accuracy: 0.880\n",
      "[epoch:   9] Test loss: 0.3352 accuracy: 0.885\n",
      "[epoch:  12] Train loss: 0.3417 accuracy: 0.880\n",
      "[epoch:  12] Test loss: 0.3305 accuracy: 0.886\n",
      "[epoch:  15] Train loss: 0.3372 accuracy: 0.881\n",
      "[epoch:  15] Test loss: 0.3418 accuracy: 0.882\n",
      "[epoch:  18] Train loss: 0.3330 accuracy: 0.883\n",
      "[epoch:  18] Test loss: 0.3347 accuracy: 0.886\n",
      "[epoch:  21] Train loss: 0.3344 accuracy: 0.883\n",
      "[epoch:  21] Test loss: 0.3348 accuracy: 0.884\n",
      "Diverging. stop.\n",
      "[epoch:   0] Train loss: 0.4925 accuracy: 0.831\n",
      "[epoch:   0] Test loss: 0.4214 accuracy: 0.848\n",
      "[epoch:   3] Train loss: 0.3807 accuracy: 0.866\n",
      "[epoch:   3] Test loss: 0.3666 accuracy: 0.872\n",
      "[epoch:   6] Train loss: 0.3665 accuracy: 0.872\n",
      "[epoch:   6] Test loss: 0.3688 accuracy: 0.872\n",
      "[epoch:   9] Train loss: 0.3663 accuracy: 0.873\n",
      "[epoch:   9] Test loss: 0.3661 accuracy: 0.872\n",
      "[epoch:  12] Train loss: 0.3648 accuracy: 0.873\n",
      "[epoch:  12] Test loss: 0.3596 accuracy: 0.875\n",
      "[epoch:  15] Train loss: 0.3672 accuracy: 0.872\n",
      "[epoch:  15] Test loss: 0.3574 accuracy: 0.875\n",
      "[epoch:  18] Train loss: 0.3598 accuracy: 0.874\n",
      "[epoch:  18] Test loss: 0.3543 accuracy: 0.877\n",
      "[epoch:  21] Train loss: 0.3772 accuracy: 0.869\n",
      "[epoch:  21] Test loss: 0.3628 accuracy: 0.874\n",
      "Diverging. stop.\n",
      "[epoch:   0] Train loss: 0.4646 accuracy: 0.838\n",
      "[epoch:   0] Test loss: 0.4461 accuracy: 0.846\n",
      "[epoch:   3] Train loss: 0.3581 accuracy: 0.877\n",
      "[epoch:   3] Test loss: 0.3434 accuracy: 0.881\n",
      "[epoch:   6] Train loss: 0.3513 accuracy: 0.878\n",
      "[epoch:   6] Test loss: 0.3306 accuracy: 0.886\n",
      "[epoch:   9] Train loss: 0.3435 accuracy: 0.880\n",
      "[epoch:   9] Test loss: 0.3183 accuracy: 0.887\n",
      "[epoch:  12] Train loss: 0.3361 accuracy: 0.882\n",
      "[epoch:  12] Test loss: 0.3331 accuracy: 0.882\n",
      "[epoch:  15] Train loss: 0.3361 accuracy: 0.884\n",
      "[epoch:  15] Test loss: 0.3255 accuracy: 0.887\n",
      "[epoch:  18] Train loss: 0.3397 accuracy: 0.882\n",
      "[epoch:  18] Test loss: 0.3321 accuracy: 0.883\n",
      "[epoch:  21] Train loss: 0.3281 accuracy: 0.886\n",
      "[epoch:  21] Test loss: 0.3161 accuracy: 0.888\n",
      "[epoch:  24] Train loss: 0.3297 accuracy: 0.885\n",
      "[epoch:  24] Test loss: 0.3364 accuracy: 0.884\n",
      "Diverging. stop.\n",
      "[epoch:   0] Train loss: 0.4916 accuracy: 0.830\n",
      "[epoch:   0] Test loss: 0.3789 accuracy: 0.870\n",
      "[epoch:   3] Train loss: 0.3555 accuracy: 0.876\n",
      "[epoch:   3] Test loss: 0.3457 accuracy: 0.881\n",
      "[epoch:   6] Train loss: 0.3588 accuracy: 0.875\n",
      "[epoch:   6] Test loss: 0.3367 accuracy: 0.883\n",
      "[epoch:   9] Train loss: 0.3515 accuracy: 0.877\n",
      "[epoch:   9] Test loss: 0.3421 accuracy: 0.882\n",
      "[epoch:  12] Train loss: 0.3540 accuracy: 0.878\n",
      "[epoch:  12] Test loss: 0.3609 accuracy: 0.875\n",
      "[epoch:  15] Train loss: 0.3529 accuracy: 0.877\n",
      "[epoch:  15] Test loss: 0.3349 accuracy: 0.883\n",
      "[epoch:  18] Train loss: 0.3536 accuracy: 0.876\n",
      "[epoch:  18] Test loss: 0.3360 accuracy: 0.883\n",
      "Early stopping at epoch 18\n",
      "[epoch:   0] Train loss: 0.4942 accuracy: 0.830\n",
      "[epoch:   0] Test loss: 0.4119 accuracy: 0.860\n",
      "[epoch:   3] Train loss: 0.3785 accuracy: 0.868\n",
      "[epoch:   3] Test loss: 0.3968 accuracy: 0.862\n",
      "[epoch:   6] Train loss: 0.3765 accuracy: 0.868\n",
      "[epoch:   6] Test loss: 0.3800 accuracy: 0.870\n",
      "[epoch:   9] Train loss: 0.3731 accuracy: 0.871\n",
      "[epoch:   9] Test loss: 0.4131 accuracy: 0.858\n",
      "[epoch:  12] Train loss: 0.3707 accuracy: 0.871\n",
      "[epoch:  12] Test loss: 0.3579 accuracy: 0.877\n",
      "[epoch:  15] Train loss: 0.3694 accuracy: 0.871\n",
      "[epoch:  15] Test loss: 0.3590 accuracy: 0.874\n",
      "[epoch:  18] Train loss: 0.3680 accuracy: 0.871\n",
      "[epoch:  18] Test loss: 0.3684 accuracy: 0.871\n",
      "[epoch:  21] Train loss: 0.3701 accuracy: 0.871\n",
      "[epoch:  21] Test loss: 0.3564 accuracy: 0.876\n",
      "Diverging. stop.\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "wd_list = [0.00005,0.0005]\n",
    "lr_list = [0.005]\n",
    "do_list = [0, 0.2, 0.5]\n",
    "\n",
    "for (lr, wd, do) in itertools.product(lr_list, wd_list, do_list):\n",
    "\n",
    "    model = mnl.MNL2(n_alts=n_alts, dim_embed=x_embed.shape[-1], dim_demo=x_trip.shape[-1], dropout=do)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    ref1 = 0\n",
    "    ref2 = 0\n",
    "\n",
    "    for epoch in range(100):\n",
    "        loss_ = 0\n",
    "        correct = 0\n",
    "        for batch, (x_batch, y_batch) in enumerate(trainloader):\n",
    "            # Compute prediction and loss\n",
    "            util = model(x_batch)\n",
    "            loss = loss_fn(util, y_batch)\n",
    "            loss_ += loss.item() * len(x_batch)\n",
    "\n",
    "            pred = torch.argmax(util, dim=1)\n",
    "            correct += torch.sum(pred == y_batch)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        if epoch % 3 == 0:\n",
    "            loss_ /= len(trainset)\n",
    "            train_acc = correct/len(trainset)\n",
    "            print(f\"[epoch: {epoch:>3d}] Train loss: {loss_:.4f} accuracy: {train_acc:.3f}\")\n",
    "            \n",
    "            correct = 0\n",
    "            test_loss_ = 0\n",
    "            for batch, (x_batch, y_batch) in enumerate(testloader):\n",
    "                util = model(x_batch)\n",
    "                loss = loss_fn(util, y_batch)\n",
    "                test_loss_ += loss.item()\n",
    "                pred = torch.argmax(util, dim=1)\n",
    "                correct += torch.sum(pred == y_batch)\n",
    "            assert batch == 0 # there is only one batch in test\n",
    "            test_acc = correct/len(testset)            \n",
    "            print(f\"[epoch: {epoch:>3d}] Test loss: {test_loss_:.4f} accuracy: {test_acc:.3f}\")\n",
    "\n",
    "            if epoch > 15:\n",
    "                if (np.abs(loss_ - ref1)/ref1<ref1*0.01) & (np.abs(loss_ - ref2)/ref2<ref2*0.01):\n",
    "                    print(\"Early stopping at epoch\", epoch)\n",
    "                    break\n",
    "                if (ref1 < loss_) & (ref1 < ref2):\n",
    "                    print(\"Diverging. stop.\")\n",
    "                    break\n",
    "                if loss_ < best:\n",
    "                    best = loss_\n",
    "                    best_test = test_loss_\n",
    "                    best_epoch = epoch\n",
    "                    best_train_acc = train_acc\n",
    "                    best_test_acc = test_acc\n",
    "            else:\n",
    "                best = loss_\n",
    "                best_test = test_loss_\n",
    "                best_epoch = epoch\n",
    "                best_train_acc = train_acc\n",
    "                best_test_acc = test_acc\n",
    "                \n",
    "            ref2 = ref1\n",
    "            ref1 = loss_\n",
    "\n",
    "    with open(out_dir+model_code+\".csv\", \"a\") as f:\n",
    "        f.write(\"%s,%s,%s,%s,%.4f,%.5f,%.1f,%d,%.4f,%.4f,%.4f,%.4f\\n\" % \\\n",
    "            (model_run_date, model_type, zoomlevel, \"MNL2\", lr, wd, do, \n",
    "             best_epoch, best, best_test, best_train_acc, best_test_acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701ed07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
