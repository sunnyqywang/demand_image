{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"models/\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from dataloader import SurveyDataset, load_aggregate_travel_behavior, load_demo\n",
    "from M1_util_train_test import load_model, test\n",
    "import mnl\n",
    "from setup import out_dir, data_dir, image_dir, model_dir, proj_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'SAE'\n",
    "load_model_name = 'Autoencoder'\n",
    "load_model_file = 'sae'\n",
    "model_code = 'M1_A1'\n",
    "zoomlevel = 'zoom13'\n",
    "output_dim = 2\n",
    "model_run_date = '22012507'\n",
    "sampling = 's'\n",
    "\n",
    "variable_names = ['active','auto','mas','pt', 'trpgen']\n",
    "\n",
    "demo_variables = ['tot_population','pct25_34yrs','pct35_50yrs','pctover65yrs',\n",
    "         'pctwhite_alone','pct_nonwhite','pctblack_alone',\n",
    "         'pct_col_grad','avg_tt_to_work','inc_per_capita']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proj_dir+\"latent_space/\"+model_type+\"_\"+zoomlevel+\"_\"+str(output_dim**2*2048)+\"_\"+\n",
    "                       model_run_date+\".pkl\", \"rb\") as f: \n",
    "    encoder_output = pkl.load(f)\n",
    "    im = pkl.load(f)\n",
    "    ct = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Embeddings\n",
    "unique_ct = list(set(ct))\n",
    "unique_ct.sort()\n",
    "ct = np.array(ct)\n",
    "aggregate_embeddings = []\n",
    "for i in unique_ct:\n",
    "    aggregate_embeddings.append(np.mean(encoder_output[ct == i], axis=0))\n",
    "aggregate_embeddings = np.array(aggregate_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Trip Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"origin_trip_behavior.csv\"\n",
    "df_pivot = load_aggregate_travel_behavior(file, unique_ct)\n",
    "\n",
    "train_test_index = df_pivot['train_test'].astype(bool).to_numpy()\n",
    "# train_test_index = np.random.rand(len(df_pivot)) < 0.2\n",
    "\n",
    "y = df_pivot[variable_names].to_numpy()\n",
    "y_train = y[~train_test_index,:4]\n",
    "y_test = y[train_test_index,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = aggregate_embeddings[~train_test_index, :]\n",
    "x_test = aggregate_embeddings[train_test_index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNL for Mode Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader and model definition\n",
    "\n",
    "trainset = SurveyDataset(torch.tensor(x_train,  dtype=torch.float), torch.tensor(y_train, dtype=torch.float))\n",
    "trainloader = DataLoader(trainset, batch_size=256, shuffle=True)\n",
    "\n",
    "testset = SurveyDataset(torch.tensor(x_test, dtype=torch.float), torch.tensor(y_test, dtype=torch.float))\n",
    "testloader = DataLoader(testset, batch_size=len(testset), shuffle=True)\n",
    "\n",
    "kldivloss = nn.KLDivLoss(reduction='sum')\n",
    "mseloss = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:  0] Train KL loss: 0.506                 RMSE 0.504                 0.199 0.415 0.151 0.140\n",
      "[epoch:  0] Test KL loss: 0.492                    RMSE 0.411                     0.216 0.326 0.044 0.117\n",
      "\t\t\t\t\t\t\tR2 score: -0.493 -0.712 -0.414 -0.363 \n",
      "[epoch: 10] Train KL loss: 0.135                 RMSE 0.224                 0.129 0.163 0.054 0.062\n",
      "[epoch: 10] Test KL loss: 0.130                    RMSE 0.220                     0.129 0.158 0.040 0.072\n",
      "\t\t\t\t\t\t\tR2 score: 0.470 0.599 -0.169 0.487 \n",
      "[epoch: 20] Train KL loss: 0.117                 RMSE 0.204                 0.117 0.148 0.052 0.056\n",
      "[epoch: 20] Test KL loss: 0.122                    RMSE 0.211                     0.124 0.147 0.040 0.075\n",
      "\t\t\t\t\t\t\tR2 score: 0.508 0.651 -0.171 0.437 \n",
      "[epoch: 30] Train KL loss: 0.110                 RMSE 0.195                 0.112 0.141 0.051 0.053\n",
      "[epoch: 30] Test KL loss: 0.118                    RMSE 0.207                     0.124 0.144 0.039 0.073\n",
      "\t\t\t\t\t\t\tR2 score: 0.509 0.666 -0.107 0.469 \n",
      "[epoch: 40] Train KL loss: 0.104                 RMSE 0.188                 0.108 0.136 0.051 0.052\n",
      "[epoch: 40] Test KL loss: 0.115                    RMSE 0.204                     0.123 0.140 0.039 0.072\n",
      "\t\t\t\t\t\t\tR2 score: 0.517 0.682 -0.107 0.480 \n",
      "[epoch: 50] Train KL loss: 0.101                 RMSE 0.184                 0.105 0.132 0.051 0.051\n",
      "[epoch: 50] Test KL loss: 0.112                    RMSE 0.201                     0.123 0.137 0.039 0.071\n",
      "\t\t\t\t\t\t\tR2 score: 0.521 0.696 -0.101 0.493 \n",
      "[epoch: 60] Train KL loss: 0.097                 RMSE 0.179                 0.102 0.128 0.050 0.049\n",
      "[epoch: 60] Test KL loss: 0.110                    RMSE 0.200                     0.123 0.135 0.039 0.071\n",
      "\t\t\t\t\t\t\tR2 score: 0.521 0.705 -0.115 0.504 \n",
      "[epoch: 70] Train KL loss: 0.094                 RMSE 0.175                 0.100 0.125 0.050 0.048\n",
      "[epoch: 70] Test KL loss: 0.110                    RMSE 0.200                     0.125 0.134 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.505 0.709 -0.117 0.508 \n",
      "[epoch: 80] Train KL loss: 0.092                 RMSE 0.172                 0.099 0.123 0.050 0.047\n",
      "[epoch: 80] Test KL loss: 0.109                    RMSE 0.199                     0.124 0.133 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.509 0.715 -0.124 0.512 \n",
      "[epoch: 90] Train KL loss: 0.089                 RMSE 0.168                 0.097 0.120 0.049 0.046\n",
      "[epoch: 90] Test KL loss: 0.108                    RMSE 0.198                     0.124 0.131 0.039 0.071\n",
      "\t\t\t\t\t\t\tR2 score: 0.513 0.722 -0.110 0.503 \n",
      "[epoch: 100] Train KL loss: 0.087                 RMSE 0.166                 0.096 0.119 0.049 0.045\n",
      "[epoch: 100] Test KL loss: 0.106                    RMSE 0.195                     0.122 0.130 0.039 0.069\n",
      "\t\t\t\t\t\t\tR2 score: 0.529 0.726 -0.122 0.521 \n",
      "[epoch: 110] Train KL loss: 0.085                 RMSE 0.163                 0.094 0.116 0.049 0.045\n",
      "[epoch: 110] Test KL loss: 0.105                    RMSE 0.194                     0.121 0.129 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.535 0.731 -0.119 0.520 \n",
      "[epoch: 120] Train KL loss: 0.084                 RMSE 0.160                 0.092 0.114 0.048 0.044\n",
      "[epoch: 120] Test KL loss: 0.104                    RMSE 0.192                     0.119 0.128 0.039 0.069\n",
      "\t\t\t\t\t\t\tR2 score: 0.549 0.735 -0.129 0.523 \n",
      "[epoch: 130] Train KL loss: 0.082                 RMSE 0.158                 0.090 0.112 0.048 0.043\n",
      "[epoch: 130] Test KL loss: 0.104                    RMSE 0.193                     0.119 0.128 0.040 0.069\n",
      "\t\t\t\t\t\t\tR2 score: 0.545 0.735 -0.151 0.526 \n",
      "[epoch: 140] Train KL loss: 0.081                 RMSE 0.157                 0.089 0.111 0.048 0.043\n",
      "[epoch: 140] Test KL loss: 0.104                    RMSE 0.193                     0.121 0.128 0.039 0.069\n",
      "\t\t\t\t\t\t\tR2 score: 0.531 0.737 -0.112 0.523 \n",
      "[epoch: 150] Train KL loss: 0.080                 RMSE 0.154                 0.088 0.109 0.048 0.043\n",
      "[epoch: 150] Test KL loss: 0.102                    RMSE 0.190                     0.118 0.127 0.039 0.069\n",
      "\t\t\t\t\t\t\tR2 score: 0.559 0.741 -0.098 0.522 \n",
      "[epoch: 160] Train KL loss: 0.078                 RMSE 0.152                 0.087 0.107 0.047 0.041\n",
      "[epoch: 160] Test KL loss: 0.102                    RMSE 0.190                     0.117 0.126 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.562 0.743 -0.110 0.519 \n",
      "[epoch: 170] Train KL loss: 0.077                 RMSE 0.150                 0.086 0.106 0.047 0.041\n",
      "[epoch: 170] Test KL loss: 0.101                    RMSE 0.188                     0.116 0.125 0.039 0.069\n",
      "\t\t\t\t\t\t\tR2 score: 0.571 0.748 -0.111 0.522 \n",
      "[epoch: 180] Train KL loss: 0.076                 RMSE 0.149                 0.085 0.105 0.047 0.040\n",
      "[epoch: 180] Test KL loss: 0.102                    RMSE 0.189                     0.116 0.125 0.040 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.572 0.747 -0.171 0.517 \n",
      "[epoch: 190] Train KL loss: 0.075                 RMSE 0.146                 0.084 0.103 0.047 0.040\n",
      "[epoch: 190] Test KL loss: 0.101                    RMSE 0.187                     0.114 0.124 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.587 0.750 -0.129 0.517 \n",
      "[epoch: 200] Train KL loss: 0.074                 RMSE 0.146                 0.084 0.103 0.047 0.039\n",
      "[epoch: 200] Test KL loss: 0.103                    RMSE 0.192                     0.121 0.127 0.040 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.537 0.740 -0.139 0.521 \n",
      "[epoch: 210] Train KL loss: 0.073                 RMSE 0.143                 0.081 0.101 0.046 0.038\n",
      "[epoch: 210] Test KL loss: 0.101                    RMSE 0.186                     0.113 0.124 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.597 0.753 -0.123 0.510 \n",
      "[epoch: 220] Train KL loss: 0.072                 RMSE 0.142                 0.081 0.100 0.046 0.039\n",
      "[epoch: 220] Test KL loss: 0.102                    RMSE 0.191                     0.118 0.126 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.554 0.744 -0.121 0.517 \n",
      "[epoch: 230] Train KL loss: 0.071                 RMSE 0.142                 0.081 0.100 0.046 0.038\n",
      "[epoch: 230] Test KL loss: 0.102                    RMSE 0.190                     0.118 0.126 0.040 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.556 0.745 -0.164 0.519 \n",
      "[epoch: 240] Train KL loss: 0.070                 RMSE 0.139                 0.079 0.098 0.045 0.037\n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.504                 RMSE 0.504                 0.200 0.415 0.149 0.141\n",
      "[epoch:  0] Test KL loss: 0.473                    RMSE 0.408                     0.215 0.323 0.044 0.117\n",
      "\t\t\t\t\t\t\tR2 score: -0.472 -0.686 -0.413 -0.352 \n",
      "[epoch: 10] Train KL loss: 0.133                 RMSE 0.221                 0.129 0.161 0.054 0.061\n",
      "[epoch: 10] Test KL loss: 0.129                    RMSE 0.219                     0.130 0.157 0.040 0.072\n",
      "\t\t\t\t\t\t\tR2 score: 0.465 0.605 -0.165 0.489 \n",
      "[epoch: 20] Train KL loss: 0.116                 RMSE 0.202                 0.116 0.147 0.052 0.056\n",
      "[epoch: 20] Test KL loss: 0.123                    RMSE 0.212                     0.126 0.147 0.041 0.075\n",
      "\t\t\t\t\t\t\tR2 score: 0.497 0.650 -0.210 0.440 \n",
      "[epoch: 30] Train KL loss: 0.109                 RMSE 0.194                 0.111 0.141 0.051 0.053\n",
      "[epoch: 30] Test KL loss: 0.118                    RMSE 0.208                     0.125 0.144 0.039 0.073\n",
      "\t\t\t\t\t\t\tR2 score: 0.500 0.666 -0.115 0.468 \n",
      "[epoch: 40] Train KL loss: 0.104                 RMSE 0.188                 0.108 0.136 0.051 0.052\n",
      "[epoch: 40] Test KL loss: 0.116                    RMSE 0.205                     0.125 0.140 0.039 0.072\n",
      "\t\t\t\t\t\t\tR2 score: 0.504 0.683 -0.121 0.482 \n",
      "[epoch: 50] Train KL loss: 0.101                 RMSE 0.183                 0.105 0.132 0.051 0.051\n",
      "[epoch: 50] Test KL loss: 0.113                    RMSE 0.203                     0.125 0.137 0.039 0.072\n",
      "\t\t\t\t\t\t\tR2 score: 0.505 0.696 -0.112 0.489 \n",
      "[epoch: 60] Train KL loss: 0.097                 RMSE 0.178                 0.102 0.128 0.050 0.049\n",
      "[epoch: 60] Test KL loss: 0.111                    RMSE 0.201                     0.123 0.136 0.039 0.071\n",
      "\t\t\t\t\t\t\tR2 score: 0.515 0.703 -0.102 0.498 \n",
      "[epoch: 70] Train KL loss: 0.094                 RMSE 0.174                 0.100 0.125 0.050 0.048\n",
      "[epoch: 70] Test KL loss: 0.110                    RMSE 0.199                     0.123 0.134 0.039 0.071\n",
      "\t\t\t\t\t\t\tR2 score: 0.518 0.711 -0.125 0.507 \n",
      "[epoch: 80] Train KL loss: 0.091                 RMSE 0.171                 0.098 0.122 0.050 0.047\n",
      "[epoch: 80] Test KL loss: 0.108                    RMSE 0.197                     0.122 0.132 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.525 0.717 -0.104 0.510 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 90] Train KL loss: 0.089                 RMSE 0.168                 0.096 0.120 0.049 0.046\n",
      "[epoch: 90] Test KL loss: 0.107                    RMSE 0.196                     0.122 0.131 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.528 0.721 -0.099 0.514 \n",
      "[epoch: 100] Train KL loss: 0.087                 RMSE 0.165                 0.094 0.118 0.049 0.046\n",
      "[epoch: 100] Test KL loss: 0.108                    RMSE 0.197                     0.124 0.131 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.514 0.723 -0.134 0.518 \n",
      "[epoch: 110] Train KL loss: 0.085                 RMSE 0.162                 0.093 0.115 0.049 0.045\n",
      "[epoch: 110] Test KL loss: 0.107                    RMSE 0.197                     0.123 0.131 0.039 0.069\n",
      "\t\t\t\t\t\t\tR2 score: 0.515 0.725 -0.131 0.521 \n",
      "[epoch: 120] Train KL loss: 0.084                 RMSE 0.161                 0.091 0.114 0.048 0.044\n",
      "[epoch: 120] Test KL loss: 0.107                    RMSE 0.196                     0.123 0.131 0.040 0.069\n",
      "\t\t\t\t\t\t\tR2 score: 0.517 0.725 -0.146 0.522 \n",
      "[epoch: 130] Train KL loss: 0.082                 RMSE 0.157                 0.089 0.111 0.048 0.043\n",
      "[epoch: 130] Test KL loss: 0.104                    RMSE 0.193                     0.120 0.128 0.040 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.539 0.736 -0.139 0.521 \n",
      "[epoch: 140] Train KL loss: 0.082                 RMSE 0.159                 0.091 0.113 0.048 0.043\n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.541                 RMSE 0.523                 0.200 0.431 0.150 0.157\n",
      "[epoch:  0] Test KL loss: 0.483                    RMSE 0.410                     0.217 0.325 0.044 0.116\n",
      "\t\t\t\t\t\t\tR2 score: -0.499 -0.704 -0.409 -0.343 \n",
      "[epoch: 10] Train KL loss: 0.141                 RMSE 0.227                 0.133 0.162 0.054 0.069\n",
      "[epoch: 10] Test KL loss: 0.136                    RMSE 0.228                     0.133 0.162 0.040 0.079\n",
      "\t\t\t\t\t\t\tR2 score: 0.437 0.575 -0.175 0.383 \n",
      "[epoch: 20] Train KL loss: 0.117                 RMSE 0.203                 0.116 0.148 0.052 0.056\n",
      "[epoch: 20] Test KL loss: 0.123                    RMSE 0.212                     0.126 0.148 0.040 0.076\n",
      "\t\t\t\t\t\t\tR2 score: 0.497 0.648 -0.139 0.423 \n",
      "[epoch: 30] Train KL loss: 0.110                 RMSE 0.195                 0.112 0.141 0.051 0.054\n",
      "[epoch: 30] Test KL loss: 0.118                    RMSE 0.208                     0.125 0.144 0.039 0.074\n",
      "\t\t\t\t\t\t\tR2 score: 0.503 0.666 -0.102 0.464 \n",
      "[epoch: 40] Train KL loss: 0.105                 RMSE 0.189                 0.108 0.137 0.051 0.053\n",
      "[epoch: 40] Test KL loss: 0.116                    RMSE 0.205                     0.125 0.140 0.039 0.073\n",
      "\t\t\t\t\t\t\tR2 score: 0.506 0.685 -0.116 0.473 \n",
      "[epoch: 50] Train KL loss: 0.101                 RMSE 0.184                 0.106 0.133 0.051 0.051\n",
      "[epoch: 50] Test KL loss: 0.114                    RMSE 0.203                     0.125 0.137 0.039 0.072\n",
      "\t\t\t\t\t\t\tR2 score: 0.506 0.697 -0.118 0.485 \n",
      "[epoch: 60] Train KL loss: 0.098                 RMSE 0.181                 0.103 0.130 0.050 0.050\n",
      "[epoch: 60] Test KL loss: 0.111                    RMSE 0.200                     0.123 0.135 0.039 0.071\n",
      "\t\t\t\t\t\t\tR2 score: 0.515 0.705 -0.120 0.498 \n",
      "[epoch: 70] Train KL loss: 0.095                 RMSE 0.177                 0.101 0.127 0.050 0.049\n",
      "[epoch: 70] Test KL loss: 0.110                    RMSE 0.199                     0.122 0.134 0.039 0.071\n",
      "\t\t\t\t\t\t\tR2 score: 0.525 0.711 -0.100 0.496 \n",
      "[epoch: 80] Train KL loss: 0.092                 RMSE 0.172                 0.098 0.123 0.050 0.048\n",
      "[epoch: 80] Test KL loss: 0.108                    RMSE 0.197                     0.122 0.132 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.529 0.720 -0.122 0.509 \n",
      "[epoch: 90] Train KL loss: 0.090                 RMSE 0.169                 0.097 0.121 0.049 0.047\n",
      "[epoch: 90] Test KL loss: 0.107                    RMSE 0.196                     0.122 0.131 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.526 0.724 -0.127 0.509 \n",
      "[epoch: 100] Train KL loss: 0.088                 RMSE 0.166                 0.095 0.119 0.049 0.046\n",
      "[epoch: 100] Test KL loss: 0.106                    RMSE 0.194                     0.120 0.130 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.538 0.729 -0.123 0.515 \n",
      "[epoch: 110] Train KL loss: 0.086                 RMSE 0.163                 0.093 0.116 0.049 0.045\n",
      "[epoch: 110] Test KL loss: 0.106                    RMSE 0.194                     0.121 0.129 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.535 0.732 -0.136 0.518 \n",
      "[epoch: 120] Train KL loss: 0.084                 RMSE 0.162                 0.093 0.116 0.049 0.045\n",
      "[epoch: 120] Test KL loss: 0.106                    RMSE 0.194                     0.122 0.128 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.526 0.735 -0.129 0.513 \n",
      "[epoch: 130] Train KL loss: 0.083                 RMSE 0.159                 0.091 0.113 0.048 0.044\n",
      "[epoch: 130] Test KL loss: 0.103                    RMSE 0.191                     0.118 0.127 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.558 0.741 -0.111 0.521 \n",
      "[epoch: 140] Train KL loss: 0.082                 RMSE 0.157                 0.089 0.111 0.048 0.043\n",
      "[epoch: 140] Test KL loss: 0.103                    RMSE 0.190                     0.117 0.126 0.039 0.069\n",
      "\t\t\t\t\t\t\tR2 score: 0.563 0.744 -0.116 0.522 \n",
      "[epoch: 150] Train KL loss: 0.080                 RMSE 0.155                 0.088 0.110 0.048 0.043\n",
      "[epoch: 150] Test KL loss: 0.103                    RMSE 0.191                     0.120 0.126 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.544 0.744 -0.115 0.517 \n",
      "[epoch: 160] Train KL loss: 0.079                 RMSE 0.152                 0.087 0.108 0.047 0.042\n",
      "[epoch: 160] Test KL loss: 0.101                    RMSE 0.188                     0.116 0.125 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.572 0.748 -0.124 0.520 \n",
      "[epoch: 170] Train KL loss: 0.077                 RMSE 0.150                 0.085 0.106 0.047 0.042\n",
      "[epoch: 170] Test KL loss: 0.102                    RMSE 0.188                     0.116 0.125 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.572 0.750 -0.133 0.521 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.491                 RMSE 0.495                 0.192 0.405 0.150 0.145\n",
      "[epoch:  0] Test KL loss: 0.475                    RMSE 0.408                     0.214 0.324 0.044 0.117\n",
      "\t\t\t\t\t\t\tR2 score: -0.464 -0.692 -0.440 -0.362 \n",
      "[epoch: 10] Train KL loss: 0.137                 RMSE 0.225                 0.131 0.162 0.054 0.067\n",
      "[epoch: 10] Test KL loss: 0.127                    RMSE 0.219                     0.130 0.155 0.040 0.073\n",
      "\t\t\t\t\t\t\tR2 score: 0.461 0.613 -0.143 0.476 \n",
      "[epoch: 20] Train KL loss: 0.116                 RMSE 0.201                 0.115 0.146 0.052 0.056\n",
      "[epoch: 20] Test KL loss: 0.123                    RMSE 0.211                     0.126 0.146 0.040 0.076\n",
      "\t\t\t\t\t\t\tR2 score: 0.498 0.654 -0.157 0.426 \n",
      "[epoch: 30] Train KL loss: 0.109                 RMSE 0.193                 0.111 0.140 0.051 0.054\n",
      "[epoch: 30] Test KL loss: 0.118                    RMSE 0.207                     0.125 0.143 0.039 0.073\n",
      "\t\t\t\t\t\t\tR2 score: 0.499 0.671 -0.115 0.466 \n",
      "[epoch: 40] Train KL loss: 0.104                 RMSE 0.188                 0.107 0.136 0.051 0.052\n",
      "[epoch: 40] Test KL loss: 0.115                    RMSE 0.204                     0.125 0.139 0.039 0.073\n",
      "\t\t\t\t\t\t\tR2 score: 0.505 0.687 -0.099 0.476 \n",
      "[epoch: 50] Train KL loss: 0.100                 RMSE 0.182                 0.104 0.131 0.051 0.051\n",
      "[epoch: 50] Test KL loss: 0.113                    RMSE 0.202                     0.125 0.137 0.039 0.072\n",
      "\t\t\t\t\t\t\tR2 score: 0.506 0.698 -0.117 0.483 \n",
      "[epoch: 60] Train KL loss: 0.097                 RMSE 0.178                 0.102 0.128 0.050 0.050\n",
      "[epoch: 60] Test KL loss: 0.111                    RMSE 0.200                     0.123 0.135 0.039 0.071\n",
      "\t\t\t\t\t\t\tR2 score: 0.517 0.707 -0.113 0.498 \n",
      "[epoch: 70] Train KL loss: 0.093                 RMSE 0.174                 0.099 0.124 0.050 0.049\n",
      "[epoch: 70] Test KL loss: 0.111                    RMSE 0.201                     0.126 0.134 0.039 0.071\n",
      "\t\t\t\t\t\t\tR2 score: 0.493 0.710 -0.100 0.502 \n",
      "[epoch: 80] Train KL loss: 0.091                 RMSE 0.171                 0.098 0.122 0.050 0.048\n",
      "[epoch: 80] Test KL loss: 0.108                    RMSE 0.197                     0.123 0.132 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.520 0.719 -0.120 0.511 \n",
      "[epoch: 90] Train KL loss: 0.089                 RMSE 0.168                 0.096 0.120 0.049 0.047\n",
      "[epoch: 90] Test KL loss: 0.106                    RMSE 0.195                     0.120 0.131 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.541 0.725 -0.125 0.515 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 100] Train KL loss: 0.087                 RMSE 0.165                 0.094 0.118 0.049 0.046\n",
      "[epoch: 100] Test KL loss: 0.106                    RMSE 0.195                     0.122 0.130 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.527 0.728 -0.113 0.519 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 1.380                 RMSE 0.548                 0.222 0.454 0.154 0.146\n",
      "[epoch:  0] Test KL loss: 2.079                    RMSE 0.453                     0.236 0.363 0.053 0.123\n",
      "\t\t\t\t\t\t\tR2 score: -0.768 -1.126 -1.048 -0.501 \n",
      "[epoch: 10] Train KL loss: 0.226                 RMSE 0.244                 0.147 0.154 0.077 0.091\n",
      "[epoch: 10] Test KL loss: 0.359                    RMSE 0.266                     0.156 0.174 0.040 0.122\n",
      "\t\t\t\t\t\t\tR2 score: 0.227 0.514 -0.159 -0.463 \n",
      "[epoch: 20] Train KL loss: 0.110                 RMSE 0.193                 0.109 0.139 0.055 0.053\n",
      "[epoch: 20] Test KL loss: 0.119                    RMSE 0.207                     0.126 0.141 0.040 0.075\n",
      "\t\t\t\t\t\t\tR2 score: 0.492 0.680 -0.163 0.448 \n",
      "[epoch: 30] Train KL loss: 0.097                 RMSE 0.179                 0.104 0.128 0.050 0.050\n",
      "[epoch: 30] Test KL loss: 0.111                    RMSE 0.201                     0.124 0.136 0.039 0.071\n",
      "\t\t\t\t\t\t\tR2 score: 0.508 0.704 -0.090 0.496 \n",
      "[epoch: 40] Train KL loss: 0.092                 RMSE 0.172                 0.099 0.122 0.050 0.047\n",
      "[epoch: 40] Test KL loss: 0.108                    RMSE 0.197                     0.122 0.132 0.040 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.525 0.721 -0.171 0.512 \n",
      "[epoch: 50] Train KL loss: 0.091                 RMSE 0.171                 0.100 0.122 0.050 0.046\n",
      "[epoch: 50] Test KL loss: 0.112                    RMSE 0.203                     0.128 0.136 0.040 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.476 0.704 -0.170 0.516 \n",
      "[epoch: 60] Train KL loss: 0.084                 RMSE 0.161                 0.093 0.114 0.049 0.044\n",
      "[epoch: 60] Test KL loss: 0.108                    RMSE 0.198                     0.123 0.131 0.041 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.517 0.721 -0.235 0.514 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 1.372                 RMSE 0.538                 0.220 0.446 0.129 0.161\n",
      "[epoch:  0] Test KL loss: 2.203                    RMSE 0.453                     0.236 0.363 0.053 0.123\n",
      "\t\t\t\t\t\t\tR2 score: -0.770 -1.128 -1.048 -0.501 \n",
      "[epoch: 10] Train KL loss: 0.186                 RMSE 0.241                 0.155 0.154 0.056 0.085\n",
      "[epoch: 10] Test KL loss: 0.304                    RMSE 0.294                     0.213 0.160 0.046 0.116\n",
      "\t\t\t\t\t\t\tR2 score: -0.441 0.589 -0.545 -0.343 \n",
      "[epoch: 20] Train KL loss: 0.112                 RMSE 0.194                 0.111 0.138 0.053 0.061\n",
      "[epoch: 20] Test KL loss: 0.125                    RMSE 0.214                     0.133 0.143 0.039 0.078\n",
      "\t\t\t\t\t\t\tR2 score: 0.438 0.670 -0.123 0.401 \n",
      "[epoch: 30] Train KL loss: 0.099                 RMSE 0.183                 0.105 0.131 0.050 0.051\n",
      "[epoch: 30] Test KL loss: 0.115                    RMSE 0.205                     0.129 0.137 0.040 0.072\n",
      "\t\t\t\t\t\t\tR2 score: 0.468 0.698 -0.142 0.480 \n",
      "[epoch: 40] Train KL loss: 0.093                 RMSE 0.175                 0.102 0.125 0.050 0.047\n",
      "[epoch: 40] Test KL loss: 0.109                    RMSE 0.198                     0.123 0.132 0.039 0.071\n",
      "\t\t\t\t\t\t\tR2 score: 0.519 0.718 -0.116 0.504 \n",
      "[epoch: 50] Train KL loss: 0.091                 RMSE 0.170                 0.098 0.122 0.050 0.045\n",
      "[epoch: 50] Test KL loss: 0.118                    RMSE 0.207                     0.125 0.143 0.040 0.071\n",
      "\t\t\t\t\t\t\tR2 score: 0.501 0.668 -0.152 0.503 \n",
      "[epoch: 60] Train KL loss: 0.114                 RMSE 0.201                 0.115 0.149 0.051 0.047\n",
      "[epoch: 60] Test KL loss: 0.158                    RMSE 0.239                     0.140 0.175 0.042 0.073\n",
      "\t\t\t\t\t\t\tR2 score: 0.376 0.508 -0.311 0.469 \n",
      "[epoch: 70] Train KL loss: 0.214                 RMSE 0.292                 0.154 0.230 0.072 0.058\n",
      "[epoch: 70] Test KL loss: 0.365                    RMSE 0.331                     0.179 0.259 0.050 0.089\n",
      "\t\t\t\t\t\t\tR2 score: -0.021 -0.079 -0.851 0.210 \n",
      "[epoch: 80] Train KL loss: 0.083                 RMSE 0.159                 0.093 0.112 0.048 0.043\n",
      "[epoch: 80] Test KL loss: 0.108                    RMSE 0.198                     0.125 0.131 0.041 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.504 0.724 -0.244 0.517 \n",
      "[epoch: 90] Train KL loss: 0.082                 RMSE 0.158                 0.089 0.114 0.049 0.041\n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 1.416                 RMSE 0.558                 0.226 0.463 0.133 0.166\n",
      "[epoch:  0] Test KL loss: 2.239                    RMSE 0.453                     0.236 0.363 0.053 0.123\n",
      "\t\t\t\t\t\t\tR2 score: -0.770 -1.128 -1.048 -0.501 \n",
      "[epoch: 10] Train KL loss: 0.227                 RMSE 0.266                 0.145 0.174 0.111 0.086\n",
      "[epoch: 10] Test KL loss: 0.293                    RMSE 0.267                     0.150 0.181 0.040 0.119\n",
      "\t\t\t\t\t\t\tR2 score: 0.279 0.472 -0.157 -0.403 \n",
      "[epoch: 20] Train KL loss: 0.115                 RMSE 0.197                 0.110 0.141 0.051 0.064\n",
      "[epoch: 20] Test KL loss: 0.121                    RMSE 0.210                     0.130 0.142 0.039 0.073\n",
      "\t\t\t\t\t\t\tR2 score: 0.458 0.673 -0.125 0.475 \n",
      "[epoch: 30] Train KL loss: 0.098                 RMSE 0.180                 0.104 0.129 0.050 0.049\n",
      "[epoch: 30] Test KL loss: 0.113                    RMSE 0.202                     0.126 0.136 0.039 0.071\n",
      "\t\t\t\t\t\t\tR2 score: 0.498 0.701 -0.103 0.495 \n",
      "[epoch: 40] Train KL loss: 0.094                 RMSE 0.175                 0.100 0.125 0.050 0.048\n",
      "[epoch: 40] Test KL loss: 0.110                    RMSE 0.199                     0.121 0.135 0.039 0.071\n",
      "\t\t\t\t\t\t\tR2 score: 0.531 0.707 -0.106 0.499 \n",
      "[epoch: 50] Train KL loss: 0.105                 RMSE 0.190                 0.109 0.139 0.051 0.046\n",
      "[epoch: 50] Test KL loss: 0.137                    RMSE 0.225                     0.134 0.161 0.041 0.072\n",
      "\t\t\t\t\t\t\tR2 score: 0.430 0.579 -0.230 0.491 \n",
      "[epoch: 60] Train KL loss: 0.181                 RMSE 0.271                 0.149 0.211 0.061 0.053\n",
      "[epoch: 60] Test KL loss: 0.313                    RMSE 0.327                     0.179 0.254 0.047 0.090\n",
      "\t\t\t\t\t\t\tR2 score: -0.016 -0.039 -0.635 0.192 \n",
      "[epoch: 70] Train KL loss: 0.098                 RMSE 0.182                 0.104 0.133 0.049 0.044\n",
      "[epoch: 70] Test KL loss: 0.135                    RMSE 0.223                     0.128 0.162 0.041 0.075\n",
      "\t\t\t\t\t\t\tR2 score: 0.477 0.578 -0.231 0.447 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 1.322                 RMSE 0.524                 0.216 0.433 0.138 0.148\n",
      "[epoch:  0] Test KL loss: 2.186                    RMSE 0.453                     0.236 0.363 0.053 0.123\n",
      "\t\t\t\t\t\t\tR2 score: -0.769 -1.127 -1.048 -0.501 \n",
      "[epoch: 10] Train KL loss: 0.196                 RMSE 0.272                 0.125 0.197 0.110 0.087\n",
      "[epoch: 10] Test KL loss: 0.254                    RMSE 0.268                     0.146 0.189 0.043 0.114\n",
      "\t\t\t\t\t\t\tR2 score: 0.318 0.427 -0.375 -0.283 \n",
      "[epoch: 20] Train KL loss: 0.109                 RMSE 0.192                 0.113 0.136 0.053 0.055\n",
      "[epoch: 20] Test KL loss: 0.127                    RMSE 0.211                     0.126 0.143 0.047 0.079\n",
      "\t\t\t\t\t\t\tR2 score: 0.494 0.671 -0.598 0.380 \n",
      "[epoch: 30] Train KL loss: 0.098                 RMSE 0.179                 0.103 0.128 0.050 0.050\n",
      "[epoch: 30] Test KL loss: 0.112                    RMSE 0.200                     0.122 0.136 0.040 0.071\n",
      "\t\t\t\t\t\t\tR2 score: 0.524 0.703 -0.174 0.499 \n",
      "[epoch: 40] Train KL loss: 0.093                 RMSE 0.173                 0.100 0.123 0.050 0.048\n",
      "[epoch: 40] Test KL loss: 0.111                    RMSE 0.202                     0.127 0.135 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.487 0.708 -0.111 0.510 \n",
      "[epoch: 50] Train KL loss: 0.090                 RMSE 0.169                 0.096 0.120 0.050 0.046\n",
      "[epoch: 50] Test KL loss: 0.106                    RMSE 0.194                     0.120 0.130 0.039 0.070\n",
      "\t\t\t\t\t\t\tR2 score: 0.545 0.729 -0.130 0.512 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 2.563                 RMSE 0.527                 0.211 0.433 0.149 0.152\n",
      "[epoch:  0] Test KL loss: 4.612                    RMSE 0.454                     0.236 0.363 0.053 0.123\n",
      "\t\t\t\t\t\t\tR2 score: -0.773 -1.131 -1.048 -0.501 \n",
      "[epoch: 10] Train KL loss: 0.407                 RMSE 0.266                 0.142 0.195 0.062 0.096\n",
      "[epoch: 10] Test KL loss: 0.632                    RMSE 0.312                     0.225 0.172 0.048 0.123\n",
      "\t\t\t\t\t\t\tR2 score: -0.609 0.523 -0.665 -0.496 \n",
      "[epoch: 20] Train KL loss: 0.205                 RMSE 0.266                 0.153 0.196 0.061 0.071\n",
      "[epoch: 20] Test KL loss: 0.222                    RMSE 0.301                     0.174 0.222 0.038 0.100\n",
      "\t\t\t\t\t\t\tR2 score: 0.040 0.209 -0.070 0.014 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 30] Train KL loss: 0.356                 RMSE 0.288                 0.158 0.223 0.065 0.063\n",
      "[epoch: 30] Test KL loss: 0.454                    RMSE 0.407                     0.266 0.292 0.040 0.090\n",
      "\t\t\t\t\t\t\tR2 score: -1.248 -0.374 -0.164 0.191 \n",
      "[epoch: 40] Train KL loss: 0.167                 RMSE 0.238                 0.124 0.182 0.066 0.063\n",
      "[epoch: 40] Test KL loss: 0.239                    RMSE 0.312                     0.207 0.214 0.055 0.072\n",
      "\t\t\t\t\t\t\tR2 score: -0.371 0.261 -1.198 0.485 \n",
      "[epoch: 50] Train KL loss: 0.377                 RMSE 0.316                 0.174 0.247 0.062 0.068\n",
      "[epoch: 50] Test KL loss: 0.748                    RMSE 0.537                     0.317 0.408 0.118 0.084\n",
      "\t\t\t\t\t\t\tR2 score: -2.211 -1.686 -9.156 0.300 \n",
      "[epoch: 60] Train KL loss: 0.139                 RMSE 0.222                 0.140 0.143 0.051 0.081\n",
      "[epoch: 60] Test KL loss: 0.253                    RMSE 0.321                     0.244 0.175 0.040 0.106\n",
      "\t\t\t\t\t\t\tR2 score: -0.900 0.509 -0.146 -0.118 \n",
      "[epoch: 70] Train KL loss: 0.088                 RMSE 0.166                 0.091 0.119 0.049 0.054\n",
      "[epoch: 70] Test KL loss: 0.126                    RMSE 0.207                     0.124 0.137 0.041 0.084\n",
      "\t\t\t\t\t\t\tR2 score: 0.509 0.696 -0.198 0.308 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 2.609                 RMSE 0.525                 0.211 0.431 0.159 0.138\n",
      "[epoch:  0] Test KL loss: 4.767                    RMSE 0.454                     0.236 0.363 0.053 0.123\n",
      "\t\t\t\t\t\t\tR2 score: -0.773 -1.131 -1.048 -0.501 \n",
      "[epoch: 10] Train KL loss: 0.585                 RMSE 0.397                 0.200 0.186 0.066 0.280\n",
      "[epoch: 10] Test KL loss: 0.252                    RMSE 0.264                     0.146 0.187 0.050 0.104\n",
      "\t\t\t\t\t\t\tR2 score: 0.325 0.435 -0.854 -0.065 \n",
      "[epoch: 20] Train KL loss: 0.132                 RMSE 0.203                 0.109 0.131 0.056 0.094\n",
      "[epoch: 20] Test KL loss: 0.158                    RMSE 0.222                     0.123 0.149 0.038 0.101\n",
      "\t\t\t\t\t\t\tR2 score: 0.518 0.640 -0.062 -0.011 \n",
      "[epoch: 30] Train KL loss: 0.091                 RMSE 0.171                 0.100 0.121 0.050 0.048\n",
      "[epoch: 30] Test KL loss: 0.122                    RMSE 0.216                     0.138 0.145 0.040 0.071\n",
      "\t\t\t\t\t\t\tR2 score: 0.396 0.662 -0.157 0.498 \n",
      "[epoch: 40] Train KL loss: 0.085                 RMSE 0.163                 0.094 0.116 0.048 0.045\n",
      "[epoch: 40] Test KL loss: 0.106                    RMSE 0.193                     0.120 0.127 0.041 0.071\n",
      "\t\t\t\t\t\t\tR2 score: 0.541 0.740 -0.231 0.505 \n",
      "[epoch: 50] Train KL loss: 0.089                 RMSE 0.166                 0.098 0.116 0.050 0.045\n",
      "[epoch: 50] Test KL loss: 0.141                    RMSE 0.223                     0.146 0.143 0.041 0.080\n",
      "\t\t\t\t\t\t\tR2 score: 0.318 0.671 -0.207 0.363 \n",
      "[epoch: 60] Train KL loss: 0.368                 RMSE 0.255                 0.167 0.158 0.067 0.088\n",
      "[epoch: 60] Test KL loss: 0.436                    RMSE 0.428                     0.155 0.224 0.051 0.326\n",
      "\t\t\t\t\t\t\tR2 score: 0.239 0.188 -0.872 -9.512 \n",
      "[epoch: 70] Train KL loss: 0.104                 RMSE 0.177                 0.109 0.106 0.056 0.072\n",
      "[epoch: 70] Test KL loss: 0.196                    RMSE 0.249                     0.173 0.136 0.043 0.108\n",
      "\t\t\t\t\t\t\tR2 score: 0.043 0.702 -0.334 -0.152 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 2.518                 RMSE 0.531                 0.210 0.438 0.156 0.147\n",
      "[epoch:  0] Test KL loss: 4.825                    RMSE 0.454                     0.236 0.363 0.053 0.123\n",
      "\t\t\t\t\t\t\tR2 score: -0.773 -1.131 -1.048 -0.501 \n",
      "[epoch: 10] Train KL loss: 0.611                 RMSE 0.447                 0.209 0.267 0.120 0.265\n",
      "[epoch: 10] Test KL loss: 0.746                    RMSE 0.422                     0.216 0.339 0.052 0.120\n",
      "\t\t\t\t\t\t\tR2 score: -0.490 -0.848 -0.950 -0.427 \n",
      "[epoch: 20] Train KL loss: 0.150                 RMSE 0.224                 0.134 0.139 0.073 0.087\n",
      "[epoch: 20] Test KL loss: 0.178                    RMSE 0.252                     0.146 0.179 0.048 0.091\n",
      "\t\t\t\t\t\t\tR2 score: 0.325 0.485 -0.709 0.185 \n",
      "[epoch: 30] Train KL loss: 0.182                 RMSE 0.263                 0.147 0.199 0.068 0.057\n",
      "[epoch: 30] Test KL loss: 0.267                    RMSE 0.305                     0.179 0.229 0.048 0.078\n",
      "\t\t\t\t\t\t\tR2 score: -0.026 0.156 -0.686 0.401 \n",
      "[epoch: 40] Train KL loss: 0.211                 RMSE 0.281                 0.159 0.209 0.070 0.070\n",
      "[epoch: 40] Test KL loss: 0.262                    RMSE 0.280                     0.167 0.189 0.049 0.110\n",
      "\t\t\t\t\t\t\tR2 score: 0.115 0.421 -0.742 -0.205 \n",
      "[epoch: 50] Train KL loss: 0.232                 RMSE 0.300                 0.162 0.234 0.076 0.054\n",
      "[epoch: 50] Test KL loss: 0.421                    RMSE 0.358                     0.197 0.280 0.049 0.093\n",
      "\t\t\t\t\t\t\tR2 score: -0.239 -0.262 -0.776 0.141 \n",
      "[epoch: 60] Train KL loss: 0.119                 RMSE 0.212                 0.134 0.149 0.049 0.047\n",
      "[epoch: 60] Test KL loss: 0.186                    RMSE 0.265                     0.168 0.185 0.042 0.076\n",
      "\t\t\t\t\t\t\tR2 score: 0.098 0.446 -0.295 0.430 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 2.675                 RMSE 0.537                 0.216 0.444 0.136 0.161\n",
      "[epoch:  0] Test KL loss: 4.943                    RMSE 0.454                     0.236 0.363 0.053 0.123\n",
      "\t\t\t\t\t\t\tR2 score: -0.773 -1.131 -1.048 -0.501 \n",
      "[epoch: 10] Train KL loss: 0.722                 RMSE 0.314                 0.231 0.185 0.067 0.085\n",
      "[epoch: 10] Test KL loss: 0.611                    RMSE 0.301                     0.198 0.185 0.053 0.119\n",
      "\t\t\t\t\t\t\tR2 score: -0.253 0.445 -1.047 -0.411 \n",
      "[epoch: 20] Train KL loss: 0.116                 RMSE 0.194                 0.111 0.135 0.067 0.053\n",
      "[epoch: 20] Test KL loss: 0.113                    RMSE 0.201                     0.122 0.136 0.039 0.075\n",
      "\t\t\t\t\t\t\tR2 score: 0.529 0.701 -0.087 0.450 \n",
      "[epoch: 30] Train KL loss: 0.091                 RMSE 0.170                 0.098 0.121 0.050 0.048\n",
      "[epoch: 30] Test KL loss: 0.108                    RMSE 0.197                     0.120 0.132 0.039 0.072\n",
      "\t\t\t\t\t\t\tR2 score: 0.538 0.717 -0.101 0.490 \n",
      "[epoch: 40] Train KL loss: 0.086                 RMSE 0.163                 0.093 0.116 0.049 0.045\n",
      "[epoch: 40] Test KL loss: 0.107                    RMSE 0.195                     0.120 0.129 0.039 0.072\n",
      "\t\t\t\t\t\t\tR2 score: 0.544 0.730 -0.135 0.482 \n",
      "[epoch: 50] Train KL loss: 0.089                 RMSE 0.166                 0.100 0.112 0.049 0.052\n",
      "[epoch: 50] Test KL loss: 0.122                    RMSE 0.209                     0.136 0.129 0.039 0.084\n",
      "\t\t\t\t\t\t\tR2 score: 0.413 0.731 -0.108 0.298 \n",
      "[epoch: 60] Train KL loss: 0.183                 RMSE 0.250                 0.126 0.143 0.059 0.151\n",
      "[epoch: 60] Test KL loss: 0.388                    RMSE 0.268                     0.188 0.139 0.046 0.122\n",
      "\t\t\t\t\t\t\tR2 score: -0.129 0.688 -0.515 -0.469 \n",
      "[epoch: 70] Train KL loss: 0.099                 RMSE 0.177                 0.101 0.113 0.056 0.072\n",
      "[epoch: 70] Test KL loss: 0.164                    RMSE 0.239                     0.169 0.133 0.043 0.095\n",
      "\t\t\t\t\t\t\tR2 score: 0.087 0.717 -0.336 0.097 \n",
      "Diverging. stop.\n"
     ]
    }
   ],
   "source": [
    "wd_list = [0.00005,0.0001,0.0005,0.001]\n",
    "lr_list = [0.001, 0.005, 0.01]\n",
    "\n",
    "for (lr, wd) in itertools.product(lr_list, wd_list):\n",
    "    # model setup\n",
    "    model = mnl.MNL(n_alts=4, n_features=x_train.shape[-1])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    # model training\n",
    "\n",
    "    ref1 = 0\n",
    "    ref2 = 0\n",
    "\n",
    "    for epoch in range(400):\n",
    "\n",
    "        kl_ = 0\n",
    "        mse_ = 0\n",
    "        mse1_ = 0\n",
    "        mse2_ = 0\n",
    "        mse3_ = 0\n",
    "        mse4_ = 0\n",
    "\n",
    "        for batch, (x_batch, y_batch) in enumerate(trainloader):\n",
    "            # Compute prediction and loss\n",
    "            util = model(x_batch)\n",
    "            probs = torch.log(nn.functional.softmax(util, dim=1))\n",
    "            kl = kldivloss(probs, y_batch)\n",
    "    #         kl = kldivloss(torch.log(util), y_batch)\n",
    "            kl_ += kl.item()\n",
    "\n",
    "            mse = mseloss(torch.exp(probs), y_batch)\n",
    "    #         mse = mseloss(util, y_batch)\n",
    "            mse_ += mse.sum().item()\n",
    "            mse1_ += mse[:,0].sum().item()\n",
    "            mse2_ += mse[:,1].sum().item()\n",
    "            mse3_ += mse[:,2].sum().item()\n",
    "            mse4_ += mse[:,3].sum().item()\n",
    "            mse = mse.sum()\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            kl.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_kl = kl_/len(trainset)\n",
    "        train_mse = np.sqrt(mse_/len(trainset))\n",
    "        train_mse1 = np.sqrt(mse1_/len(trainset))\n",
    "        train_mse2 = np.sqrt(mse2_/len(trainset))\n",
    "        train_mse3 = np.sqrt(mse3_/len(trainset))\n",
    "        train_mse4 = np.sqrt(mse4_/len(trainset))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[epoch: {epoch:>2d}] Train KL loss: {train_kl:.3f} \\\n",
    "                RMSE {train_mse:.3f} \\\n",
    "                {train_mse1:.3f} {train_mse2:.3f} {train_mse3:.3f} {train_mse4:.3f}\")\n",
    "        loss_ = train_kl\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            if epoch > 50:\n",
    "                if (np.abs(loss_ - ref1)/ref1<ref1*0.01) & (np.abs(loss_ - ref2)/ref2<ref2*0.01):\n",
    "                    print(\"Early stopping at epoch\", epoch)\n",
    "                    break\n",
    "                if (ref1 < loss_) & (ref1 < ref2):\n",
    "                    print(\"Diverging. stop.\")\n",
    "                    break\n",
    "                if loss_ < best:\n",
    "                    best = loss_\n",
    "                    best_epoch = epoch\n",
    "            else:\n",
    "                best = loss_\n",
    "                best_epoch = epoch\n",
    "\n",
    "            ref2 = ref1\n",
    "            ref1 = loss_\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "\n",
    "            kl_ = 0\n",
    "            mse_ = 0 \n",
    "            mse1_ = 0\n",
    "            mse2_ = 0\n",
    "            mse3_ = 0\n",
    "            mse4_ = 0\n",
    "\n",
    "            for batch, (x_batch, y_batch) in enumerate(testloader):\n",
    "                util = model(x_batch)\n",
    "                probs = torch.log(nn.functional.softmax(util,dim=1))\n",
    "                kl = kldivloss(probs, y_batch)\n",
    "        #         kl = kldivloss(torch.log(util), y_batch)\n",
    "                kl_ += kl.item()\n",
    "\n",
    "                mse = mseloss(torch.exp(probs), y_batch)\n",
    "        #         mse = mseloss(util, y_batch)\n",
    "                mse_ += mse.sum().item()\n",
    "                mse1_ += mse[:,0].sum().item()\n",
    "                mse2_ += mse[:,1].sum().item()\n",
    "                mse3_ += mse[:,2].sum().item()\n",
    "                mse4_ += mse[:,3].sum().item()\n",
    "\n",
    "            test_kl = kl_/len(testset)\n",
    "            test_mse = np.sqrt(mse_/len(testset))\n",
    "            test_mse1 = np.sqrt(mse1_/len(testset))\n",
    "            test_mse2 = np.sqrt(mse2_/len(testset))\n",
    "            test_mse3 = np.sqrt(mse3_/len(testset))\n",
    "            test_mse4 = np.sqrt(mse4_/len(testset))\n",
    "\n",
    "            r1 = r2_score(y_batch.numpy()[:,0],torch.exp(probs).detach().numpy()[:,0])\n",
    "            r2 = r2_score(y_batch.numpy()[:,1],torch.exp(probs).detach().numpy()[:,1])\n",
    "            r3 = r2_score(y_batch.numpy()[:,2],torch.exp(probs).detach().numpy()[:,2])\n",
    "            r4 = r2_score(y_batch.numpy()[:,3],torch.exp(probs).detach().numpy()[:,3])\n",
    "\n",
    "            print(f\"[epoch: {epoch:>2d}] Test KL loss: {kl_/len(testset):.3f}\\\n",
    "                    RMSE {np.sqrt(mse_/len(testset)):.3f} \\\n",
    "                    {np.sqrt(mse1_/len(testset)):.3f} {np.sqrt(mse2_/len(testset)):.3f} {np.sqrt(mse3_/len(testset)):.3f} {np.sqrt(mse4_/len(testset)):.3f}\")\n",
    "            print(f\"\\t\\t\\t\\t\\t\\t\\tR2 score: {r1:.3f} {r2:.3f} {r3:.3f} {r4:.3f} \")\n",
    "\n",
    "\n",
    "    with open(out_dir+sampling+\"_\"+model_code+\"_mode_choice.csv\", \"a\") as f:\n",
    "        f.write(\"%s,%s,%s,%s,%.4f,%d,%.5f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f\\n\" % \\\n",
    "            (model_run_date, model_type, zoomlevel, \"MNL\", lr, -1, wd, \n",
    "              train_kl, train_mse, train_mse1, train_mse2, train_mse3, train_mse4,\n",
    "              test_kl, test_mse, test_mse1, test_mse2, test_mse3, test_mse4,\n",
    "              r1, r2, r3, r4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
    "for i in range(4):\n",
    "    ax.scatter(y_batch.detach().numpy()[:,i], torch.exp(probs).detach().numpy()[:,i], s=10)\n",
    "\n",
    "ax.set_xlabel(\"True Probs\")\n",
    "ax.set_ylabel(\"Predicted Probs\")\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR for trip generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "trpgen_train =  y[~train_test_index,1]\n",
    "trpgen_test =  y[train_test_index,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999992920083 0.6338394586288374\n"
     ]
    }
   ],
   "source": [
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(x_train, trpgen_train)\n",
    "with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "    f.write(\"%s,%s,%s,%.4f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], -1, \n",
    "        lr.score(x_train, trpgen_train), lr.score(x_test, trpgen_test), 'lr', zoomlevel,\n",
    "        np.sum(lr.coef_ != 0), len(lr.coef_)))\n",
    "print(lr.score(x_train, trpgen_train), lr.score(x_test, trpgen_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5728778022482439 0.6169675750359441\n",
      "0.5503212214260211 0.5951281381560225\n",
      "0.5236648507049044 0.5682894503641429\n",
      "0.49634690975265683 0.5403427395681156\n",
      "0.47191943672004866 0.5149414192449036\n",
      "0.44559977700064646 0.48677331734781715\n",
      "0.41650727112071995 0.4549009282108033\n",
      "0.3846463433394226 0.4193335028537325\n",
      "0.35001318508944945 0.38006316247477145\n",
      "0.3126112227305379 0.3370967883954815\n"
     ]
    }
   ],
   "source": [
    "for a in np.linspace(0.005, 0.014, 10):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, trpgen_train)\n",
    "    with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "        f.write(\"%s,%s,%s,%.6f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "            lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test), 'lasso', zoomlevel,\n",
    "            np.sum(lasso.coef_ != 0), len(lasso.coef_)))\n",
    "    print(lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9345039258920709 0.7496458570594625\n",
      "0.9221918023537091 0.7570908794667534\n",
      "0.9121977737790844 0.7620496143734405\n",
      "0.903797893353648 0.7655328952418716\n",
      "0.8965583219538752 0.7680582725384554\n",
      "0.8901993615528274 0.7699273495773279\n",
      "0.8845309463328569 0.7713270404072865\n",
      "0.8794180152603241 0.7723783558926351\n",
      "0.8747615136789396 0.7731653323645468\n",
      "0.870486437795585 0.7737478915434046\n"
     ]
    }
   ],
   "source": [
    "for a in np.linspace(1,4,10):\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, trpgen_train)\n",
    "    with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "        f.write(\"%s,%s,%s,%.4f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "            ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "            np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
