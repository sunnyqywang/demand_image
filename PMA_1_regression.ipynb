{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"models/\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from dataloader import SurveyDataset, load_aggregate_travel_behavior, load_demo\n",
    "from M1_util_train_test import load_model, test\n",
    "import mnl\n",
    "from setup import out_dir, data_dir, image_dir, model_dir, proj_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'SAE'\n",
    "load_model_name = 'Autoencoder'\n",
    "load_model_file = 'sae'\n",
    "model_code = 'A'\n",
    "zoomlevel = 'zoom13_bilateral'\n",
    "output_dim = 1\n",
    "model_run_date = '220115'\n",
    "\n",
    "variable_names = ['active','auto','mas','pt', 'trpgen']\n",
    "\n",
    "demo_variables = ['tot_population','pct25_34yrs','pct35_50yrs','pctover65yrs',\n",
    "         'pctwhite_alone','pct_nonwhite','pctblack_alone',\n",
    "         'pct_col_grad','avg_tt_to_work','inc_per_capita']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proj_dir+\"latent_space/\"+model_type+\"_\"+zoomlevel+\"_\"+str(output_dim**2*2048)+\"_\"+\n",
    "                       model_run_date+\".pkl\", \"rb\") as f: \n",
    "    encoder_output = pkl.load(f)\n",
    "    im = pkl.load(f)\n",
    "    ct = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Embeddings\n",
    "unique_ct = list(set(ct))\n",
    "unique_ct.sort()\n",
    "ct = np.array(ct)\n",
    "aggregate_embeddings = []\n",
    "for i in unique_ct:\n",
    "    aggregate_embeddings.append(np.mean(encoder_output[ct == i], axis=0))\n",
    "aggregate_embeddings = np.array(aggregate_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Trip Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"origin_trip_behavior.csv\"\n",
    "df_pivot = load_aggregate_travel_behavior(file, unique_ct)\n",
    "\n",
    "train_test_index = df_pivot['train_test'].astype(bool).to_numpy()\n",
    "# train_test_index = np.random.rand(len(df_pivot)) < 0.2\n",
    "\n",
    "y = df_pivot[variable_names].to_numpy()\n",
    "y_train = y[~train_test_index,:4]\n",
    "y_test = y[train_test_index,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = aggregate_embeddings[~train_test_index, :]\n",
    "x_test = aggregate_embeddings[train_test_index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNL for Mode Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader and model definition\n",
    "\n",
    "trainset = SurveyDataset(torch.tensor(x_train,  dtype=torch.float), torch.tensor(y_train, dtype=torch.float))\n",
    "trainloader = DataLoader(trainset, batch_size=256, shuffle=True)\n",
    "\n",
    "testset = SurveyDataset(torch.tensor(x_test, dtype=torch.float), torch.tensor(y_test, dtype=torch.float))\n",
    "testloader = DataLoader(testset, batch_size=len(testset), shuffle=True)\n",
    "\n",
    "kldivloss = nn.KLDivLoss(reduction='sum')\n",
    "mseloss = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:  0] Train KL loss: 0.660                 RMSE 0.596                 0.196 0.496 0.187 0.188\n",
      "[epoch:  0] Test KL loss: 0.299                    RMSE 0.360                     0.177 0.276 0.096 0.115\n",
      "\t\t\t\t\t\t\tR2 score: -0.001 -0.227 -5.758 -0.305 \n",
      "[epoch: 10] Train KL loss: 0.129                 RMSE 0.212                 0.124 0.151 0.053 0.062\n",
      "[epoch: 10] Test KL loss: 0.219                    RMSE 0.307                     0.178 0.230 0.037 0.091\n",
      "\t\t\t\t\t\t\tR2 score: -0.010 0.149 -0.014 0.181 \n",
      "[epoch: 20] Train KL loss: 0.110                 RMSE 0.193                 0.114 0.136 0.053 0.053\n",
      "[epoch: 20] Test KL loss: 0.203                    RMSE 0.297                     0.167 0.224 0.037 0.093\n",
      "\t\t\t\t\t\t\tR2 score: 0.110 0.188 -0.024 0.142 \n",
      "[epoch: 30] Train KL loss: 0.105                 RMSE 0.188                 0.110 0.133 0.052 0.052\n",
      "[epoch: 30] Test KL loss: 0.205                    RMSE 0.298                     0.166 0.225 0.038 0.095\n",
      "\t\t\t\t\t\t\tR2 score: 0.119 0.182 -0.044 0.100 \n",
      "[epoch: 40] Train KL loss: 0.103                 RMSE 0.185                 0.108 0.131 0.052 0.051\n",
      "[epoch: 40] Test KL loss: 0.204                    RMSE 0.297                     0.165 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.131 0.190 -0.044 0.090 \n",
      "[epoch: 50] Train KL loss: 0.101                 RMSE 0.182                 0.106 0.129 0.052 0.051\n",
      "[epoch: 50] Test KL loss: 0.204                    RMSE 0.296                     0.165 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.136 0.195 -0.049 0.088 \n",
      "[epoch: 60] Train KL loss: 0.100                 RMSE 0.180                 0.105 0.127 0.052 0.051\n",
      "[epoch: 60] Test KL loss: 0.203                    RMSE 0.295                     0.164 0.222 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.142 0.202 -0.058 0.091 \n",
      "[epoch: 70] Train KL loss: 0.098                 RMSE 0.179                 0.104 0.126 0.052 0.050\n",
      "[epoch: 70] Test KL loss: 0.205                    RMSE 0.296                     0.165 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.138 0.193 -0.050 0.081 \n",
      "[epoch: 80] Train KL loss: 0.097                 RMSE 0.178                 0.103 0.125 0.052 0.050\n",
      "[epoch: 80] Test KL loss: 0.208                    RMSE 0.299                     0.167 0.226 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.115 0.177 -0.053 0.091 \n",
      "[epoch: 90] Train KL loss: 0.096                 RMSE 0.176                 0.102 0.124 0.051 0.050\n",
      "[epoch: 90] Test KL loss: 0.208                    RMSE 0.299                     0.166 0.226 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.119 0.179 -0.060 0.086 \n",
      "[epoch: 100] Train KL loss: 0.095                 RMSE 0.175                 0.101 0.123 0.051 0.049\n",
      "[epoch: 100] Test KL loss: 0.206                    RMSE 0.297                     0.165 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.131 0.191 -0.070 0.090 \n",
      "[epoch: 110] Train KL loss: 0.094                 RMSE 0.173                 0.100 0.122 0.051 0.049\n",
      "[epoch: 110] Test KL loss: 0.205                    RMSE 0.296                     0.164 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.140 0.191 -0.053 0.083 \n",
      "[epoch: 120] Train KL loss: 0.094                 RMSE 0.172                 0.100 0.122 0.051 0.049\n",
      "[epoch: 120] Test KL loss: 0.206                    RMSE 0.297                     0.165 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.133 0.192 -0.071 0.093 \n",
      "[epoch: 130] Train KL loss: 0.093                 RMSE 0.171                 0.099 0.121 0.051 0.049\n",
      "[epoch: 130] Test KL loss: 0.204                    RMSE 0.295                     0.163 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.150 0.200 -0.072 0.079 \n",
      "[epoch: 140] Train KL loss: 0.093                 RMSE 0.171                 0.098 0.120 0.051 0.048\n",
      "[epoch: 140] Test KL loss: 0.206                    RMSE 0.296                     0.164 0.223 0.039 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.139 0.195 -0.089 0.088 \n",
      "[epoch: 150] Train KL loss: 0.092                 RMSE 0.170                 0.098 0.120 0.051 0.048\n",
      "[epoch: 150] Test KL loss: 0.206                    RMSE 0.296                     0.165 0.224 0.039 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.137 0.192 -0.084 0.090 \n",
      "[epoch: 160] Train KL loss: 0.091                 RMSE 0.169                 0.097 0.119 0.051 0.048\n",
      "[epoch: 160] Test KL loss: 0.209                    RMSE 0.299                     0.166 0.226 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.124 0.176 -0.075 0.082 \n",
      "[epoch: 170] Train KL loss: 0.091                 RMSE 0.169                 0.097 0.119 0.050 0.047\n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.663                 RMSE 0.597                 0.198 0.499 0.192 0.179\n",
      "[epoch:  0] Test KL loss: 0.301                    RMSE 0.361                     0.177 0.277 0.101 0.113\n",
      "\t\t\t\t\t\t\tR2 score: 0.003 -0.234 -6.429 -0.256 \n",
      "[epoch: 10] Train KL loss: 0.128                 RMSE 0.212                 0.124 0.151 0.053 0.061\n",
      "[epoch: 10] Test KL loss: 0.219                    RMSE 0.308                     0.178 0.231 0.037 0.091\n",
      "\t\t\t\t\t\t\tR2 score: -0.014 0.142 0.010 0.186 \n",
      "[epoch: 20] Train KL loss: 0.110                 RMSE 0.193                 0.114 0.136 0.053 0.053\n",
      "[epoch: 20] Test KL loss: 0.204                    RMSE 0.298                     0.168 0.225 0.037 0.093\n",
      "\t\t\t\t\t\t\tR2 score: 0.102 0.182 -0.020 0.142 \n",
      "[epoch: 30] Train KL loss: 0.106                 RMSE 0.188                 0.111 0.133 0.052 0.052\n",
      "[epoch: 30] Test KL loss: 0.204                    RMSE 0.298                     0.166 0.225 0.038 0.095\n",
      "\t\t\t\t\t\t\tR2 score: 0.122 0.184 -0.040 0.100 \n",
      "[epoch: 40] Train KL loss: 0.103                 RMSE 0.185                 0.109 0.131 0.052 0.051\n",
      "[epoch: 40] Test KL loss: 0.206                    RMSE 0.299                     0.166 0.226 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.118 0.179 -0.040 0.090 \n",
      "[epoch: 50] Train KL loss: 0.101                 RMSE 0.183                 0.107 0.129 0.052 0.051\n",
      "[epoch: 50] Test KL loss: 0.207                    RMSE 0.299                     0.166 0.226 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.118 0.177 -0.042 0.081 \n",
      "[epoch: 60] Train KL loss: 0.100                 RMSE 0.181                 0.105 0.128 0.052 0.051\n",
      "[epoch: 60] Test KL loss: 0.205                    RMSE 0.297                     0.165 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.128 0.189 -0.049 0.089 \n",
      "[epoch: 70] Train KL loss: 0.098                 RMSE 0.179                 0.104 0.126 0.052 0.050\n",
      "[epoch: 70] Test KL loss: 0.205                    RMSE 0.297                     0.165 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.133 0.190 -0.045 0.087 \n",
      "[epoch: 80] Train KL loss: 0.097                 RMSE 0.177                 0.103 0.125 0.052 0.050\n",
      "[epoch: 80] Test KL loss: 0.206                    RMSE 0.297                     0.166 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.126 0.187 -0.063 0.088 \n",
      "[epoch: 90] Train KL loss: 0.096                 RMSE 0.176                 0.102 0.124 0.051 0.050\n",
      "[epoch: 90] Test KL loss: 0.205                    RMSE 0.296                     0.164 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.139 0.196 -0.063 0.084 \n",
      "[epoch: 100] Train KL loss: 0.095                 RMSE 0.175                 0.101 0.123 0.051 0.049\n",
      "[epoch: 100] Test KL loss: 0.204                    RMSE 0.295                     0.164 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.141 0.199 -0.068 0.088 \n",
      "[epoch: 110] Train KL loss: 0.095                 RMSE 0.174                 0.101 0.123 0.051 0.049\n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.646                 RMSE 0.589                 0.197 0.491 0.182 0.183\n",
      "[epoch:  0] Test KL loss: 0.296                    RMSE 0.357                     0.178 0.273 0.094 0.113\n",
      "\t\t\t\t\t\t\tR2 score: -0.007 -0.201 -5.440 -0.272 \n",
      "[epoch: 10] Train KL loss: 0.129                 RMSE 0.213                 0.125 0.152 0.053 0.061\n",
      "[epoch: 10] Test KL loss: 0.220                    RMSE 0.307                     0.178 0.230 0.037 0.091\n",
      "\t\t\t\t\t\t\tR2 score: -0.015 0.148 -0.019 0.184 \n",
      "[epoch: 20] Train KL loss: 0.110                 RMSE 0.193                 0.114 0.136 0.053 0.053\n",
      "[epoch: 20] Test KL loss: 0.204                    RMSE 0.298                     0.168 0.225 0.037 0.093\n",
      "\t\t\t\t\t\t\tR2 score: 0.105 0.185 -0.023 0.141 \n",
      "[epoch: 30] Train KL loss: 0.106                 RMSE 0.188                 0.110 0.133 0.052 0.052\n",
      "[epoch: 30] Test KL loss: 0.204                    RMSE 0.297                     0.166 0.225 0.038 0.095\n",
      "\t\t\t\t\t\t\tR2 score: 0.125 0.187 -0.039 0.100 \n",
      "[epoch: 40] Train KL loss: 0.103                 RMSE 0.185                 0.109 0.131 0.052 0.051\n",
      "[epoch: 40] Test KL loss: 0.205                    RMSE 0.298                     0.166 0.225 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.124 0.184 -0.041 0.089 \n",
      "[epoch: 50] Train KL loss: 0.101                 RMSE 0.183                 0.107 0.129 0.052 0.051\n",
      "[epoch: 50] Test KL loss: 0.204                    RMSE 0.296                     0.165 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.137 0.196 -0.053 0.087 \n",
      "[epoch: 60] Train KL loss: 0.100                 RMSE 0.181                 0.105 0.128 0.052 0.051\n",
      "[epoch: 60] Test KL loss: 0.204                    RMSE 0.296                     0.164 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.138 0.195 -0.048 0.083 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 70] Train KL loss: 0.098                 RMSE 0.179                 0.104 0.126 0.052 0.050\n",
      "[epoch: 70] Test KL loss: 0.204                    RMSE 0.296                     0.165 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.136 0.195 -0.051 0.091 \n",
      "[epoch: 80] Train KL loss: 0.097                 RMSE 0.177                 0.103 0.125 0.051 0.050\n",
      "[epoch: 80] Test KL loss: 0.205                    RMSE 0.296                     0.164 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.139 0.193 -0.049 0.083 \n",
      "[epoch: 90] Train KL loss: 0.096                 RMSE 0.176                 0.102 0.124 0.051 0.050\n",
      "[epoch: 90] Test KL loss: 0.205                    RMSE 0.296                     0.165 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.137 0.194 -0.059 0.084 \n",
      "[epoch: 100] Train KL loss: 0.095                 RMSE 0.174                 0.101 0.123 0.051 0.049\n",
      "[epoch: 100] Test KL loss: 0.204                    RMSE 0.295                     0.164 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.144 0.201 -0.068 0.088 \n",
      "[epoch: 110] Train KL loss: 0.095                 RMSE 0.173                 0.100 0.122 0.051 0.049\n",
      "[epoch: 110] Test KL loss: 0.205                    RMSE 0.296                     0.164 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.138 0.196 -0.070 0.091 \n",
      "[epoch: 120] Train KL loss: 0.094                 RMSE 0.172                 0.100 0.122 0.051 0.049\n",
      "[epoch: 120] Test KL loss: 0.205                    RMSE 0.296                     0.165 0.223 0.039 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.137 0.197 -0.084 0.093 \n",
      "[epoch: 130] Train KL loss: 0.093                 RMSE 0.172                 0.099 0.121 0.051 0.048\n",
      "[epoch: 130] Test KL loss: 0.205                    RMSE 0.296                     0.165 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.136 0.192 -0.068 0.092 \n",
      "[epoch: 140] Train KL loss: 0.093                 RMSE 0.171                 0.099 0.121 0.051 0.048\n",
      "[epoch: 140] Test KL loss: 0.208                    RMSE 0.298                     0.166 0.225 0.039 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.124 0.183 -0.089 0.090 \n",
      "[epoch: 150] Train KL loss: 0.092                 RMSE 0.170                 0.098 0.120 0.051 0.048\n",
      "[epoch: 150] Test KL loss: 0.205                    RMSE 0.296                     0.164 0.223 0.039 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.139 0.196 -0.088 0.094 \n",
      "[epoch: 160] Train KL loss: 0.092                 RMSE 0.169                 0.098 0.119 0.051 0.048\n",
      "[epoch: 160] Test KL loss: 0.206                    RMSE 0.297                     0.165 0.224 0.038 0.095\n",
      "\t\t\t\t\t\t\tR2 score: 0.132 0.188 -0.075 0.096 \n",
      "[epoch: 170] Train KL loss: 0.091                 RMSE 0.169                 0.097 0.119 0.051 0.047\n",
      "[epoch: 170] Test KL loss: 0.204                    RMSE 0.295                     0.164 0.222 0.039 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.147 0.202 -0.095 0.096 \n",
      "[epoch: 180] Train KL loss: 0.091                 RMSE 0.168                 0.097 0.118 0.050 0.047\n",
      "[epoch: 180] Test KL loss: 0.206                    RMSE 0.297                     0.165 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.137 0.189 -0.076 0.093 \n",
      "[epoch: 190] Train KL loss: 0.090                 RMSE 0.167                 0.096 0.118 0.050 0.047\n",
      "[epoch: 190] Test KL loss: 0.206                    RMSE 0.296                     0.164 0.224 0.039 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.140 0.192 -0.096 0.086 \n",
      "[epoch: 200] Train KL loss: 0.090                 RMSE 0.166                 0.096 0.117 0.050 0.047\n",
      "[epoch: 200] Test KL loss: 0.204                    RMSE 0.295                     0.164 0.223 0.038 0.095\n",
      "\t\t\t\t\t\t\tR2 score: 0.145 0.197 -0.081 0.097 \n",
      "Early stopping at epoch 205\n",
      "[epoch:  0] Train KL loss: 0.650                 RMSE 0.591                 0.196 0.493 0.176 0.193\n",
      "[epoch:  0] Test KL loss: 0.297                    RMSE 0.359                     0.178 0.274 0.091 0.117\n",
      "\t\t\t\t\t\t\tR2 score: -0.005 -0.212 -5.027 -0.350 \n",
      "[epoch: 10] Train KL loss: 0.129                 RMSE 0.213                 0.124 0.152 0.053 0.062\n",
      "[epoch: 10] Test KL loss: 0.220                    RMSE 0.307                     0.178 0.230 0.038 0.091\n",
      "\t\t\t\t\t\t\tR2 score: -0.014 0.147 -0.045 0.176 \n",
      "[epoch: 20] Train KL loss: 0.110                 RMSE 0.193                 0.114 0.136 0.053 0.054\n",
      "[epoch: 20] Test KL loss: 0.202                    RMSE 0.297                     0.167 0.224 0.038 0.093\n",
      "\t\t\t\t\t\t\tR2 score: 0.112 0.191 -0.029 0.141 \n",
      "[epoch: 30] Train KL loss: 0.106                 RMSE 0.188                 0.110 0.133 0.052 0.052\n",
      "[epoch: 30] Test KL loss: 0.203                    RMSE 0.297                     0.166 0.224 0.038 0.095\n",
      "\t\t\t\t\t\t\tR2 score: 0.127 0.190 -0.045 0.103 \n",
      "[epoch: 40] Train KL loss: 0.103                 RMSE 0.185                 0.108 0.131 0.052 0.052\n",
      "[epoch: 40] Test KL loss: 0.205                    RMSE 0.298                     0.166 0.225 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.126 0.185 -0.042 0.090 \n",
      "[epoch: 50] Train KL loss: 0.101                 RMSE 0.182                 0.106 0.129 0.052 0.051\n",
      "[epoch: 50] Test KL loss: 0.203                    RMSE 0.296                     0.164 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.138 0.196 -0.048 0.088 \n",
      "[epoch: 60] Train KL loss: 0.100                 RMSE 0.180                 0.105 0.128 0.052 0.051\n",
      "[epoch: 60] Test KL loss: 0.205                    RMSE 0.297                     0.166 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.127 0.188 -0.053 0.089 \n",
      "[epoch: 70] Train KL loss: 0.098                 RMSE 0.179                 0.104 0.126 0.052 0.050\n",
      "[epoch: 70] Test KL loss: 0.206                    RMSE 0.298                     0.165 0.225 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.128 0.184 -0.045 0.083 \n",
      "[epoch: 80] Train KL loss: 0.097                 RMSE 0.177                 0.103 0.125 0.051 0.050\n",
      "[epoch: 80] Test KL loss: 0.204                    RMSE 0.295                     0.164 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.143 0.201 -0.066 0.086 \n",
      "[epoch: 90] Train KL loss: 0.096                 RMSE 0.176                 0.102 0.124 0.051 0.050\n",
      "[epoch: 90] Test KL loss: 0.205                    RMSE 0.296                     0.165 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.136 0.198 -0.074 0.093 \n",
      "[epoch: 100] Train KL loss: 0.095                 RMSE 0.175                 0.101 0.123 0.051 0.049\n",
      "[epoch: 100] Test KL loss: 0.206                    RMSE 0.297                     0.165 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.133 0.191 -0.066 0.088 \n",
      "[epoch: 110] Train KL loss: 0.095                 RMSE 0.174                 0.100 0.123 0.051 0.049\n",
      "[epoch: 110] Test KL loss: 0.206                    RMSE 0.297                     0.165 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.135 0.192 -0.068 0.087 \n",
      "[epoch: 120] Train KL loss: 0.094                 RMSE 0.173                 0.100 0.122 0.051 0.049\n",
      "[epoch: 120] Test KL loss: 0.204                    RMSE 0.295                     0.164 0.222 0.039 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.145 0.203 -0.090 0.091 \n",
      "[epoch: 130] Train KL loss: 0.093                 RMSE 0.171                 0.099 0.121 0.051 0.049\n",
      "[epoch: 130] Test KL loss: 0.205                    RMSE 0.296                     0.165 0.223 0.039 0.095\n",
      "\t\t\t\t\t\t\tR2 score: 0.135 0.196 -0.088 0.097 \n",
      "[epoch: 140] Train KL loss: 0.093                 RMSE 0.171                 0.098 0.120 0.051 0.048\n",
      "[epoch: 140] Test KL loss: 0.204                    RMSE 0.295                     0.164 0.223 0.038 0.095\n",
      "\t\t\t\t\t\t\tR2 score: 0.141 0.200 -0.079 0.103 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.549                 RMSE 0.534                 0.194 0.441 0.161 0.162\n",
      "[epoch:  0] Test KL loss: 0.295                    RMSE 0.356                     0.200 0.272 0.037 0.104\n",
      "\t\t\t\t\t\t\tR2 score: -0.278 -0.197 0.009 -0.068 \n",
      "[epoch: 10] Train KL loss: 0.118                 RMSE 0.202                 0.123 0.139 0.056 0.059\n",
      "[epoch: 10] Test KL loss: 0.212                    RMSE 0.295                     0.163 0.220 0.040 0.101\n",
      "\t\t\t\t\t\t\tR2 score: 0.155 0.217 -0.170 -0.008 \n",
      "[epoch: 20] Train KL loss: 0.105                 RMSE 0.188                 0.110 0.133 0.052 0.052\n",
      "[epoch: 20] Test KL loss: 0.205                    RMSE 0.297                     0.165 0.224 0.038 0.097\n",
      "\t\t\t\t\t\t\tR2 score: 0.138 0.187 -0.051 0.067 \n",
      "[epoch: 30] Train KL loss: 0.102                 RMSE 0.183                 0.107 0.129 0.052 0.051\n",
      "[epoch: 30] Test KL loss: 0.203                    RMSE 0.296                     0.164 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.140 0.198 -0.045 0.088 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 40] Train KL loss: 0.099                 RMSE 0.180                 0.105 0.127 0.052 0.050\n",
      "[epoch: 40] Test KL loss: 0.204                    RMSE 0.296                     0.165 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.137 0.197 -0.057 0.088 \n",
      "[epoch: 50] Train KL loss: 0.097                 RMSE 0.177                 0.103 0.125 0.051 0.050\n",
      "[epoch: 50] Test KL loss: 0.205                    RMSE 0.297                     0.165 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.128 0.188 -0.052 0.094 \n",
      "[epoch: 60] Train KL loss: 0.096                 RMSE 0.175                 0.102 0.124 0.051 0.049\n",
      "[epoch: 60] Test KL loss: 0.205                    RMSE 0.296                     0.164 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.140 0.192 -0.052 0.083 \n",
      "[epoch: 70] Train KL loss: 0.094                 RMSE 0.173                 0.101 0.122 0.051 0.049\n",
      "[epoch: 70] Test KL loss: 0.209                    RMSE 0.299                     0.166 0.226 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.118 0.177 -0.068 0.086 \n",
      "[epoch: 80] Train KL loss: 0.093                 RMSE 0.172                 0.099 0.121 0.051 0.049\n",
      "[epoch: 80] Test KL loss: 0.208                    RMSE 0.298                     0.165 0.225 0.038 0.097\n",
      "\t\t\t\t\t\t\tR2 score: 0.131 0.182 -0.072 0.073 \n",
      "[epoch: 90] Train KL loss: 0.092                 RMSE 0.170                 0.099 0.120 0.051 0.048\n",
      "[epoch: 90] Test KL loss: 0.206                    RMSE 0.297                     0.165 0.224 0.038 0.095\n",
      "\t\t\t\t\t\t\tR2 score: 0.130 0.188 -0.067 0.101 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.563                 RMSE 0.541                 0.192 0.448 0.164 0.168\n",
      "[epoch:  0] Test KL loss: 0.292                    RMSE 0.354                     0.200 0.271 0.037 0.103\n",
      "\t\t\t\t\t\t\tR2 score: -0.271 -0.181 0.005 -0.054 \n",
      "[epoch: 10] Train KL loss: 0.118                 RMSE 0.202                 0.122 0.139 0.056 0.059\n",
      "[epoch: 10] Test KL loss: 0.213                    RMSE 0.297                     0.164 0.223 0.039 0.101\n",
      "\t\t\t\t\t\t\tR2 score: 0.141 0.199 -0.133 -0.002 \n",
      "[epoch: 20] Train KL loss: 0.105                 RMSE 0.188                 0.110 0.133 0.052 0.053\n",
      "[epoch: 20] Test KL loss: 0.204                    RMSE 0.296                     0.164 0.223 0.038 0.097\n",
      "\t\t\t\t\t\t\tR2 score: 0.147 0.195 -0.054 0.065 \n",
      "[epoch: 30] Train KL loss: 0.101                 RMSE 0.183                 0.107 0.129 0.052 0.051\n",
      "[epoch: 30] Test KL loss: 0.203                    RMSE 0.295                     0.164 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.139 0.199 -0.045 0.092 \n",
      "[epoch: 40] Train KL loss: 0.099                 RMSE 0.180                 0.105 0.127 0.052 0.050\n",
      "[epoch: 40] Test KL loss: 0.206                    RMSE 0.297                     0.166 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.127 0.187 -0.056 0.086 \n",
      "[epoch: 50] Train KL loss: 0.097                 RMSE 0.177                 0.103 0.125 0.051 0.050\n",
      "[epoch: 50] Test KL loss: 0.206                    RMSE 0.297                     0.165 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.134 0.190 -0.056 0.081 \n",
      "[epoch: 60] Train KL loss: 0.095                 RMSE 0.175                 0.101 0.123 0.051 0.049\n",
      "[epoch: 60] Test KL loss: 0.206                    RMSE 0.297                     0.165 0.225 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.134 0.186 -0.049 0.081 \n",
      "[epoch: 70] Train KL loss: 0.094                 RMSE 0.173                 0.101 0.122 0.051 0.049\n",
      "[epoch: 70] Test KL loss: 0.206                    RMSE 0.296                     0.165 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.133 0.194 -0.081 0.095 \n",
      "[epoch: 80] Train KL loss: 0.093                 RMSE 0.172                 0.100 0.121 0.051 0.048\n",
      "[epoch: 80] Test KL loss: 0.207                    RMSE 0.297                     0.165 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.131 0.187 -0.072 0.089 \n",
      "[epoch: 90] Train KL loss: 0.092                 RMSE 0.171                 0.099 0.120 0.051 0.048\n",
      "[epoch: 90] Test KL loss: 0.206                    RMSE 0.296                     0.164 0.224 0.039 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.141 0.194 -0.091 0.082 \n",
      "[epoch: 100] Train KL loss: 0.091                 RMSE 0.169                 0.098 0.119 0.051 0.048\n",
      "[epoch: 100] Test KL loss: 0.209                    RMSE 0.298                     0.166 0.226 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.127 0.178 -0.080 0.081 \n",
      "[epoch: 110] Train KL loss: 0.091                 RMSE 0.168                 0.098 0.119 0.050 0.047\n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.540                 RMSE 0.529                 0.191 0.437 0.157 0.167\n",
      "[epoch:  0] Test KL loss: 0.291                    RMSE 0.354                     0.199 0.272 0.037 0.103\n",
      "\t\t\t\t\t\t\tR2 score: -0.265 -0.190 0.024 -0.061 \n",
      "[epoch: 10] Train KL loss: 0.117                 RMSE 0.203                 0.124 0.138 0.055 0.060\n",
      "[epoch: 10] Test KL loss: 0.213                    RMSE 0.297                     0.164 0.224 0.038 0.101\n",
      "\t\t\t\t\t\t\tR2 score: 0.146 0.194 -0.054 -0.009 \n",
      "[epoch: 20] Train KL loss: 0.105                 RMSE 0.187                 0.110 0.132 0.052 0.052\n",
      "[epoch: 20] Test KL loss: 0.203                    RMSE 0.296                     0.164 0.223 0.038 0.097\n",
      "\t\t\t\t\t\t\tR2 score: 0.148 0.197 -0.041 0.074 \n",
      "[epoch: 30] Train KL loss: 0.101                 RMSE 0.183                 0.107 0.129 0.052 0.051\n",
      "[epoch: 30] Test KL loss: 0.204                    RMSE 0.296                     0.165 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.137 0.195 -0.044 0.089 \n",
      "[epoch: 40] Train KL loss: 0.099                 RMSE 0.179                 0.105 0.127 0.052 0.050\n",
      "[epoch: 40] Test KL loss: 0.205                    RMSE 0.297                     0.165 0.224 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.129 0.191 -0.058 0.093 \n",
      "[epoch: 50] Train KL loss: 0.097                 RMSE 0.177                 0.103 0.125 0.051 0.050\n",
      "[epoch: 50] Test KL loss: 0.208                    RMSE 0.299                     0.166 0.226 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.121 0.179 -0.057 0.079 \n",
      "[epoch: 60] Train KL loss: 0.095                 RMSE 0.175                 0.101 0.123 0.051 0.049\n",
      "[epoch: 60] Test KL loss: 0.205                    RMSE 0.295                     0.164 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.143 0.199 -0.075 0.083 \n",
      "[epoch: 70] Train KL loss: 0.094                 RMSE 0.173                 0.100 0.122 0.051 0.049\n",
      "[epoch: 70] Test KL loss: 0.206                    RMSE 0.296                     0.165 0.223 0.039 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.133 0.195 -0.094 0.093 \n",
      "[epoch: 80] Train KL loss: 0.093                 RMSE 0.172                 0.099 0.121 0.051 0.048\n",
      "[epoch: 80] Test KL loss: 0.204                    RMSE 0.296                     0.164 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.142 0.197 -0.068 0.095 \n",
      "[epoch: 90] Train KL loss: 0.092                 RMSE 0.170                 0.098 0.120 0.051 0.048\n",
      "[epoch: 90] Test KL loss: 0.207                    RMSE 0.297                     0.165 0.224 0.039 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.132 0.187 -0.083 0.087 \n",
      "[epoch: 100] Train KL loss: 0.091                 RMSE 0.169                 0.098 0.119 0.051 0.047\n",
      "[epoch: 100] Test KL loss: 0.206                    RMSE 0.296                     0.165 0.224 0.039 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.134 0.194 -0.105 0.095 \n",
      "[epoch: 110] Train KL loss: 0.091                 RMSE 0.168                 0.097 0.118 0.050 0.047\n",
      "[epoch: 110] Test KL loss: 0.205                    RMSE 0.296                     0.165 0.223 0.039 0.095\n",
      "\t\t\t\t\t\t\tR2 score: 0.138 0.194 -0.092 0.100 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.543                 RMSE 0.531                 0.192 0.440 0.154 0.167\n",
      "[epoch:  0] Test KL loss: 0.293                    RMSE 0.355                     0.200 0.272 0.037 0.103\n",
      "\t\t\t\t\t\t\tR2 score: -0.275 -0.196 0.026 -0.056 \n",
      "[epoch: 10] Train KL loss: 0.116                 RMSE 0.202                 0.123 0.139 0.055 0.058\n",
      "[epoch: 10] Test KL loss: 0.209                    RMSE 0.295                     0.163 0.221 0.039 0.100\n",
      "\t\t\t\t\t\t\tR2 score: 0.152 0.212 -0.101 0.017 \n",
      "[epoch: 20] Train KL loss: 0.105                 RMSE 0.189                 0.111 0.134 0.052 0.052\n",
      "[epoch: 20] Test KL loss: 0.207                    RMSE 0.299                     0.166 0.226 0.038 0.097\n",
      "\t\t\t\t\t\t\tR2 score: 0.124 0.173 -0.035 0.070 \n",
      "[epoch: 30] Train KL loss: 0.102                 RMSE 0.183                 0.107 0.129 0.052 0.051\n",
      "[epoch: 30] Test KL loss: 0.203                    RMSE 0.296                     0.165 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.136 0.195 -0.043 0.091 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 40] Train KL loss: 0.099                 RMSE 0.181                 0.106 0.127 0.052 0.050\n",
      "[epoch: 40] Test KL loss: 0.209                    RMSE 0.301                     0.168 0.228 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.104 0.163 -0.032 0.086 \n",
      "[epoch: 50] Train KL loss: 0.097                 RMSE 0.177                 0.103 0.125 0.051 0.050\n",
      "[epoch: 50] Test KL loss: 0.206                    RMSE 0.297                     0.165 0.224 0.039 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.133 0.194 -0.083 0.084 \n",
      "[epoch: 60] Train KL loss: 0.096                 RMSE 0.175                 0.102 0.124 0.051 0.049\n",
      "[epoch: 60] Test KL loss: 0.204                    RMSE 0.295                     0.164 0.222 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.143 0.202 -0.073 0.095 \n",
      "[epoch: 70] Train KL loss: 0.094                 RMSE 0.174                 0.101 0.122 0.051 0.049\n",
      "[epoch: 70] Test KL loss: 0.203                    RMSE 0.294                     0.163 0.221 0.039 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.155 0.211 -0.085 0.088 \n",
      "[epoch: 80] Train KL loss: 0.093                 RMSE 0.172                 0.100 0.121 0.051 0.048\n",
      "[epoch: 80] Test KL loss: 0.207                    RMSE 0.298                     0.166 0.225 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.125 0.184 -0.072 0.095 \n",
      "[epoch: 90] Train KL loss: 0.092                 RMSE 0.171                 0.099 0.120 0.051 0.048\n",
      "[epoch: 90] Test KL loss: 0.204                    RMSE 0.294                     0.163 0.222 0.039 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.149 0.205 -0.100 0.089 \n",
      "[epoch: 100] Train KL loss: 0.092                 RMSE 0.169                 0.098 0.119 0.051 0.047\n",
      "[epoch: 100] Test KL loss: 0.204                    RMSE 0.295                     0.164 0.223 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.147 0.200 -0.080 0.095 \n",
      "[epoch: 110] Train KL loss: 0.091                 RMSE 0.168                 0.097 0.118 0.050 0.047\n",
      "[epoch: 110] Test KL loss: 0.207                    RMSE 0.297                     0.165 0.225 0.039 0.095\n",
      "\t\t\t\t\t\t\tR2 score: 0.129 0.185 -0.083 0.098 \n",
      "[epoch: 120] Train KL loss: 0.090                 RMSE 0.167                 0.097 0.118 0.050 0.047\n",
      "[epoch: 120] Test KL loss: 0.206                    RMSE 0.296                     0.164 0.223 0.039 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.141 0.195 -0.110 0.091 \n",
      "[epoch: 130] Train KL loss: 0.090                 RMSE 0.167                 0.097 0.118 0.050 0.047\n",
      "Diverging. stop.\n"
     ]
    }
   ],
   "source": [
    "wd_list = [0.00005,0.0001,0.0005,0.001]\n",
    "lr_list = [0.005, 0.01]\n",
    "\n",
    "for (lr, wd) in itertools.product(lr_list, wd_list):\n",
    "    # model setup\n",
    "    model = mnl.MNL(n_alts=4, n_features=x_train.shape[-1])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    # model training\n",
    "\n",
    "    ref1 = 0\n",
    "    ref2 = 0\n",
    "\n",
    "    for epoch in range(400):\n",
    "\n",
    "        kl_ = 0\n",
    "        mse_ = 0\n",
    "        mse1_ = 0\n",
    "        mse2_ = 0\n",
    "        mse3_ = 0\n",
    "        mse4_ = 0\n",
    "\n",
    "        for batch, (x_batch, y_batch) in enumerate(trainloader):\n",
    "            # Compute prediction and loss\n",
    "            util = model(x_batch)\n",
    "            probs = torch.log(nn.functional.softmax(util, dim=1))\n",
    "            kl = kldivloss(probs, y_batch)\n",
    "    #         kl = kldivloss(torch.log(util), y_batch)\n",
    "            kl_ += kl.item()\n",
    "\n",
    "            mse = mseloss(torch.exp(probs), y_batch)\n",
    "    #         mse = mseloss(util, y_batch)\n",
    "            mse_ += mse.sum().item()\n",
    "            mse1_ += mse[:,0].sum().item()\n",
    "            mse2_ += mse[:,1].sum().item()\n",
    "            mse3_ += mse[:,2].sum().item()\n",
    "            mse4_ += mse[:,3].sum().item()\n",
    "            mse = mse.sum()\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            kl.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_kl = kl_/len(trainset)\n",
    "        train_mse = np.sqrt(mse_/len(trainset))\n",
    "        train_mse1 = np.sqrt(mse1_/len(trainset))\n",
    "        train_mse2 = np.sqrt(mse2_/len(trainset))\n",
    "        train_mse3 = np.sqrt(mse3_/len(trainset))\n",
    "        train_mse4 = np.sqrt(mse4_/len(trainset))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[epoch: {epoch:>2d}] Train KL loss: {train_kl:.3f} \\\n",
    "                RMSE {train_mse:.3f} \\\n",
    "                {train_mse1:.3f} {train_mse2:.3f} {train_mse3:.3f} {train_mse4:.3f}\")\n",
    "        loss_ = train_kl\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            if epoch > 50:\n",
    "                if (np.abs(loss_ - ref1)/ref1<ref1*0.01) & (np.abs(loss_ - ref2)/ref2<ref2*0.01):\n",
    "                    print(\"Early stopping at epoch\", epoch)\n",
    "                    break\n",
    "                if (ref1 < loss_) & (ref1 < ref2):\n",
    "                    print(\"Diverging. stop.\")\n",
    "                    break\n",
    "                if loss_ < best:\n",
    "                    best = loss_\n",
    "                    best_epoch = epoch\n",
    "            else:\n",
    "                best = loss_\n",
    "                best_epoch = epoch\n",
    "\n",
    "            ref2 = ref1\n",
    "            ref1 = loss_\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "\n",
    "            kl_ = 0\n",
    "            mse_ = 0 \n",
    "            mse1_ = 0\n",
    "            mse2_ = 0\n",
    "            mse3_ = 0\n",
    "            mse4_ = 0\n",
    "\n",
    "            for batch, (x_batch, y_batch) in enumerate(testloader):\n",
    "                util = model(x_batch)\n",
    "                probs = torch.log(nn.functional.softmax(util,dim=1))\n",
    "                kl = kldivloss(probs, y_batch)\n",
    "        #         kl = kldivloss(torch.log(util), y_batch)\n",
    "                kl_ += kl.item()\n",
    "\n",
    "                mse = mseloss(torch.exp(probs), y_batch)\n",
    "        #         mse = mseloss(util, y_batch)\n",
    "                mse_ += mse.sum().item()\n",
    "                mse1_ += mse[:,0].sum().item()\n",
    "                mse2_ += mse[:,1].sum().item()\n",
    "                mse3_ += mse[:,2].sum().item()\n",
    "                mse4_ += mse[:,3].sum().item()\n",
    "\n",
    "            test_kl = kl_/len(testset)\n",
    "            test_mse = np.sqrt(mse_/len(testset))\n",
    "            test_mse1 = np.sqrt(mse1_/len(testset))\n",
    "            test_mse2 = np.sqrt(mse2_/len(testset))\n",
    "            test_mse3 = np.sqrt(mse3_/len(testset))\n",
    "            test_mse4 = np.sqrt(mse4_/len(testset))\n",
    "\n",
    "            r1 = r2_score(y_batch.numpy()[:,0],torch.exp(probs).detach().numpy()[:,0])\n",
    "            r2 = r2_score(y_batch.numpy()[:,1],torch.exp(probs).detach().numpy()[:,1])\n",
    "            r3 = r2_score(y_batch.numpy()[:,2],torch.exp(probs).detach().numpy()[:,2])\n",
    "            r4 = r2_score(y_batch.numpy()[:,3],torch.exp(probs).detach().numpy()[:,3])\n",
    "\n",
    "            print(f\"[epoch: {epoch:>2d}] Test KL loss: {kl_/len(testset):.3f}\\\n",
    "                    RMSE {np.sqrt(mse_/len(testset)):.3f} \\\n",
    "                    {np.sqrt(mse1_/len(testset)):.3f} {np.sqrt(mse2_/len(testset)):.3f} {np.sqrt(mse3_/len(testset)):.3f} {np.sqrt(mse4_/len(testset)):.3f}\")\n",
    "            print(f\"\\t\\t\\t\\t\\t\\t\\tR2 score: {r1:.3f} {r2:.3f} {r3:.3f} {r4:.3f} \")\n",
    "\n",
    "\n",
    "    with open(out_dir+model_code+\"_mode_choice.csv\", \"a\") as f:\n",
    "        f.write(\"%s,%s,%s,%s,%.4f,%d,%.5f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f\\n\" % \\\n",
    "            (model_run_date, model_type, zoomlevel, \"MNL\", lr, -1, wd, \n",
    "              train_kl, train_mse, train_mse1, train_mse2, train_mse3, train_mse4,\n",
    "              test_kl, test_mse, test_mse1, test_mse2, test_mse3, test_mse4,\n",
    "              r1, r2, r3, r4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
    "for i in range(4):\n",
    "    ax.scatter(y_batch.detach().numpy()[:,i], torch.exp(probs).detach().numpy()[:,i], s=10)\n",
    "\n",
    "ax.set_xlabel(\"True Probs\")\n",
    "ax.set_ylabel(\"Predicted Probs\")\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_ylim([0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression / Logistic Regression for Mode Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03861788072428318 0.05915592752590282 -0.015364276970998736 0.04173291554608982\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = \"LR\"\n",
    "# 1->L1; 0->L2\n",
    "regularization = 0\n",
    "reg_parameter = 2\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(20,4))\n",
    "\n",
    "pred_train = []\n",
    "pred_test = []\n",
    "train_mse_lr = []\n",
    "train_r_lr = []\n",
    "test_mse_lr = []\n",
    "test_r_lr = []\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "    y_train_lr = y[~train_test_index,i]\n",
    "    y_test_lr = y[train_test_index,i]\n",
    "\n",
    "    if model == \"LR\":\n",
    "    # Normal Linear Regression\n",
    "        lr = linear_model.Ridge(alpha=reg_parameter)\n",
    "        lr.fit(x_train, y_train_lr)\n",
    "\n",
    "    elif model == \"GLM\":\n",
    "    # GLM with logit link (ensure probs are \\in [0,1])\n",
    "        lr = sm.GLM(y_train_lr, x_train, family=sm.families.Binomial())\n",
    "        lr = lr.fit_regularized(alpha=reg_parameter, L1_wt=regularization)\n",
    "    else:\n",
    "        print(\"ERROR\")\n",
    "        break\n",
    "        \n",
    "    ax[i].scatter(y_train_lr, lr.predict(x_train), s=5)\n",
    "    ax[i].scatter(y_test_lr, lr.predict(x_test), s=5)\n",
    "    ax[i].plot([0, max(y_train_lr)],[0, max(y_train_lr)])\n",
    "\n",
    "    pred_train.append(lr.predict(x_train))\n",
    "    pred_test.append(lr.predict(x_test)) \n",
    "\n",
    "    train_mse_lr.append(np.sqrt(mean_squared_error(lr.predict(x_train), y_train_lr)))\n",
    "    train_r_lr.append(r2_score(y_train_lr, lr.predict(x_train)))\n",
    "    test_mse_lr.append(np.sqrt(mean_squared_error(lr.predict(x_test), y_test_lr)))\n",
    "    test_r_lr.append(r2_score(y_test_lr, lr.predict(x_test)))\n",
    "\n",
    "pred_train = np.array(pred_train).T\n",
    "pred_test = np.array(pred_test).T\n",
    "\n",
    "train_mse = np.sqrt(mean_squared_error(pred_train, y[~train_test_index,:4]))\n",
    "test_mse = np.sqrt(mean_squared_error(pred_test, y[train_test_index,:4]))\n",
    "\n",
    "# the entropy function will normalize the probabilities\n",
    "# not suitable for probabilities gotten from linear regression and GLM\n",
    "\n",
    "# train_kl = entropy(y[~train_test_index,:4], pred_train, axis=1).mean()\n",
    "# test_kl = entropy(y[train_test_index,:4], pred_test, axis=1).mean()\n",
    "\n",
    "print(test_r_lr[0], test_r_lr[1], test_r_lr[2], test_r_lr[3])\n",
    "                                                                                                     \n",
    "fig, ax = plt.subplots(1,2, figsize=(6,3))\n",
    "ax[0].hist(pred_train.sum(axis=1), bins = np.linspace(0.875, 1.275, 9))\n",
    "ax[1].hist(pred_test.sum(axis=1), bins = np.linspace(0.875, 1.275, 9));\n",
    "fig \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_dir+model_code+\"_mode_choice.csv\", \"a\") as f:\n",
    "    f.write(\"%s,%s,%s,%s,%d,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f\\n\" % \\\n",
    "                      (model_run_date, model_type, zoomlevel, model, regularization, reg_parameter,\n",
    "                      -1, train_mse, train_mse_lr[0], train_mse_lr[1], train_mse_lr[2], train_mse_lr[3],\n",
    "                      -1, test_mse, test_mse_lr[0], test_mse_lr[1], test_mse_lr[2], test_mse_lr[3],\n",
    "                      test_r_lr[0], test_r_lr[1], test_r_lr[2], test_r_lr[3])) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR for trip generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trpgen_train =  y[~train_test_index,1]\n",
    "trpgen_test =  y[train_test_index,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999977559361426 -1.1354757925689754\n"
     ]
    }
   ],
   "source": [
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(x_train, trpgen_train)\n",
    "with open(out_dir+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "    f.write(\"%s,%s,%s,%.4f,,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], -1, \n",
    "        lr.score(x_train, trpgen_train), lr.score(x_test, trpgen_test), 'lr', zoomlevel,\n",
    "        np.sum(lr.coef_ != 0), len(lr.coef_)))\n",
    "print(lr.score(x_train, trpgen_train), lr.score(x_test, trpgen_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.595524044110789e-14 -0.022658976492462957\n",
      "-5.595524044110789e-14 -0.022658976492462957\n",
      "-5.595524044110789e-14 -0.022658976492462957\n",
      "-5.595524044110789e-14 -0.022658976492462957\n",
      "-5.595524044110789e-14 -0.022658976492462957\n",
      "-5.595524044110789e-14 -0.022658976492462957\n",
      "-5.595524044110789e-14 -0.022658976492462957\n",
      "-5.595524044110789e-14 -0.022658976492462957\n",
      "-5.595524044110789e-14 -0.022658976492462957\n",
      "-5.595524044110789e-14 -0.022658976492462957\n"
     ]
    }
   ],
   "source": [
    "for a in np.linspace(0.005, 0.014, 10):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, trpgen_train)\n",
    "    with open(out_dir+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "        f.write(\"%s,%s,%s,%.6f,,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "            lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test), 'lasso', zoomlevel,\n",
    "            np.sum(lasso.coef_ != 0), len(lasso.coef_)))\n",
    "    print(lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34874319703756884 0.09794299931801165\n",
      "0.30471725338388667 0.08127266568118652\n",
      "0.2706321656368852 0.06884692964830619\n",
      "0.24341074185642464 0.05915592752590282\n",
      "0.22115700177747188 0.05135860168579243\n",
      "0.20262236958808044 0.04493742259766953\n",
      "0.18694648787762935 0.03955192198670132\n",
      "0.1735159951352866 0.03496739198629484\n",
      "0.1618813615704915 0.03101582568258443\n",
      "0.15170552372717627 0.02757388114271553\n"
     ]
    }
   ],
   "source": [
    "for a in np.linspace(1,4,10):\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, trpgen_train)\n",
    "    with open(out_dir+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "        f.write(\"%s,%s,%s,%.4f,,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "            ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "            np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
