{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook documents part 2 of the complementarity of image and demographic information: the ability of latent space extracted from Autoencoders to predict mode choice and trip generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"models/\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "from dataloader import SurveyDataset, load_aggregate_travel_behavior, load_demo\n",
    "from M1_util_train_test import load_model, test\n",
    "import linear_reg\n",
    "import mnl\n",
    "from setup import out_dir, data_dir, image_dir, model_dir, proj_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_version = '1571'\n",
    "\n",
    "model_type = 'AE'\n",
    "model_code = 'BM1_A1'\n",
    "sampling = 's'\n",
    "\n",
    "zoomlevel = 'zoom13'\n",
    "output_dim = 1\n",
    "model_run_date = '220211'\n",
    "\n",
    "variable_names = ['active','auto','mas','pt', 'trpgen']\n",
    "\n",
    "demo_variables = ['tot_population','pct25_34yrs','pct35_50yrs','pctover65yrs',\n",
    "         'pctwhite_alone','pct_nonwhite','pctblack_alone',\n",
    "         'pct_col_grad','avg_tt_to_work','inc_per_capita']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Load Model Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proj_dir+\"latent_space/\"+model_type+\"_\"+zoomlevel+\"_\"+str(output_dim**2*2048)+\"_\"+\n",
    "                       model_run_date+\".pkl\", \"rb\") as f: \n",
    "    encoder_output = pkl.load(f)\n",
    "    im = pkl.load(f)\n",
    "    ct = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Embeddings\n",
    "unique_ct = list(set(ct))\n",
    "unique_ct.sort()\n",
    "ct = np.array(ct)\n",
    "aggregate_embeddings = []\n",
    "for i in unique_ct:\n",
    "    aggregate_embeddings.append(np.mean(encoder_output[ct == i], axis=0))\n",
    "aggregate_embeddings = np.array(aggregate_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Trip Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"origin_trip_behavior.csv\"\n",
    "df_pivot = load_aggregate_travel_behavior(file, data_version)\n",
    "\n",
    "train_test_index = df_pivot['train_test'].astype(bool).to_numpy()\n",
    "# train_test_index = np.random.rand(len(df_pivot)) < 0.2\n",
    "\n",
    "y = df_pivot[variable_names].to_numpy()\n",
    "y_train = y[~train_test_index,:4]\n",
    "y_test = y[train_test_index,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = aggregate_embeddings[~train_test_index, :]\n",
    "x_test = aggregate_embeddings[train_test_index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Auto Share\n",
    "Highlight:\n",
    "\n",
    "Auto Lasso\n",
    "Parameter: 4.00e-06 Train R2: 0.6226 \t Test R: 0.6770 \t Nonzero coef: 81\n",
    "\n",
    "Auto Ridge\n",
    "Parameter: 2.00e-03 Train R2: 0.6554 \t Test R: 0.6694\n",
    "\n",
    "PT Lasso\n",
    "Parameter: 6.00e-06 Train R2: 0.4318 \t Test R: 0.4435 \t Nonzero coef: 22\n",
    "\n",
    "PT Ridge\n",
    "Parameter: 5.00e-03 Train R2: 0.5089 \t Test R: 0.4509\n",
    "\n",
    "Active Lasso\n",
    "Parameter: 3.00e-06 Train R2: 0.5354 \t Test R: 0.5541 \t Nonzero coef: 85\n",
    "\n",
    "Active Ridge\n",
    "Parameter: 3.00e-03 Train R2: 0.5592 \t Test R: 0.5524\n",
    "\n",
    "Trip Gen Lasso\n",
    "Parameter: 1.00e-03 Train R2: 0.0400 \t Test R: 0.0301 \t Nonzero coef: 13\n",
    "\n",
    "Trip Gen Ridge\n",
    "Parameter: 5.00e-02 Train R2: 0.0555 \t Test R: 0.0162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_train = y[~train_test_index,1]\n",
    "auto_test = y[train_test_index,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 1.0000 \t Test R2: -0.4400\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression without Regularization\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(x_train, auto_train)\n",
    "# with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#     f.write(\"%s,%s,%s,%.4f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], -1, \n",
    "#         lr.score(x_train, auto_train), lr.score(x_test, auto_test), 'lr', zoomlevel,\n",
    "#         np.sum(lr.coef_ != 0), len(lr.coef_)))\n",
    "print(\"Train R2: %.4f \\t Test R2: %.4f\" % (lr.score(x_train, auto_train), lr.score(x_test, auto_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  after removing the cwd from sys.path.\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.156e-02, tolerance: 7.704e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.9984 \t Test R: -0.7392 \t Nonzero coef: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.783e+00, tolerance: 7.704e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 1.00e-07 Train R2: 0.9454 \t Test R: 0.2914 \t Nonzero coef: 1132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.851e+00, tolerance: 7.704e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 1.00e-06 Train R2: 0.7122 \t Test R: 0.6494 \t Nonzero coef: 282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.945e-02, tolerance: 7.704e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 2.00e-06 Train R2: 0.6604 \t Test R: 0.6727 \t Nonzero coef: 156\n",
      "Parameter: 3.00e-06 Train R2: 0.6376 \t Test R: 0.6763 \t Nonzero coef: 114\n",
      "Parameter: 4.00e-06 Train R2: 0.6226 \t Test R: 0.6770 \t Nonzero coef: 81\n",
      "Parameter: 5.00e-06 Train R2: 0.6134 \t Test R: 0.6752 \t Nonzero coef: 63\n",
      "Parameter: 6.00e-06 Train R2: 0.6055 \t Test R: 0.6717 \t Nonzero coef: 53\n",
      "Parameter: 7.00e-06 Train R2: 0.5982 \t Test R: 0.6666 \t Nonzero coef: 48\n",
      "Parameter: 8.00e-06 Train R2: 0.5917 \t Test R: 0.6608 \t Nonzero coef: 39\n",
      "Parameter: 1.00e-05 Train R2: 0.5826 \t Test R: 0.6534 \t Nonzero coef: 31\n",
      "Parameter: 2.00e-05 Train R2: 0.5549 \t Test R: 0.6273 \t Nonzero coef: 14\n",
      "Parameter: 5.00e-05 Train R2: 0.4927 \t Test R: 0.5396 \t Nonzero coef: 7\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "for a in (1e-6)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, auto_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d\" % (a, lasso.score(x_train, auto_train), \n",
    "                                                                                  lasso.score(x_test, auto_test), \n",
    "                                                                                  np.sum(lasso.coef_ != 0)))\n",
    "\n",
    "#     with open(out_dir+\"BA_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%.6f,%.4f,%.4f,%s,%d,%d\\n\" % (a, \n",
    "#             lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test), 'lasso', \n",
    "#             np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:197: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  \"Singular matrix in solving dual problem. Using \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.9959 \t Test R: -0.4623\n",
      "Parameter: 1.00e-04 Train R2: 0.7900 \t Test R: 0.6202\n",
      "Parameter: 1.00e-03 Train R2: 0.6796 \t Test R: 0.6655\n",
      "Parameter: 2.00e-03 Train R2: 0.6554 \t Test R: 0.6694\n",
      "Parameter: 3.00e-03 Train R2: 0.6421 \t Test R: 0.6693\n",
      "Parameter: 4.00e-03 Train R2: 0.6330 \t Test R: 0.6678\n",
      "Parameter: 5.00e-03 Train R2: 0.6260 \t Test R: 0.6659\n",
      "Parameter: 6.00e-03 Train R2: 0.6203 \t Test R: 0.6636\n",
      "Parameter: 7.00e-03 Train R2: 0.6155 \t Test R: 0.6612\n",
      "Parameter: 8.00e-03 Train R2: 0.6112 \t Test R: 0.6586\n",
      "Parameter: 1.00e-02 Train R2: 0.6039 \t Test R: 0.6534\n",
      "Parameter: 2.00e-02 Train R2: 0.5776 \t Test R: 0.6271\n",
      "Parameter: 5.00e-02 Train R2: 0.5246 \t Test R: 0.5617\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "\n",
    "for a in (1e-3)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, auto_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, auto_train), \n",
    "                                                              ridge.score(x_test, auto_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  import sys\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.719e-02, tolerance: 1.373e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.9960 \t Test R: -1.8425 \t Nonzero coef: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.159e-01, tolerance: 1.373e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 1.00e-07 Train R2: 0.8526 \t Test R: 0.2409 \t Nonzero coef: 799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.311e-03, tolerance: 1.373e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 1.00e-06 Train R2: 0.5404 \t Test R: 0.4324 \t Nonzero coef: 161\n",
      "Parameter: 2.00e-06 Train R2: 0.4969 \t Test R: 0.4397 \t Nonzero coef: 81\n",
      "Parameter: 3.00e-06 Train R2: 0.4728 \t Test R: 0.4403 \t Nonzero coef: 59\n",
      "Parameter: 4.00e-06 Train R2: 0.4532 \t Test R: 0.4408 \t Nonzero coef: 42\n",
      "Parameter: 5.00e-06 Train R2: 0.4409 \t Test R: 0.4423 \t Nonzero coef: 29\n",
      "Parameter: 6.00e-06 Train R2: 0.4318 \t Test R: 0.4435 \t Nonzero coef: 22\n",
      "Parameter: 7.00e-06 Train R2: 0.4260 \t Test R: 0.4408 \t Nonzero coef: 17\n",
      "Parameter: 8.00e-06 Train R2: 0.4215 \t Test R: 0.4387 \t Nonzero coef: 16\n",
      "Parameter: 1.00e-05 Train R2: 0.4121 \t Test R: 0.4336 \t Nonzero coef: 14\n",
      "Parameter: 2.00e-05 Train R2: 0.3690 \t Test R: 0.3912 \t Nonzero coef: 7\n",
      "Parameter: 5.00e-05 Train R2: 0.2584 \t Test R: 0.2642 \t Nonzero coef: 3\n"
     ]
    }
   ],
   "source": [
    "pt_train = y[~train_test_index,3]\n",
    "pt_test = y[train_test_index,3]\n",
    "\n",
    "# Lasso\n",
    "for a in (1e-6)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, pt_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d\" % (a, lasso.score(x_train, pt_train), \n",
    "                                                                                  lasso.score(x_test, pt_test), \n",
    "                                                                                  np.sum(lasso.coef_ != 0)))\n",
    "\n",
    "#     with open(out_dir+\"BA_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%.6f,%.4f,%.4f,%s,%d,%d\\n\" % (a, \n",
    "#             lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test), 'lasso', \n",
    "#             np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:197: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  \"Singular matrix in solving dual problem. Using \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.9838 \t Test R: -1.5064\n",
      "Parameter: 1.00e-04 Train R2: 0.7145 \t Test R: 0.3986\n",
      "Parameter: 1.00e-03 Train R2: 0.5733 \t Test R: 0.4392\n",
      "Parameter: 2.00e-03 Train R2: 0.5437 \t Test R: 0.4469\n",
      "Parameter: 3.00e-03 Train R2: 0.5278 \t Test R: 0.4495\n",
      "Parameter: 4.00e-03 Train R2: 0.5170 \t Test R: 0.4506\n",
      "Parameter: 5.00e-03 Train R2: 0.5089 \t Test R: 0.4509\n",
      "Parameter: 6.00e-03 Train R2: 0.5024 \t Test R: 0.4508\n",
      "Parameter: 7.00e-03 Train R2: 0.4970 \t Test R: 0.4505\n",
      "Parameter: 8.00e-03 Train R2: 0.4923 \t Test R: 0.4500\n",
      "Parameter: 1.00e-02 Train R2: 0.4846 \t Test R: 0.4488\n",
      "Parameter: 2.00e-02 Train R2: 0.4597 \t Test R: 0.4398\n",
      "Parameter: 5.00e-02 Train R2: 0.4185 \t Test R: 0.4089\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "\n",
    "for a in (1e-3)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, pt_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, pt_train), \n",
    "                                                              ridge.score(x_test, pt_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  \n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.582e-02, tolerance: 3.791e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.9976 \t Test R: -1.6841 \t Nonzero coef: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.109e+00, tolerance: 3.791e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 1.00e-07 Train R2: 0.9115 \t Test R: 0.1510 \t Nonzero coef: 1013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.146e-01, tolerance: 3.791e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 1.00e-06 Train R2: 0.6169 \t Test R: 0.5213 \t Nonzero coef: 238\n",
      "Parameter: 2.00e-06 Train R2: 0.5626 \t Test R: 0.5487 \t Nonzero coef: 134\n",
      "Parameter: 3.00e-06 Train R2: 0.5354 \t Test R: 0.5541 \t Nonzero coef: 85\n",
      "Parameter: 4.00e-06 Train R2: 0.5183 \t Test R: 0.5513 \t Nonzero coef: 60\n",
      "Parameter: 5.00e-06 Train R2: 0.5059 \t Test R: 0.5517 \t Nonzero coef: 45\n",
      "Parameter: 6.00e-06 Train R2: 0.4972 \t Test R: 0.5472 \t Nonzero coef: 39\n",
      "Parameter: 7.00e-06 Train R2: 0.4887 \t Test R: 0.5425 \t Nonzero coef: 35\n",
      "Parameter: 8.00e-06 Train R2: 0.4815 \t Test R: 0.5367 \t Nonzero coef: 28\n",
      "Parameter: 1.00e-05 Train R2: 0.4714 \t Test R: 0.5276 \t Nonzero coef: 22\n",
      "Parameter: 2.00e-05 Train R2: 0.4385 \t Test R: 0.4899 \t Nonzero coef: 12\n",
      "Parameter: 5.00e-05 Train R2: 0.3489 \t Test R: 0.3816 \t Nonzero coef: 4\n"
     ]
    }
   ],
   "source": [
    "active_train = y[~train_test_index,0]\n",
    "active_test = y[train_test_index,0]\n",
    "\n",
    "for a in (1e-6)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, active_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d\" % (a, lasso.score(x_train, active_train), \n",
    "                                                                                  lasso.score(x_test, active_test), \n",
    "                                                                                  np.sum(lasso.coef_ != 0)))\n",
    "\n",
    "#     with open(out_dir+\"BA_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%.6f,%.4f,%.4f,%s,%d,%d\\n\" % (a, \n",
    "#             lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test), 'lasso', \n",
    "#             np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:197: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  \"Singular matrix in solving dual problem. Using \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.9969 \t Test R: -0.8904\n",
      "Parameter: 1.00e-04 Train R2: 0.7400 \t Test R: 0.5110\n",
      "Parameter: 1.00e-03 Train R2: 0.6048 \t Test R: 0.5468\n",
      "Parameter: 2.00e-03 Train R2: 0.5754 \t Test R: 0.5514\n",
      "Parameter: 3.00e-03 Train R2: 0.5592 \t Test R: 0.5524\n",
      "Parameter: 4.00e-03 Train R2: 0.5481 \t Test R: 0.5521\n",
      "Parameter: 5.00e-03 Train R2: 0.5396 \t Test R: 0.5509\n",
      "Parameter: 6.00e-03 Train R2: 0.5327 \t Test R: 0.5494\n",
      "Parameter: 7.00e-03 Train R2: 0.5268 \t Test R: 0.5476\n",
      "Parameter: 8.00e-03 Train R2: 0.5218 \t Test R: 0.5457\n",
      "Parameter: 1.00e-02 Train R2: 0.5132 \t Test R: 0.5414\n",
      "Parameter: 2.00e-02 Train R2: 0.4840 \t Test R: 0.5187\n",
      "Parameter: 5.00e-02 Train R2: 0.4311 \t Test R: 0.4605\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "\n",
    "for a in (1e-3)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, active_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, active_train), \n",
    "                                                              ridge.score(x_test, active_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Trip Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  \n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.118e+03, tolerance: 3.630e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.9938 \t Test R: -3.8934 \t Nonzero coef: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.006e+04, tolerance: 3.630e+01\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 1.00e-05 Train R2: 0.8816 \t Test R: -1.1713 \t Nonzero coef: 1176\n",
      "Parameter: 6.00e-04 Train R2: 0.0692 \t Test R: 0.0130 \t Nonzero coef: 38\n",
      "Parameter: 7.00e-04 Train R2: 0.0549 \t Test R: 0.0223 \t Nonzero coef: 25\n",
      "Parameter: 8.00e-04 Train R2: 0.0473 \t Test R: 0.0281 \t Nonzero coef: 18\n",
      "Parameter: 1.00e-03 Train R2: 0.0400 \t Test R: 0.0301 \t Nonzero coef: 13\n",
      "Parameter: 1.10e-03 Train R2: 0.0374 \t Test R: 0.0301 \t Nonzero coef: 11\n",
      "Parameter: 1.20e-03 Train R2: 0.0353 \t Test R: 0.0293 \t Nonzero coef: 10\n",
      "Parameter: 1.30e-03 Train R2: 0.0334 \t Test R: 0.0290 \t Nonzero coef: 9\n",
      "Parameter: 1.40e-03 Train R2: 0.0320 \t Test R: 0.0281 \t Nonzero coef: 8\n",
      "Parameter: 1.50e-03 Train R2: 0.0306 \t Test R: 0.0271 \t Nonzero coef: 8\n",
      "Parameter: 2.00e-03 Train R2: 0.0228 \t Test R: 0.0230 \t Nonzero coef: 5\n",
      "Parameter: 5.00e-03 Train R2: 0.0076 \t Test R: 0.0035 \t Nonzero coef: 2\n"
     ]
    }
   ],
   "source": [
    "trpgen_train = y[~train_test_index,-1]\n",
    "trpgen_test = y[train_test_index,-1]\n",
    "\n",
    "for a in (1e-4)*np.array([0,0.1,6,7,8,10,11,12,13,14,15,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, trpgen_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d\" % (a, lasso.score(x_train, trpgen_train), \n",
    "                                                                                  lasso.score(x_test, trpgen_test), \n",
    "                                                                                  np.sum(lasso.coef_ != 0)))\n",
    "#     with open(out_dir+\"BA_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%.6f,%.4f,%.4f,%s,%d,%d\\n\" % (a, \n",
    "#             lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test), 'lasso', \n",
    "#             np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:197: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  \"Singular matrix in solving dual problem. Using \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.9954 \t Test R: -3.3639\n",
      "Parameter: 1.00e-03 Train R2: 0.2700 \t Test R: -0.1056\n",
      "Parameter: 1.00e-02 Train R2: 0.1170 \t Test R: -0.0025\n",
      "Parameter: 2.00e-02 Train R2: 0.0855 \t Test R: 0.0103\n",
      "Parameter: 3.00e-02 Train R2: 0.0707 \t Test R: 0.0142\n",
      "Parameter: 4.00e-02 Train R2: 0.0617 \t Test R: 0.0157\n",
      "Parameter: 5.00e-02 Train R2: 0.0555 \t Test R: 0.0162\n",
      "Parameter: 6.00e-02 Train R2: 0.0509 \t Test R: 0.0162\n",
      "Parameter: 7.00e-02 Train R2: 0.0474 \t Test R: 0.0161\n",
      "Parameter: 8.00e-02 Train R2: 0.0445 \t Test R: 0.0158\n",
      "Parameter: 1.00e-01 Train R2: 0.0401 \t Test R: 0.0152\n",
      "Parameter: 2.00e-01 Train R2: 0.0291 \t Test R: 0.0120\n",
      "Parameter: 5.00e-01 Train R2: 0.0184 \t Test R: 0.0069\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "\n",
    "for a in (1e-2)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, trpgen_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, trpgen_train), \n",
    "                                                              ridge.score(x_test, trpgen_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Regression (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "mseloss = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pytorch_lr(w1_list, lr_list, x1, x2, y1, y2):\n",
    "    \n",
    "    \n",
    "    trainset = SurveyDataset(torch.tensor(x1,  dtype=torch.float), torch.tensor(y1, dtype=torch.float))\n",
    "    trainloader = DataLoader(trainset, batch_size=len(trainset), shuffle=False)\n",
    "\n",
    "    testset = SurveyDataset(torch.tensor(x2, dtype=torch.float), torch.tensor(y2, dtype=torch.float))\n",
    "    testloader = DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "\n",
    "    # decay rates for embedding\n",
    "#     w1_list = [1.5]\n",
    "    # decay rates for demo (There is no demo in this case)\n",
    "    w2_list = [0]\n",
    "    # lr_list = [0.005,0.01, 0.02]\n",
    "#     lr_list = [0.002]\n",
    "\n",
    "    dim_demo = 0\n",
    "    dim_embed = x1.shape[1]\n",
    "\n",
    "    for lr in lr_list:\n",
    "\n",
    "        for w1, w2 in itertools.product(w1_list, w2_list):\n",
    "\n",
    "            # model setup\n",
    "            model = linear_reg.LR(dim_embed=dim_embed, dim_demo=dim_demo)\n",
    "\n",
    "            embed_params = []\n",
    "            demo_params = []\n",
    "            other_params = []\n",
    "            for name, m in model.named_parameters():\n",
    "        #             print(name)\n",
    "                if 'embed' in name:\n",
    "                    embed_params.append(m)\n",
    "                elif 'demo' in name:\n",
    "                    demo_params.append(m)\n",
    "                else:\n",
    "                    other_params.append(m)\n",
    "\n",
    "            optimizer = torch.optim.SGD([{'params':embed_params,'weight_decay':w1},\n",
    "                                          {'params':demo_params,'weight_decay':w2},\n",
    "                                          {'params':other_params,'weight_decay':0}], lr=lr)\n",
    "\n",
    "            # model training\n",
    "            ref1 = 0\n",
    "            ref2 = 0\n",
    "\n",
    "            for epoch in range(1000):\n",
    "\n",
    "                mse_ = 0\n",
    "\n",
    "                for batch, (x_batch, y_batch) in enumerate(trainloader):\n",
    "                    # Compute prediction and loss\n",
    "                    pred = model(x_batch, None)\n",
    "                    pred = F.relu(pred).squeeze()\n",
    "\n",
    "                    mse = mseloss(pred, y_batch)\n",
    "                    mse_ += mse.item()\n",
    "\n",
    "                    # Backpropagation\n",
    "                    optimizer.zero_grad()\n",
    "                    mse.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                train_r = r2_score(y_batch.numpy(), pred.detach().numpy())\n",
    "                train_rmse = np.sqrt(mse_/len(trainset))\n",
    "\n",
    "                if epoch % 20 == 0:\n",
    "                    print(f\"[epoch: {epoch:>3d}] Train RMSE : {train_rmse:.4f} R2 score: {train_r:.3f} \")\n",
    "                loss_ = train_rmse\n",
    "\n",
    "                if epoch % 5 == 0:\n",
    "                    if epoch > 50:\n",
    "                        if (np.abs(loss_ - ref1)/ref1<ref1*0.001) & (np.abs(loss_ - ref2)/ref2<ref2*0.001):\n",
    "                            print(\"Early stopping at epoch\", epoch)\n",
    "                            break\n",
    "                        if (ref1 < loss_) & (ref1 < ref2):\n",
    "                            print(\"Diverging. stop.\")\n",
    "                            break\n",
    "                        if loss_ < best:\n",
    "                            best = loss_\n",
    "                            best_epoch = epoch\n",
    "                    else:\n",
    "                        best = loss_\n",
    "                        best_epoch = epoch\n",
    "\n",
    "                    ref2 = ref1\n",
    "                    ref1 = loss_\n",
    "\n",
    "                if epoch % 20 == 0:\n",
    "\n",
    "                    mse_ = 0 \n",
    "\n",
    "                    for batch, (x_batch, y_batch) in enumerate(testloader):\n",
    "                        pred = model(x_batch, None)\n",
    "                        pred = F.relu(pred).squeeze()\n",
    "\n",
    "                        mse = mseloss(pred, y_batch)\n",
    "                        mse_ += mse.item()\n",
    "\n",
    "                    test_rmse = np.sqrt(mse_/len(testset))\n",
    "                    test_r = r2_score(y_batch.numpy(),pred.detach().numpy())\n",
    "\n",
    "                    print(f\"[epoch: {epoch:>3d}] Test RMSE {test_rmse:.4f} R2 score: {test_r:.3f} \")\n",
    "\n",
    "    #         with open(out_dir+model_code+\"_regression_trpgen.csv\", \"a\") as f:\n",
    "    #             f.write(\"%s,%s,%s,%s,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f\\n\" % \\\n",
    "    #                 (model_run_date, model_type, zoomlevel, \"LR\", lr, w1, \n",
    "    #                   train_rmse, train_r, test_rmse, test_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:   0] Train RMSE : 0.0214 R2 score: -10.921 \n",
      "[epoch:   0] Test RMSE 0.0651 R2 score: -12.113 \n",
      "[epoch:  20] Train RMSE : 0.0214 R2 score: -10.921 \n",
      "[epoch:  20] Test RMSE 0.0651 R2 score: -12.113 \n",
      "[epoch:  40] Train RMSE : 0.0214 R2 score: -10.921 \n",
      "[epoch:  40] Test RMSE 0.0651 R2 score: -12.113 \n",
      "Early stopping at epoch 55\n"
     ]
    }
   ],
   "source": [
    "pytorch_lr(w1_list=[0.00001], lr_list=[1], x1=x_train, x2=x_test, y1=auto_train, y2=auto_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:   0] Train RMSE : 0.0032 R2 score: -0.502 \n",
      "[epoch:   0] Test RMSE 0.0095 R2 score: -0.513 \n",
      "[epoch:  20] Train RMSE : 0.0032 R2 score: -0.502 \n",
      "[epoch:  20] Test RMSE 0.0095 R2 score: -0.513 \n",
      "[epoch:  40] Train RMSE : 0.0032 R2 score: -0.502 \n",
      "[epoch:  40] Test RMSE 0.0095 R2 score: -0.513 \n",
      "Early stopping at epoch 55\n"
     ]
    }
   ],
   "source": [
    "pytorch_lr(w1_list=[0.00001], lr_list=[0.5], x1=x_train, x2=x_test, y1=pt_train, y2=pt_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), out_dir+\"image_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MNL for Mode Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader and model definition\n",
    "\n",
    "trainset = SurveyDataset(torch.tensor(x_train,  dtype=torch.float), torch.tensor(y_train, dtype=torch.float))\n",
    "trainloader = DataLoader(trainset, batch_size=len(trainset), shuffle=False)\n",
    "\n",
    "testset = SurveyDataset(torch.tensor(x_test, dtype=torch.float), torch.tensor(y_test, dtype=torch.float))\n",
    "testloader = DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "\n",
    "kldivloss = nn.KLDivLoss(reduction='sum')\n",
    "mseloss = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_train = np.sum(np.power(y_train - np.mean(y_train, axis=0), 2), axis=0)\n",
    "sst_test = np.sum(np.power(y_test - np.mean(y_test, axis=0), 2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 5e-05\n",
      "[epoch:  0] Train KL loss: 0.698                 RMSE 0.611                 0.197 0.511 0.195 0.188\n",
      "\t\t\t\t\t\t\tR2 score: -0.455 -3.791 -8.968 -2.630 \n",
      "[epoch:  0] Test KL loss: 0.469                RMSE 0.486                 0.171 0.401 0.153 0.150\n",
      "\t\t\t\t\t\t\tR2 score: -0.194 -2.176 -14.844 -1.420 \n",
      "[epoch: 20] Train KL loss: 0.221                 RMSE 0.295                 0.157 0.223 0.062 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.085 0.089 -0.006 0.050 \n",
      "[epoch: 20] Test KL loss: 0.200                RMSE 0.282                 0.150 0.216 0.040 0.095\n",
      "\t\t\t\t\t\t\tR2 score: 0.091 0.082 -0.060 0.035 \n",
      "[epoch: 40] Train KL loss: 0.199                 RMSE 0.275                 0.146 0.205 0.062 0.092\n",
      "\t\t\t\t\t\t\tR2 score: 0.201 0.226 -0.006 0.135 \n",
      "[epoch: 40] Test KL loss: 0.178                RMSE 0.261                 0.139 0.197 0.040 0.091\n",
      "\t\t\t\t\t\t\tR2 score: 0.219 0.232 -0.056 0.121 \n",
      "[epoch: 60] Train KL loss: 0.182                 RMSE 0.259                 0.138 0.191 0.062 0.088\n",
      "\t\t\t\t\t\t\tR2 score: 0.286 0.330 -0.006 0.209 \n",
      "[epoch: 60] Test KL loss: 0.160                RMSE 0.242                 0.130 0.181 0.040 0.087\n",
      "\t\t\t\t\t\t\tR2 score: 0.316 0.353 -0.052 0.198 \n",
      "[epoch: 80] Train KL loss: 0.170                 RMSE 0.247                 0.133 0.180 0.062 0.084\n",
      "\t\t\t\t\t\t\tR2 score: 0.345 0.403 -0.007 0.267 \n",
      "[epoch: 80] Test KL loss: 0.147                RMSE 0.228                 0.123 0.168 0.039 0.083\n",
      "\t\t\t\t\t\t\tR2 score: 0.386 0.443 -0.050 0.257 \n",
      "[epoch: 100] Train KL loss: 0.162                 RMSE 0.238                 0.128 0.172 0.062 0.082\n",
      "\t\t\t\t\t\t\tR2 score: 0.390 0.458 -0.007 0.310 \n",
      "[epoch: 100] Test KL loss: 0.138                RMSE 0.217                 0.118 0.158 0.040 0.081\n",
      "\t\t\t\t\t\t\tR2 score: 0.436 0.507 -0.053 0.304 \n",
      "[epoch: 120] Train KL loss: 0.156                 RMSE 0.231                 0.124 0.166 0.062 0.080\n",
      "\t\t\t\t\t\t\tR2 score: 0.422 0.496 -0.007 0.343 \n",
      "[epoch: 120] Test KL loss: 0.131                RMSE 0.209                 0.114 0.151 0.040 0.078\n",
      "\t\t\t\t\t\t\tR2 score: 0.468 0.551 -0.055 0.340 \n",
      "[epoch: 140] Train KL loss: 0.152                 RMSE 0.226                 0.123 0.162 0.062 0.078\n",
      "\t\t\t\t\t\t\tR2 score: 0.439 0.519 -0.007 0.366 \n",
      "[epoch: 140] Test KL loss: 0.126                RMSE 0.203                 0.112 0.145 0.039 0.077\n",
      "\t\t\t\t\t\t\tR2 score: 0.490 0.585 -0.049 0.366 \n",
      "[epoch: 160] Train KL loss: 0.149                 RMSE 0.222                 0.121 0.158 0.062 0.077\n",
      "\t\t\t\t\t\t\tR2 score: 0.455 0.540 -0.005 0.386 \n",
      "[epoch: 160] Test KL loss: 0.123                RMSE 0.199                 0.111 0.141 0.039 0.076\n",
      "\t\t\t\t\t\t\tR2 score: 0.503 0.607 -0.050 0.388 \n",
      "[epoch: 180] Train KL loss: 0.147                 RMSE 0.220                 0.120 0.156 0.062 0.076\n",
      "\t\t\t\t\t\t\tR2 score: 0.466 0.554 -0.004 0.398 \n",
      "[epoch: 180] Test KL loss: 0.120                RMSE 0.195                 0.109 0.137 0.039 0.074\n",
      "\t\t\t\t\t\t\tR2 score: 0.514 0.627 -0.044 0.405 \n",
      "[epoch: 200] Train KL loss: 0.145                 RMSE 0.218                 0.119 0.154 0.062 0.076\n",
      "\t\t\t\t\t\t\tR2 score: 0.471 0.564 -0.002 0.411 \n",
      "[epoch: 200] Test KL loss: 0.118                RMSE 0.192                 0.109 0.135 0.039 0.074\n",
      "\t\t\t\t\t\t\tR2 score: 0.520 0.640 -0.044 0.419 \n",
      "[epoch: 220] Train KL loss: 0.144                 RMSE 0.216                 0.118 0.153 0.062 0.075\n",
      "\t\t\t\t\t\t\tR2 score: 0.476 0.573 -0.001 0.421 \n",
      "[epoch: 220] Test KL loss: 0.116                RMSE 0.191                 0.108 0.133 0.040 0.073\n",
      "\t\t\t\t\t\t\tR2 score: 0.526 0.649 -0.057 0.432 \n",
      "[epoch: 240] Train KL loss: 0.143                 RMSE 0.215                 0.118 0.152 0.062 0.074\n",
      "\t\t\t\t\t\t\tR2 score: 0.480 0.578 -0.000 0.429 \n",
      "[epoch: 240] Test KL loss: 0.115                RMSE 0.189                 0.108 0.132 0.040 0.072\n",
      "\t\t\t\t\t\t\tR2 score: 0.528 0.655 -0.056 0.441 \n",
      "Diverging. stop.\n"
     ]
    }
   ],
   "source": [
    "wd_list = [0.00005]\n",
    "lr_list = [0.01]\n",
    "\n",
    "for (lr, wd) in itertools.product(lr_list, wd_list):\n",
    "    print(lr, wd)\n",
    "    # model setup\n",
    "    model = mnl.MNL(n_alts=4, n_features=x_train.shape[-1])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    # model training\n",
    "\n",
    "    ref1 = 0\n",
    "    ref2 = 0\n",
    "\n",
    "    for epoch in range(400):\n",
    "\n",
    "        kl_ = 0\n",
    "        mse_ = 0\n",
    "        mse1_ = 0\n",
    "        mse2_ = 0\n",
    "        mse3_ = 0\n",
    "        mse4_ = 0\n",
    "\n",
    "        for batch, (x_batch, y_batch) in enumerate(trainloader):\n",
    "            # Compute prediction and loss\n",
    "            util = model(x_batch)\n",
    "            probs = torch.log(nn.functional.softmax(util, dim=1))\n",
    "            kl = kldivloss(probs, y_batch)\n",
    "    #         kl = kldivloss(torch.log(util), y_batch)\n",
    "            kl_ += kl.item()\n",
    "\n",
    "            mse = mseloss(torch.exp(probs), y_batch)\n",
    "    #         mse = mseloss(util, y_batch)\n",
    "            mse_ += mse.sum().item()\n",
    "            mse1_ += mse[:,0].sum().item()\n",
    "            mse2_ += mse[:,1].sum().item()\n",
    "            mse3_ += mse[:,2].sum().item()\n",
    "            mse4_ += mse[:,3].sum().item()\n",
    "            mse = mse.sum()\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            kl.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_kl = kl_/len(trainset)\n",
    "        train_mse = np.sqrt(mse_/len(trainset))\n",
    "        train_mse1 = np.sqrt(mse1_/len(trainset))\n",
    "        train_mse2 = np.sqrt(mse2_/len(trainset))\n",
    "        train_mse3 = np.sqrt(mse3_/len(trainset))\n",
    "        train_mse4 = np.sqrt(mse4_/len(trainset))\n",
    "        \n",
    "        train_r1 = 1 - mse1_ / sst_train[0]\n",
    "        train_r2 = 1 - mse2_ / sst_train[1]\n",
    "        train_r3 = 1 - mse3_ / sst_train[2]\n",
    "        train_r4 = 1 - mse4_ / sst_train[3]\n",
    "        \n",
    "        loss_ = train_kl\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            \n",
    "            kl_ = 0\n",
    "            mse_ = 0 \n",
    "            mse1_ = 0\n",
    "            mse2_ = 0\n",
    "            mse3_ = 0\n",
    "            mse4_ = 0\n",
    "\n",
    "            for batch, (x_batch, y_batch) in enumerate(testloader):\n",
    "                util = model(x_batch)\n",
    "                probs = torch.log(nn.functional.softmax(util,dim=1))\n",
    "                kl = kldivloss(probs, y_batch)\n",
    "        #         kl = kldivloss(torch.log(util), y_batch)\n",
    "                kl_ += kl.item()\n",
    "\n",
    "                mse = mseloss(torch.exp(probs), y_batch)\n",
    "        #         mse = mseloss(util, y_batch)\n",
    "                mse_ += mse.sum().item()\n",
    "                mse1_ += mse[:,0].sum().item()\n",
    "                mse2_ += mse[:,1].sum().item()\n",
    "                mse3_ += mse[:,2].sum().item()\n",
    "                mse4_ += mse[:,3].sum().item()\n",
    "\n",
    "            test_kl = kl_/len(testset)\n",
    "            test_mse = np.sqrt(mse_/len(testset))\n",
    "            test_mse1 = np.sqrt(mse1_/len(testset))\n",
    "            test_mse2 = np.sqrt(mse2_/len(testset))\n",
    "            test_mse3 = np.sqrt(mse3_/len(testset))\n",
    "            test_mse4 = np.sqrt(mse4_/len(testset))\n",
    "\n",
    "            r1 = r2_score(y_batch.numpy()[:,0],torch.exp(probs).detach().numpy()[:,0])\n",
    "            r2 = r2_score(y_batch.numpy()[:,1],torch.exp(probs).detach().numpy()[:,1])\n",
    "            r3 = r2_score(y_batch.numpy()[:,2],torch.exp(probs).detach().numpy()[:,2])\n",
    "            r4 = r2_score(y_batch.numpy()[:,3],torch.exp(probs).detach().numpy()[:,3])\n",
    "\n",
    "            if epoch > 50:\n",
    "                if (np.abs(loss_ - ref1)/ref1<ref1*0.01) & (np.abs(loss_ - ref2)/ref2<ref2*0.01):\n",
    "                    print(\"Early stopping at epoch\", epoch)\n",
    "                    break\n",
    "                if (ref1 < loss_) & (ref1 < ref2):\n",
    "                    print(\"Diverging. stop.\")\n",
    "                    break\n",
    "                if loss_ < best:\n",
    "                    best = loss_\n",
    "                    best_epoch = epoch\n",
    "                    output = (train_kl, train_mse, train_mse1, train_mse2, train_mse3, train_mse4,\n",
    "                              test_kl, test_mse, test_mse1, test_mse2, test_mse3, test_mse4,\n",
    "                              r1, r2, r3, r4, train_r1, train_r2, train_r3, train_r4)\n",
    "            else:\n",
    "                best = loss_\n",
    "                best_epoch = epoch\n",
    "                output = (train_kl, train_mse, train_mse1, train_mse2, train_mse3, train_mse4,\n",
    "                              test_kl, test_mse, test_mse1, test_mse2, test_mse3, test_mse4,\n",
    "                              r1, r2, r3, r4, train_r1, train_r2, train_r3, train_r4)\n",
    "                \n",
    "            ref2 = ref1\n",
    "            ref1 = loss_\n",
    "            \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"[epoch: {epoch:>2d}] Train KL loss: {train_kl:.3f} \\\n",
    "                RMSE {train_mse:.3f} \\\n",
    "                {train_mse1:.3f} {train_mse2:.3f} {train_mse3:.3f} {train_mse4:.3f}\")\n",
    "            print(f\"\\t\\t\\t\\t\\t\\t\\tR2 score: {train_r1:.3f} {train_r2:.3f} {train_r3:.3f} {train_r4:.3f} \")\n",
    "            print(f\"[epoch: {epoch:>2d}] Test KL loss: {kl_/len(testset):.3f}\\\n",
    "                RMSE {np.sqrt(mse_/len(testset)):.3f} \\\n",
    "                {np.sqrt(mse1_/len(testset)):.3f} {np.sqrt(mse2_/len(testset)):.3f} {np.sqrt(mse3_/len(testset)):.3f} {np.sqrt(mse4_/len(testset)):.3f}\")\n",
    "            print(f\"\\t\\t\\t\\t\\t\\t\\tR2 score: {r1:.3f} {r2:.3f} {r3:.3f} {r4:.3f} \")\n",
    "            \n",
    "    with open(out_dir+sampling+\"_\"+model_code+\"_mode_choice.csv\", \"a\") as f:\n",
    "        f.write(\"%s,%s,%s,%s,%d,%.4f,%.5f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f\\n\" % \\\n",
    "            ((model_run_date, model_type, zoomlevel, \"MNL\", output_dim, lr, wd) + output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVIAAAFBCAYAAAAllyfaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAzXklEQVR4nO3deZxcZZ3v8c+vq9PpEAKBLBAjIYF0iGGHZgkxEKJByejgODIIeJGoMFwQZdRB1MFx5F7HcdSLG8NEhk0QEUVBjSKakISYkHRYshGSAGYhkJAFskCnu6uf+8ep6j59upZTderU0v198+KVWs459Zzqql89z+9ZjjnnEBGR4tVVugAiIrVOgVREJCIFUhGRiBRIRUQiUiAVEYlIgVREJKLYAqmZ3Wlm281sVZbnzcy+b2YbzGyFmZ0WV1lEROIUZ430buD9OZ6/EGhK/X818F8xlkVEJDaxBVLn3AJgV45NLgLudZ4lwFAzGxVXeURE4lLJHOloYLPv/pbUYyIiNaW+gq9tGR7LOF/VzK7Ga/4zePDg0ydOnBhnuUSkH1q+fPkO59yIYvatZCDdAhzlu/9OYGumDZ1zs4HZAM3Nza6lpSX+0olIv2JmG4vdt5JN+0eBK1K992cDbzrnXq1geUREihJbjdTMHgCmAcPNbAvwr8AAAOfc7cAcYCawAXgLmBVXWURE4hRbIHXOXZrneQdcF9fri4iUi2Y2iYhEpEAqIhKRAqmISEQKpCIiESmQiohEpEAqIhKRAqmISEQKpCIiESmQiohEpEAqIhKRAqmISEQKpCIiESmQiohEpEAqIhKRAqmISEQKpCIiESmQiohEpEAqIhKRAqmISEQKpCIiESmQiohEpEAqIhKRAqmISEQKpCIiESmQiohEpEAqIhKRAqmI1Ja1c+B3X/D+LeExxw61o4rdXYFURGrH2jnwy0/Ash97/5YimKaOOWyQjSz2EAqkIlI7XpwL7W97t9vf9u7nEqb26j9mkRRIRaQ8StEkP3Y6DBjk3R4wyLuf6/Wy1V79ZTl2OiQaii8TUB9pbxGJx9o5Xk3p2OkwcWalSxNdOqi1vw3P3gd/f2dx5zVxprdvvvdm7Rz4/Y29a68TZ/Yuy9nXQmdH8eeGAqlIdfAHTihN0KmEbD8AmZrk+c4pfazGQ6B1T/cx0/ulm/XB46ydAw99HJJt3Y8lGrrf25a7epZl+d3gOos63TQFUpFKC9aQjp5aeNCpBrlqncdO9x5rfzt/kzx4rLSWO+HdN8Do5tw/NC/O7RlEAcZN666NvvxE9+OJBi9IR6QcqUilBWtrED4PWE1ydQSlm+RnXBWuhp2pA8glYdH3etcogx1OwZxnogGaZ3Uf1x9kx00DXMgTzE6BVKTSgh0ozbMKCzrVIl9H0MSZ8Dff9m7n63TyH8uvswO2Lu/5WOMhvV/n4ntg/AXe/+dc7wXQtXN6bzvqRDi06OGjXcy56NG4nJqbm11LS0uliyFSWn2lcynfefib7AMGeT8UkHmf9LH2b4e1v8veIXTGVd6+2Y6Rfj1LwJEnwqvPdj8//gLv+K8+S/PsfbRsTVoxp61AKiLl87sveMOR0sZfABsX9gysmQLwn78OC7+T+ZiTLoL1f8x8jODrWR3U1XvN+3TzP9XUjxJI1bQXkfIJNv8h3AD7XB1CO9ZnP8ax072aaJrr9PKiZ1zl/RvslCqSAqlInOKYF17Lgp1OzbPCdaz1yJmmapXpfY67sPs5S/TOgx55olcTTW/fPMvL1fpfOyINfxIpRCG5zFINQu9r/GNBIdwA++BAfOi9z6LveXnUJbd5Q6Sg+/1PNHg10OZZ3dtPnOkNxl/4XaL23CuQioRVaGAsZhB6fxQMrGG3S48L/d0XYPfG7s4of/M+/f4n2+Cwo3u/TuseNPxJpJwKXTCjkHnh/VHUtId/Lv1Lc3s234+dHu79P3Z6d5ogAgVSkbAKDYyFDkLvTwpdDi9T0PX/sHV2pKZ51nnN9fR7ffRUb2RAtvd/4kyY8tmeHVJFUNNeJKywC2YE91EA7S1Yu2+5K/v7mi2l0niIFwBd0rdxJ7y6svd41fTMJv8xW+7ybjfPgkvuY+etH9he7OkokIoUQoExt7Cdcf6594kGb/77hrbsc+czpVSW3BYIouTeJ33M4KImLz/hzX6KQE17kf6uVEO0wjTX068F3WmPcdO6g1q2ufPBlEpwLn46P5qeV58rDROcb59sgydvrd4V8s3s/Wb2gpltMLObMjx/qJn9xsyeM7PVZjYr03FEJCalvHRHvs644GtB7/Gc2eboB3PN/kCZaIAjT/JyoRff091qyJafDi5qYnXZa7YhxRZIzSwB/Ai4EJgEXGpmkwKbXQescc6dDEwDvmNm0ZaqFpHwCh2JkM3aOd4QpHSAyhQQs71WOuiNv8DrHEofL1ct2b8PePPnNy7svc3ffLt3iiG4qMm7/ynywPw4c6RnAhuccy8BmNnPgIuANb5tHDDEzAw4GNgFRFuqWkTCK3Sd0Ez8HTuJBi84jTqx98LLwddqPMQLlunXTM+5T68XmkzlTJsugOd/69Ua/TnUiTO919iQIS2QaUFov2Cue3QzO79dnZ1No4HNvvtbgLMC2/wQeBTYCgwBLnGu91LVZnY1cDXAmDFjYimsSJ+WrRMoXbNL92AXw1/TTOcel9zWu5fdP+qh8ZCe2/gXs/bnL9vfhjWP9Lzv7zjKFJyDC0KHmTwxcSZ/fcNtzr5BbnHmSDOtohKcQvA+4FngHcApwA/NLDBRFpxzs51zzc655hEjRpS6nCJ9W5g86MaFsOGPxeVJC1mIJN3cbt2TfTHrREN3isAyhCj/XPpgLtR/3LQoKYuQ4gykWwD/iqnvxKt5+s0CHnaeDcDLwMQYyyTS/+TLg0bNk/qD2dnXeo/lypVC7sWsL77H+/+Mq7xOpKDgSlD+BaO3LKNXHS5TGUq8mEycTftlQJOZjQNeAT4KXBbYZhPwHmChmR0BHAe8FGOZRCovUzM7zoWd8+VBS5EnTZc5PT6zrt7LlfoXCQlun2lyQ6a59D//X93z6P0XsfPLdME7q4Nj39u7DP6cbvo6UO/5auHn7BNbIHXOdZjZp4HHgARwp3NutZldk3r+duAW4G4zW4n3M/JF59yOuMokUnGZZulAvKtE5ZuRVcyMrUxa7uoOZOnAlycvmfe1Js6Ef/hJz1lImfbJdME715l5oRL/NZ/S14FKrxZVpFhnNjnn5gBzAo/d7ru9FbggzjKIVJVszei4V4nKF7TyPV/JS6FkKluwPMdOh6fvyX4JZv9+/quIghf0W+5i7FAr+uJNmtkkUk6ZZtxU+ypRYQftN8/qzo36r9xZjvL4x4aOOqXn4Hy/TDXX1BTVKDObNNdepJyyNaNL0bTOp9haZdh1VdPBLO7zyFaeMKmC4Bz/cdO8xzf8MVKRFEhFyi3TF75Ui6FkC5ZRVusvpDMqzkVd0ufWeIhXjmI6xzL9kGVq7hdIgVSkr8gVLKOs1l+qzqgogsvinX1t9llLuY6RPof0cKn0cnq95wEVRIFUpK/IFSyjDnGq9PKBwXNr3dMzGOYL8vlGS0SkziaRviJXp1Wtr9af7dzCdoRl+pEJLsUXgWqkIn1FmPGitRZA07KdW9iURbYaefqxiBRIRUqt2sZc+lWybMUIljfYgZZeui/ZljtlkWu0xMOfgrb9kYppzkW/FGk5NTc3u5aWlkoXQySzYKdINTWjS122uIOyf9pnoqHnuNDg0n3jpmWf9ZTPf58Hrz5L8+x9tGxNZlpsKS/lSEXCCrPQRaaLulWLUi3iDKVdWT/b8X9/Y/fg+WRbz/cyuHRfpqmgYZ33xciXZFYgFQkjbOAIXsbi5SfCB5kwgTrKqkWlnEFVyqAclH6v38yxPGjjId3BL+q5pObz73zbFb2wswKpSBhhA8fEmd2zZcCrLYUJMmEvHBelFljKnvs4p7Vm6k33TzldO8dbFLqzw7scs/869sWq4oWdRfqOQgJHcM55mCATJlCXohaY7TpGxRwnruFU/tpm+tIl/vyo/31wyd7rk1aAeu1Fwoh7dk+YAfOlWDe0lOIYThWsbZ5zfe+1QqvtfUCBVCS8sIHDv8JQumkfZt3NfIG6GqZqZlOqHvwwtc1C3ocyDffS8CeRUqumIVDlCCRhzjdsOUr53hV4LDNb7pwraoVn1UglvFobzF0p1VJzjLLiUyHyzS4qpBylfO+iLNRSIHU2SThxjxvsa0rVqRNFnEOU/PJ1xBVajkLfu2xDwsq4YLYCqYRTri9lJiW+4mO/Ua5Akq8HP85y5PqB95fr7Gu9z2xMnyE17SWcSvWUlqt52heVM8WQqyMuznLka76nb8f8GVIglXAqlfcrY56rT6qWFZ/iKkeYH/gyfIbUtJdwKtXRVO0XhpPKCjMxoAyfIQ1/kvwqPZxHowUkqhCfIQ1/knhVunldLc1TqV0xf4bUtJf81LwujEYZ9DuqkUp+1TLAvNyKSSlolEG/pEAq4fS35nWxATFXGkS53vxq9D1SIK1WNfqB6jMKzQun/16Nh3jpj3THXOMhXjO/8RBvVaP+WlMt9pLJNfIeKZBWoxr+QOVVKz8QhUxACI5qOPtab9Uif/C0hLeaEVTHeNjg3yHOv0vYz3OlOzUjUCCNqpQfwPSxdm+s2Q9UTrX0A1FIXjgYAFr3eHPFf/eFnkvC1dV762xWusMu+Hc4+9rCasv+zzzkf4+C78/cW7zbmaaSVtk6o2EpkEZRysAQvCpimEvM1ppaq3GEzQtnCwD+xy0BE/8GBo+sfG08+Hd44ffh/y7+z+nT93iPJdtyf/797wPA9jXeMYLb13CnpoY/RVHKhTyCV0UcNy2eyzhkUq7hOqUeRlUtw4yyza6ZONOr7dXVezXS9X+sjgAR/DsMb+r5fOMh2fcNfk7TC1jnu47V398JIyd1P5Zt+2pYNasIqpFGUcqmSPBYxV6ju1DlbG6XssZRbWmCbLXX1j1ecx6qpxYe/DsEA1quayD5P6fp61KFaTkFFw/pSy0tFEijKWVg6C+LgqSPnf7yFvta1ZAmCJMfr9a8Xzrwr53j5eTDppKCn1MI/5mt4aZ7PpprX63K1btd7nn0pXq9apj/H/b1q3WkQjAvP25a+VpCVUhz7WtRri9XHM3WbK9X7lpCqWqSla7dFHIe1TqZIZjvPOzonuWs1h+AKqTOpkrId9mOUq9Gn+/1ypngL2WHU6Zy12rHWSXkOgddWqYgCqSVkC9QlvpLWsnLhASFWT8yl1yBspxf/qjnUaxS/lDkOodq+szUADXtKyFfB0Spm63V1uFRbFM3X8qjEh1nlcrLlirlk+0cqu0zU+XyBlIz+yxwF7AXuAM4FbjJOffHmMvWd4UJlKX8klY6n1gq+QJlX//yZ6slxvF37SufmTLJ22tvZs855042s/cB1wE3A3c5504rRwGD+k2vvfQWpqc8WwdJX+g4yTSnPz21sxIjF/qYuHvtLfXvTLwA+pyZWa4dRGJRbE2+2gbvFyvTQPpKj6UVIFwgXW5mfwTGAV8ysyFAZ7zFkrIoVS2tnLW9Ygb096WAE/yh6MupjBoSJpB+EjgFeMk595aZDQNmxVoqiV+pamnlru0V83rlzp2W64dFecyqkTeQOuc6zWws8DEzc8CTzrlfxV4yiVepamnlru0V83rlDDjl/mGp1sH+/UzecaRmdhtwDbASWAX8o5n9KMzBzez9ZvaCmW0ws5uybDPNzJ41s9VmNr+QwksEYcaqhhmzWO6B6cWWu1yTDjT+sl8K02u/GjjBpTY0szpgpXPu+Dz7JYB1wAxgC7AMuNQ5t8a3zVDgL8D7nXObzGykc257ruOq176Ewk5Trba55KUqd1xlq+TrS9Hi7rV/ARgDbEzdPwpYEWK/M4ENzrmXUoX8GXARsMa3zWXAw865TQD5gqiUWK5mYTXPJS9VueP4AVDesl/K2rQ3s9+Y2aPAMOB5M3vCzJ4AngdGhDj2aGCz7/6W1GN+E4DDUsdebmZXFFR6iU8pmuyVWHg5bLnjnE5azrULpCrkqpF+O+KxM401DeYR6oHTgfcAg4DFZrbEObeux4HMrgauBhgzZkzEYkkoUWtWlRq7GbbcfWlIlFRc1kDqnOvq+DGzI4AzUneXhmyCb8FLA6S9E9iaYZsdzrn9wH4zWwCcjJdb9ZdlNjAbvBxpiNeWUojSZK9koApT7r4+nVTKKkyv/T8AS4GLgX8AnjKzj4Q49jKgyczGmVkD8FHg0cA2jwBTzazezA4CzsJLHUitq/Zl5iq1epP0SWE6m74CnJGuhZrZCOBPwC9y7eSc6zCzTwOPAQngTufcajO7JvX87c65583sD3idV53AHc65VcWfjlSNWuh00RhMKZEww59WOudO9N2vA57zP1ZOGv4kInGIe/jTH8zsMeCB1P1LAC2XLSKSkjOQplZ5+j5eR9O78XriZ2uKaD/VF5aiE4lBzkDqnHNm9mvn3OnAw2Uqk1SjvrIUnUgMwlyzaYmZnZF/M+nTNIdcJKswgfR8vGD6opmtMLOVZhZmiqj0JdU+nEmkgsJ0Nl0YeymkOOVeULnahzOJVEjWQGpmI4EvA+PxltD7d+fcnnIVTPKoRM5S4y5FMsrVtL8X2A/8ADgYr/deqoVyliJVI1cgPdI59xXn3GPOueuBk8pVKAlBOUuRqpErR2pmdhjdqzgl/Pedc7viLpzkoJylSNXIFUgPBZbTczm8p1P/OuCYuAolISlnKVIVci2jN7aM5RARqVlhxpGKiEgOCqQiIhEpkIqIRJRrQP7huXZUr72IiCdXr/1yvN55w7sc8+7U7aHAJmBc3IUTEakFWZv2zrlxzrlj8C4V8kHn3HDn3DDgA2hJPRGRLmFypGc457pWxHfO/R44L74iiYjUljCrP+0ws38B7sNr6n8M2BlrqUREakiYGumlwAjgV6n/R6QeExERQtRIU73znzWzg51z+8pQJhGRmpK3Rmpm55jZGmBN6v7JZnZb7CUTEakRYZr2/w94H6m8qHPuOeDcOAslIlJLQs1scs5tDjyUjKEsIiI1KUyv/WYzOwdwZtYAfAZ4Pt5iiYjUjjA10muA64DRwBbgFODaGMskIlJTwtRIj3POXe5/wMymAIviKZKISG0JUyP9QcjHRET6pVyrP00GzgFGmNnnfE8dAiTiLpiISK3I1bRvwLsMcz0wxPf4HuAjcRZKRKSW5Lpm03xgvpnd7ZzbWMYyiYjUlDA50jvMbGj6jpkdZmaPxVckEZHaEiaQDnfOvZG+45zbDYyMrUQiIjUmTCDtNLMx6TtmdjTecnoiIkK4caRfAZ40s/mp++cCV8dXJBGR2hJmGb0/mNlpwNl412z6J+fcjthLJiJSI7I27c1sYurf0/AufrcVeAUYk3pMRETIXSP9PHAV8J0MzzlgeiwlEhGpMbnGkV6V+vf88hVHRKT25Joi+uFcOzrndElmERFyN+0/mPp3JN6c+7mp++cDT6Br24uIALmb9rMAzOy3wCTn3Kup+6OAH5WneCIi1S/MgPyx6SCasg2YEFN5RERqTpgB+U+k5tY/gNdb/1FgXqylEhGpIXlrpM65TwO3AyfjXWZktnPu+jAHN7P3m9kLZrbBzG7Ksd0ZZpY0My3PJyI1J0yNFOBpYK9z7k9mdpCZDXHO7c21g5kl8HKpM/Cu9bTMzB51zq3JsN1/AFpRSkRqUt4aqZldBfwC+O/UQ6OBX4c49pnABufcS865NuBnwEUZtrse+CWwPUyBRUSqTZjOpuuAKXgr4+OcW0+4ZfRGA5t997ekHutiZqOBv8NLHYiI1KQwgfRAqkYJgJnVE24ZPcvwWHC/W4EvOueSOQ9kdrWZtZhZy+uvvx7ipUVEyidMjnS+mX0ZGGRmM/Cuaf+bEPttAY7y3X8n3sInfs3Az8wMYDgw08w6nHO/9m/knJsNzAZobm7WWqgiUlXC1Ei/CLwOrAT+EZgD/EuI/ZYBTWY2zswa8IZNPerfwDk3zjk31jk3Fi8Pe20wiIqIVLucNVIzqwNWOOdOAH5cyIGdcx1m9mm83vgEcKdzbrWZXZN6XnlREekTcgZS51ynmT1nZmOcc5sKPbhzbg5eDdb/WMYA6py7stDji4hUgzA50lHAajNbCuxPP+ic+9vYSiUiUkPCBNJ/i70UIiI1LNd6pI3ANcB4vI6m/3HOdZSrYCIitSJXr/09eMOTVgIXkvmSIyIi/V6upv0k59yJAGb2P8DS8hRJRKS25KqRtqdvqEkvIpJdrhrpyWa2J3Xb8GY27Undds65Q2IvnYhIDch1qZFEOQsiIlKrwkwRFRGRHBRIRUQiUiAVEYlIgVREJCIFUhGRiBRIRUQiUiAVEYlIgVREJCIFUhGRiBRIRUQiUiAVEYlIgVREJCIFUhGRiBRIRUQiUiAVEYlIgVREJCIFUhGRiBRIRUQiUiAVEYlIgVREJCIFUhGRiBRIRUQiUiAVEYlIgVREJKL6ShdApNo8vmYbC9e/ztSmEcyYdESliyM1QDVSEZ/H12zjMw88w72LN/KZB57h8TXbKl0kqQEKpCI+C9e/ztvtSQDebk+ycP3rFS6R1AIFUuk3Hl+zja8+sipnLXNq0wgGDUgAMGhAgqlNI8pVPKlhypFKv5Busr/dnuShli18/9JTM+Y/Z0w6gu9feqpypFIQBVLpFzI12bMFyRmTjlAAlYKoaS/9gprsEifVSKXkqnH4kJrsEicFUimpsLnISlCTXeKipn0NC9MLXW4aPiT9kQJpCNUYsMoxcLyY865ELrIa/z7Sv6hpn0ccTdVS5BAL6YUutozFnHe5c5HVnEqQ/kM10jxK3VQtVU0yas0vXy0uynnPmHQEX7/ohLIEtEqlElQLFj8F0jxK3VQt9osf/OKma35XTD664FpYmGBeK8OFKpVK0Hx88Yu1aW9m7we+BySAO5xz3ww8fznwxdTdfcD/ds49F2eZCuVvqg5pHNAV+IqtbU1tGsFDLVt4uz0Z+oufrflabC90mLRArQwXqkQ5406rSO2JLZCaWQL4ETAD2AIsM7NHnXNrfJu9DJznnNttZhcCs4Gz4ipTsdJfEn8w+8S7x7G3tT30l9efFy30i1/qL27YYB7HcKE4xpgWUs5SvH4xP4bSt5lzLp4Dm00Gvuace1/q/pcAnHP/nmX7w4BVzrnRuY7b3NzsWlpaSl3cvL76yCruXbyx636izkh2OgYNSORtWvtrlGG2L/X+2Y5Z7tpmsedRSFlzbet//YTBNdPG88/vO67oc6n22roUxsyWO+eai9k3zqb9aGCz7/4Wctc2Pwn8PtMTZnY1cDXAmDFjSlW+gvhrIQmDZKf3A+TPc2b7YkWtUUZtvmb60ldicHox70MhvfL5tvW/ftLB7fNf5JSjhhb1Pmhwv/jF2dlkGR7LWP01s/PxAukXMz3vnJvtnGt2zjWPGFGZZpS/c+eaaeN7dHAMaRyQs/OhFB0ixfaEV1PHSDHvQzD4/vSpjaG3DXbkTW0aQcL3qUx2Ok0YkJKIM5BuAY7y3X8nsDW4kZmdBNwBXOSc2xljeSLx1+pOOWooZx9zOOcf5+U797a25/wCR+lhj6qaZhqFfR/8IxSmNo2gIdH9MV20YWfWH4N8gXrGpCO4Ztp4EnWWdRuRYsSZI60H1gHvAV4BlgGXOedW+7YZA8wFrnDO/SXMccuRIw02hR9fs43r7n+atmQn9XVGnRltyc6uPB9Q8hxmqQTzkoV2kkV97ULTEZnyqD99aiPzXuj+Abhi8tF8/aITin7NWslv1ko5+4ooOdLYAimAmc0EbsUb/nSnc+7/mtk1AM65283sDuDvgXR7rSPfiRQSSDN9EB9fs62reXjZWUd3Bcr0dtA7KAa/yH7pL3W2D301fBnSZRjSOIA7n3y5LAG/2I6lYKfeFZOPZmrTiKr9oYpLHB2Mklu1djbhnJsDzAk8drvv9qeAT8Xx2pk6HoCumiV4zcSrzj2mK7g81LKFs485PG9T2PCSvf6mYabOh2qZvpgu21cfWVW28Y/FdrBlGlpUK2NaS0ljVWtLn51rny03mA6i6dt/WvNaj+3AC5DBMYIL1++gI9VTn6gzpjYN76rRBqVrgJt3vRX7fPhCgkswSA1pHMBXH1mVd/9iatXFjrXMFjT7Wy+5xqrWllib9nF4x/jj3T2Pzi1o2Iw/l+mvkdbXGf943rG9mrvQeyjTrLuWhsrT+V833Uniz6eWKhhEHZMZtpkfpYlZDWmNWqb3r7yqtmkfh5372/jMA8/k/UJnq9lcde4x/Ne8DXQCdWacctTQrDUgv8vOOpolL+3KW0Pw14Tbkp2cf9wIjjr8oJJ/GYpt+hXazI/SxOxvtchS0/tXO2oukEL4L3SmD+Le1nbSjfu2ZCcL178eanxm2DxdsEmWrfkfVdSmX9j91cQUya/mmvYDRzW5Yz71g6KbyeXoDS1Xkyzq64TdP67zUdNVqknVDn+KQ9gcaS594Qtc6+eg4T1SbfpVjrQUaj33VC3DqqKo9uE9tf5DJeVVcws779zfxnX3P10Vi+lWapX0apr2WaxqXji6mtYnkNpQc4EUvE6i7/15fUUv9VDJL1s1B6GwKrn+QD594YdKyqtmm/arXnmTVa+8WbGmbSWbpuWe6RNXM7daUywaqSCFqtlAmlZsEMu0MIl/vn2+wDGkcUDO+9lex2/v3LnsX7SIwVOmMGT69ILKnz5W1Euf5NMX8rGF6o9TUiWamg2kmea7+4VdKT192ZD0LJ+fLfXWom5LdmYNHI+v2caf1rzW47G9re0Zy5AtCO2dO5dXPvd5XGsrb/zyYUZ/9zsFBdNyBbhq7xSKS7XWlqU61WwgvfDEUQw/uCF0oPQvHRcMDv759v65+JkCh//YAGe9upozd6znzIkXAj2njOYKQvsXLcK1tgLgWlvZv2hRQYE027FL3QxXM1ckv5oNpKtfeYP5N2YOPMEgc/sTG0g6uH/JRq6ZNr5XcHjvpCPZtOvljPPjg4HDf+yzXl3Nl5ffT0NHG3ZrC3vHDO0RDHMFocFTpvDGLx/GtbZijY0MnjKloPPPdOw4aqlq5orkV7OB9LU9B3rc99fEelxfKXWROvCu0/OjeRu47vzxfP/SU3npkTmctn0dEwc0cErq/hkr5wOw7MTzOOaimb0Cx4zd6zh05e9ZOryJM3esp6GjDchcq8wVhIZMn87o734nVI40Uy4107HjWiZPzVyR3GpuZtPAUU1u1MdvZcTBDSz7lxlA9pWe0qscpWukaYk64+4JrYy49ZauGuHhV36cnf9zJ7Sncp11dQy7+ipG3nBD1357585ly2dvgPZ2kol69n/ooxz6u1/gWlvpbBjIk5d+NmPwjcKfS7XGxpy5VM0WEilev5zZdMShg7puZ7pA2l2zzuwRRH40b0PX7WSnY9f8BQz35Sj3zZ3bHUQBOjvZ+eM7GHTSSV2Ba/eDD3Ztk0h2MGrHJg777ndY+MDv+En7ESx+cySDMqxMFSVvGTaXunfuXE5ctIjZxx7P44dNqGgzPMpohFLsL1JuNRtIzXdB0hm71zFk5Rxahk/gqVHHd10gLR1I0tcuv33+iyQ7HYk6483jT8eemd9V0zt4+nQOvPzXnsE0meT1H93mBVCgY0fva/MtOfJ4rh/R2uvyzP5Lm0TJW4bJpfprrSMaG5lxw808vj713gRea+HdD7Nr/gIOP+9cpl754YzHihoEo4xGiLq/SCXUbCB1qas9b7/1Vob/+A4+kExywctLeGZEE78fO5mF64/qEUS6gukTG0h2Ov79zRHMvuFmmjavpm7IEDr37mXYJz/BvoVPcmB11/X5OLB6NV3Z2Pp67/+ODqyhgcMuuYSXHpnD1c8s4umRXhBPGD06lcIMH8oVvMLkUoO11n23/ZAX3vV+HhpzUo/AvfDuhzn4P7/G8GQ7rUvnshB6BNNSBLGooxGi7i9SCTU3RXTEW29w1qureXH7XvbOncvOH98BSS9QNXQmOWvbWm5quY8Zu9f12ndva3tXrvSkTSvYNX8BdUOGsOvue9h9/0/Zdfc9vD7xVN4efEjmF08F0IHHH8/hn5jF7gcfZOp93+FvX17El5fdy+TXVnPNtPE9O5XyDNxPB6/d9/+UVz73efbOncveuXN57ZZb2Dt3rrfP9OkcefPNWQPK4ClTsMZGwBtbO3bPq9zUch8nbVrRY3rjrvkLaEx6Ne7GZDu75i/ocZxMQaxQ/rJYQwNtW7Z0nUfB+xcxmkGkEmquRjr0wD5uarmPb/Ix9i860BVE/RqT7YzavJrH10zpumLopHccytOb3gC8YUs3tdxHY7KdnUsTXcdwra0c9Mv7qSd7B5x76y2vlrp2LSSTqXqxF8T/uW0NTQOaee2Wn3fVHoMD9YP3g8Fr94MP8tZTSwuqFaZrra/feisH1q3veg/O3LGek3y148PPO5fWpXNpTLbTmhjA4eed65UpVSOuGzIEa2zsSiPUDRnCa7fckrOZH6xNp8uy+8EH2f+Xxeyfv4D9Ty5i2FWf6tFxl+9clCOVWlJzgRS8IHHa9nUM/vhlXfnDHszY2FrX4/pM/ustnbZ9XVfNjGQSEl4w7ayro76zk1AyBPDdr7zGphs+R13bga4gOLXp+JwD2oM5UCB00zZTSiDdNO8Y0MCZF1/IVF/teOqVH2YhsMmXIw2OCjj8yo/TuXdvV009V0DPlgoYMn06+xctYn9793sc7LjLJX0MkVpRk4G0rS7B0yMnsOTI43npks9wxsr5NK54GutoxwHmHIMe+Tmnnu41o0/bvq4rhwnw9MgJXLBpKY3J9h7BY2NrHQ2/fpDGZDsdVkfCue5Orbo68AdZM3DOe71UmTYziHe0eRnVdBCccfP0nAPagzUwoKtGmqtpmy2I5avNTb3yw+DLiwZrxJ1793LkzTfz2i235A3oufKZg6dMYffPHuz+wUkmle+UPqsmAyl4yV2vxjmShnGXcObA47l01RyO2evNgR/Q0cbla//IUfu205hs54JNS/lm88dYPvoEDpo2jX3nj2fU5tU9As6RwMKmd7HzV7/g8MENHHHmKexb+CTJ3bs59G8/CMCbj/6Gjm3bumqymyadwXP7vMAOcPKOF7sCdDoI+ge0Z6pFBmtgYZq22YJYobU5f42YRIK6IUN6PZ4toOfaZsj06Qy76lNdOWzlO6Uvq7kB+Sc0DnIPjR3LUyMnsm3wsK6a5lmvrubClxdz2uvrGOC8mmMnPXvTXhpyJM/OuISvfOMaHl+zrSt/6r9AXY+mbkMDzjlob+8aDL9/0SJ23//TrmPuu/BDXH7QuV0phHdvW8Nnh+5i4gdn5GwK5xtcn0+pjvX4mm288YPvcfz8R7DOzh7HCjMUKt82GhMqtaJfXbPphMZB7oGx4+isq6OhM0lrYgDLjngX52xdQQJIAokM+6Wb4J0NA9n1ua/yyfWDaEt2ctarq2nesY6zL57J1Cs/zGu33NIjUPoddvllDJ4ypVcAW3Lk8RmDclDw2IddfhlH3nxz0e9FtiAVJnilf0jaF8zvUZMvRblEalG/m9mUwFHf6eXeGpPtvHvriq7ecy+YGolAz3v6+bq2A+yav4C2I2b06L3v+M4y9o4Z6uX2fv5Qz4H5dA/FyZSHnEG49UCjLlQSlKkZH2YsaHqSwEmbVnSdf/qHRk1wkcLVZCA1320XuA/QcUwTA1v30bF1a699OxsGstM1cN2KXzFi386u3vv69jbW/uZxfjPtcj504mkMevqprn0GTmhixA03ZM1phlWOoT1hBrSnJwn4Ry8Y0DZmHMfc9AU1wUUKVHMD8oOCQbTd6vjdKReya/J0r2c9rb6et087i18eM5VTl/2BD7y0iNN3bqCjzvst6WwYyA/2DOPexRu5ddAJdDYM9I7f2NgjiEaVb3B9VGEGtKev+fT0yAm0JryRDZ0NAxVERYpUkznSh8aO7fV4Ethw6GgemHgBAF9edi8NnT3Heu4+4ijWMZiztq3temzweefS8M53snDr27z01+1dnVdfOnQ7F3VsqbpOkjALoITNkS5c/zozdq+jKTB6QaQ/6nedTZkC6Y6Gg/nhqRdz2vZ1nL7teUa/tavH8+kUQBJwVke966SzYSAjPnElrc8/z95Ff6Guo4O2ugQrRk5g7JUf65qHXi09z4Uuk6drs4uE1y8DaTA36oD2ugQNncmMeVO/TqBz+BEc1nwK+56Y33tmFHT1yAMlG7IU1VcfWcW9izd23b9i8tF8/aITMm6rtUlFChMlkNZ8jjTNoKspnyuIgnfS9Tu2sffxP2UMotDdURPsvHnhDz8vXaELVMj17HVtdpHyqdlAGgyWXo20+3ZQxnp3ep49kKyvo63pKG+ZPKCzvp6fDF7JbQMWcSC1YNOBepjd+BTzNs3rcZh5m+bxjSXf6PV4qaUvL3LF5KPz1jALCboiEk3NNu2Dtg6FxZOM09c7tg6DY7fCEXu6n9/XAAe3BXZKJGg75h2s6dzKY6c6Guoa+MzDbSQ6OmlPwHf/ro7lTXWcvr6Tk152rBhnLG+qY+roqdz23tuYt2keD617iCWvLqG9s53GRCPfOvdbnD/m/DjfgtCUIxUJr98NyA9ywF+PhA8sdQzsgFG74bdnWtf9A/XwWHP3/XQO1SWTNKzfzLvq4bFT65j44gESHd4Py4AknPSyY3kTLG+qY3lT9+steXUJ33/6+/xkzU9oTXanBlqTrSzeupjzx5zPvE3zWLx1MZPfMbligVUXrRMpj5pt2vsZMH4rDOzw7g/sgKmrHMvHw19HeEH1wfMS/PZMY09jd1og/e/ADi9orhxXx4HUT8uBelgxLnO2tb2znSc2P9EjiAI0JhqZ/I7JzNs0jxsX3MgDLzzAjQtujL3JLyKV1SdqpB0Gw/b2fGzkHhixxwuWo3c5INm7Rpr6Nx00lzcZt36orkczPpPGRCPjDh3H+jfWdz026fBJXHPyNZw/5ny+seQbXUHWX0sVkb6pTwTSfY0w9O3ej6frkwOSXg01XWM1vJrq8ibjoAP0CJrBZnxQ09Amrj/1ehZvXdzj8fbO7rn5k98xmV9t+BWtydauWqqI9F01H0jbE7Dj0MyB1G/PQXDoW3TlTB88ry5rjTPNMJyvv39A3QCuP/X6rtplOlgCrH9jPTcuuLGrs+lb536r4jlSESmPmuy1//nYsbx4JLw5GP50qhcMP/dwJwM6vWb+X1P9K2O3Qb3zhkV998Pedvma7X5TR08FYOfbOxk2aBgXT7i4R1Cct2keP3jmBz2a+JcedylfPvvLpTpdESmTftVr/+Zg+NZHetcmv/vh7iD5TFM9Ew+fyGffOoeG5c9z+8DFLB/vLbycbrbXUYdL/QdQb/VMOGwC699Y3zWUKRg4g9LP3bjgRjXjRfqxmquRDho3yE38t4lcPOFidrbuZOWOlbQl22hINHDi8BMZ1jisV3M6PRTp4IaDWbvLW7Dk4gkXA/DQuoe67hc7bKkahjqJSDT9aq59c3Oza2lpqXQxRKSP0Vx7EZEKijWQmtn7zewFM9tgZjdleN7M7Pup51eY2WlxlkdEJA6xBVIzSwA/Ai4EJgGXmtmkwGYXAk2p/68G/iuu8oiIxCXOGumZwAbn3EvOuTbgZ8BFgW0uAu51niXAUDMbFWOZRERKLs5AOhrY7Lu/JfVYoduIiFS1OMeRZlrxIzhEIMw2mNnVeE1/gANmtipi2arZcGBHpQsRI51f7erL5wZwXLE7xhlItwBH+e6/EwheHznMNjjnZgOzAcyspdghCrVA51fb+vL59eVzA+/8it03zqb9MqDJzMaZWQPwUeDRwDaPAlekeu/PBt50zr0aY5lEREouthqpc67DzD4NPAYkgDudc6vN7JrU87cDc4CZwAbgLWBWXOUREYlLrHPtnXNz8IKl/7HbfbcdcF2Bh51dgqJVM51fbevL59eXzw0inF/NTREVEak2miIqIhJR1QbSvj69NMT5XZ46rxVm9hczO7kS5SxGvnPzbXeGmSXN7CPlLF9UYc7PzKaZ2bNmttrM5pe7jFGE+Gweama/MbPnUudXM30bZnanmW3PNoSy6LjinKu6//E6p14EjgEagOeASYFtZgK/xxuLejbwVKXLXeLzOwc4LHX7wlo5vzDn5ttuLl4O/SOVLneJ/3ZDgTXAmNT9kZUud4nP78vAf6RujwB2AQ2VLnvI8zsXOA1YleX5ouJKtdZI+/r00rzn55z7i3Nud+ruErwxtrUgzN8O4Hrgl8D2chauBMKc32XAw865TQDOuVo6xzDn54AhZmbAwXiBtKO8xSyOc24BXnmzKSquVGsg7evTSwst+yfxfiVrQd5zM7PRwN8Bt1N7wvztJgCHmdkTZrbczK4oW+miC3N+PwTehTd5ZiXwWedcZ3mKF7ui4kq1XmqkZNNLq1TospvZ+XiB9N2xlqh0wpzbrcAXnXNJr1JTU8KcXz1wOvAeYBCw2MyWOOfWxV24Eghzfu8DngWmA8cCj5vZQufcnpjLVg5FxZVqDaQlm15apUKV3cxOAu4ALnTO7SxT2aIKc27NwM9SQXQ4MNPMOpxzvy5LCaMJ+9nc4ZzbD+w3swXAyUAtBNIw5zcL+KbzkoobzOxlYCKwtDxFjFVxcaXSyd8sCd964CVgHN0J7+MD2/wNPZPCSytd7hKf3xi8GV/nVLq8pT63wPZ3U1udTWH+du8C/pza9iBgFXBCpctewvP7L+BrqdtHAK8Awytd9gLOcSzZO5uKiitVWSN1fXx6acjz+yowDLgtVXPrcDWwYETIc6tZYc7POfe8mf0BWAF0Anc452pixbKQf79bgLvNbCVewPmic64mVoUysweAacBwM9sC/CswAKLFFc1sEhGJqFp77UVEaoYCqYhIRAqkIiIRKZCKiESkQCoiElFVDn+Svs/MhuGNtQQ4EkgCr6fun+m8ed5RX+MJYBTQCuwDPuGceyHkvmOB3zrnTohaDun7FEilIpw3U+sUADP7GrDPOfft9PNmVu+cK8VCGJc751pSV6L9T+Bv/U+aWcI5lyzB60g/pqa9VA0zu9vMvmtm84D/MLOvmdkXfM+vStUUMbOPmdnS1Jqf/21miTyHXwCMT+27z8y+bmZPAZPN7HOpY68ysxt8+9Sb2T2pdSl/YWYHpfb/ppmtST3+7d4vJf2NAqlUmwnAe51zn8+2gZm9C7gEmOKcOwUvLXB5nuN+EG+lIoDBeFMEzwLexpu9chbelMCrzOzU1HbHAbOdcycBe4BrzexwvJWrjk89/n8KP0XpaxRIpdo8FKKp/R681ZWWmdmzqfvHZNn2/tQ2U4B07TaJtxYqeKtq/co5t985tw94GJiaem6zc25R6vZ9qW334OVc7zCzD+NNI5R+TjlSqTb7fbc76Plj35j614B7nHNfCnG8y51zLYHHWn3BOtc6fsH50y41F/1MvOD9UeDTeMvJST+mGqlUs7/iXRaC1LVzxqUe/zPwETMbmXrucDM7usjXWAB8yMwOMrPBeM32hannxpjZ5NTtS4Enzexg4FDnXWr8BlIdZtK/qUYq1eyXwBWppvkyUut5OufWmNm/AH80szqgHbgO2FjoCzjnnjazu+leS/MO59wzqU6t54GPm9l/A+vxlo87FHjEzBrxarP/VPzpSV+h1Z9ERCJS015EJCIFUhGRiBRIRUQiUiAVEYlIgVREJCIFUhGRiBRIRUQiUiAVEYno/wPiYt9s6x1i+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
    "for i in range(4):\n",
    "    ax.scatter(y_batch.detach().numpy()[:,i], torch.exp(probs).detach().numpy()[:,i], s=10)\n",
    "\n",
    "ax.set_xlabel(\"True Probs\")\n",
    "ax.set_ylabel(\"Predicted Probs\")\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
