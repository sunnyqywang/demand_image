{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fdaa969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/dreambig/qingyi/stable-diffusion\")\n",
    "\n",
    "from dataloader import image_loader\n",
    "from setup import *\n",
    "from scripts.img2img import load_model_from_config\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7205dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26707 images in dataset\n",
      "4713 images in dataset\n"
     ]
    }
   ],
   "source": [
    "# data loaders\n",
    "zoomlevel = 'zoom15'\n",
    "data_version = '1571'\n",
    "sampling = 'stratified'\n",
    "batch_size = 4\n",
    "num_workers = 8\n",
    "image_size = 256\n",
    "\n",
    "train_loader, test_loader = image_loader(image_dir+zoomlevel+\"/\", data_dir, batch_size, \n",
    "         num_workers, \n",
    "         image_size, \n",
    "         data_version=data_version, \n",
    "         sampling=sampling, \n",
    "         augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2262c0cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /dreambig/qingyi/stable-diffusion/logs/2023-06-24T19-02-38_autoencoder_kl_8x8x64/checkpoints/epoch=000020.ckpt\n",
      "Global Step: 150066\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 64, 8, 8) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m OmegaConf\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dreambig/qingyi/stable-diffusion/models/first_stage_models/kl-f32/config.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dreambig/qingyi/stable-diffusion/scripts/img2img.py:43\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[0;34m(config, ckpt, verbose)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected keys:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(u)\n\u001b[0;32m---> 43\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/mixins/device_dtype_mixin.py:127\u001b[0m, in \u001b[0;36mDeviceDtypeModuleMixin.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    125\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__update_properties(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:688\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    672\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 601\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:688\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    672\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# ckpt = '/dreambig/qingyi/stable-diffusion/logs/2023-06-21T17-21-27_autoencoder_kl_32x32x4/checkpoints/epoch=000002.ckpt'\n",
    "ckpt = '/dreambig/qingyi/stable-diffusion/logs/2023-06-24T19-02-38_autoencoder_kl_8x8x64/checkpoints/epoch=000020.ckpt'\n",
    "# ckpt = '/dreambig/qingyi/stable-diffusion/models/first_stage_models/kl-f32/model.ckpt'\n",
    "config = OmegaConf.load('/dreambig/qingyi/stable-diffusion/models/first_stage_models/kl-f32/config.yaml')\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = load_model_from_config(config, ckpt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3c252e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t10\t20\t30\t40\t50\t60\t70\t80\t90\t100\t110\t120\t130\t140\t150\t160\t170\t180\t190\t200\t210\t220\t230\t240\t250\t260\t270\t280\t290\t300\t310\t320\t330\t340\t350\t360\t370\t380\t390\t400\t410\t420\t430\t440\t450\t460\t470\t480\t490\t500\t510\t520\t530\t540\t550\t560\t570\t580\t590\t600\t610\t620\t630\t640\t650\t660\t670\t680\t690\t700\t710\t720\t730\t740\t750\t760\t770\t780\t790\t800\t810\t820\t830\t840\t850\t860\t870\t880\t890\t900\t910\t920\t930\t940\t950\t960\t970\t980\t990\t1000\t1010\t1020\t1030\t1040\t1050\t1060\t1070\t1080\t1090\t1100\t1110\t1120\t1130\t1140\t1150\t1160\t1170\t0\t10\t20\t30\t40\t50\t60\t70\t80\t90\t100\t110\t120\t130\t140\t150\t160\t170\t180\t190\t200\t210\t220\t230\t240\t250\t260\t270\t280\t290\t300\t310\t320\t330\t340\t350\t360\t370\t380\t390\t400\t410\t420\t430\t440\t450\t460\t470\t480\t490\t500\t510\t520\t530\t540\t550\t560\t570\t580\t590\t600\t610\t620\t630\t640\t650\t660\t670\t680\t690\t700\t710\t720\t730\t740\t750\t760\t770\t780\t790\t800\t810\t820\t830\t840\t850\t860\t870\t880\t890\t900\t910\t920\t930\t940\t950\t960\t970\t980\t990\t1000\t1010\t1020\t1030\t1040\t1050\t1060\t1070\t1080\t1090\t1100\t1110\t1120\t1130\t1140\t1150\t1160\t1170\t1180\t1190\t1200\t1210\t1220\t1230\t1240\t1250\t1260\t1270\t1280\t1290\t1300\t1310\t1320\t1330\t1340\t1350\t1360\t1370\t1380\t1390\t1400\t1410\t1420\t1430\t1440\t1450\t1460\t1470\t1480\t1490\t1500\t1510\t1520\t1530\t1540\t1550\t1560\t1570\t1580\t1590\t1600\t1610\t1620\t1630\t1640\t1650\t1660\t1670\t1680\t1690\t1700\t1710\t1720\t1730\t1740\t1750\t1760\t1770\t1780\t1790\t1800\t1810\t1820\t1830\t1840\t1850\t1860\t1870\t1880\t1890\t1900\t1910\t1920\t1930\t1940\t1950\t1960\t1970\t1980\t1990\t2000\t2010\t2020\t2030\t2040\t2050\t2060\t2070\t2080\t2090\t2100\t2110\t2120\t2130\t2140\t2150\t2160\t2170\t2180\t2190\t2200\t2210\t2220\t2230\t2240\t2250\t2260\t2270\t2280\t2290\t2300\t2310\t2320\t2330\t2340\t2350\t2360\t2370\t2380\t2390\t2400\t2410\t2420\t2430\t2440\t2450\t2460\t2470\t2480\t2490\t2500\t2510\t2520\t2530\t2540\t2550\t2560\t2570\t2580\t2590\t2600\t2610\t2620\t2630\t2640\t2650\t2660\t2670\t2680\t2690\t2700\t2710\t2720\t2730\t2740\t2750\t2760\t2770\t2780\t2790\t2800\t2810\t2820\t2830\t2840\t2850\t2860\t2870\t2880\t2890\t2900\t2910\t2920\t2930\t2940\t2950\t2960\t2970\t2980\t2990\t3000\t3010\t3020\t3030\t3040\t3050\t3060\t3070\t3080\t3090\t3100\t3110\t3120\t3130\t3140\t3150\t3160\t3170\t3180\t3190\t3200\t3210\t3220\t3230\t3240\t3250\t3260\t3270\t3280\t3290\t3300\t3310\t3320\t3330\t3340\t3350\t3360\t3370\t3380\t3390\t3400\t3410\t3420\t3430\t3440\t3450\t3460\t3470\t3480\t3490\t3500\t3510\t3520\t3530\t3540\t3550\t3560\t3570\t3580\t3590\t3600\t3610\t3620\t3630\t3640\t3650\t3660\t3670\t3680\t3690\t3700\t3710\t3720\t3730\t3740\t3750\t3760\t3770\t3780\t3790\t3800\t3810\t3820\t3830\t3840\t3850\t3860\t3870\t3880\t3890\t3900\t3910\t3920\t3930\t3940\t3950\t3960\t3970\t3980\t3990\t4000\t4010\t4020\t4030\t4040\t4050\t4060\t4070\t4080\t4090\t4100\t4110\t4120\t4130\t4140\t4150\t4160\t4170\t4180\t4190\t4200\t4210\t4220\t4230\t4240\t4250\t4260\t4270\t4280\t4290\t4300\t4310\t4320\t4330\t4340\t4350\t4360\t4370\t4380\t4390\t4400\t4410\t4420\t4430\t4440\t4450\t4460\t4470\t4480\t4490\t4500\t4510\t4520\t4530\t4540\t4550\t4560\t4570\t4580\t4590\t4600\t4610\t4620\t4630\t4640\t4650\t4660\t4670\t4680\t4690\t4700\t4710\t4720\t4730\t4740\t4750\t4760\t4770\t4780\t4790\t4800\t4810\t4820\t4830\t4840\t4850\t4860\t4870\t4880\t4890\t4900\t4910\t4920\t4930\t4940\t4950\t4960\t4970\t4980\t4990\t5000\t5010\t5020\t5030\t5040\t5050\t5060\t5070\t5080\t5090\t5100\t5110\t5120\t5130\t5140\t5150\t5160\t5170\t5180\t5190\t5200\t5210\t5220\t5230\t5240\t5250\t5260\t5270\t5280\t5290\t5300\t5310\t5320\t5330\t5340\t5350\t5360\t5370\t5380\t5390\t5400\t5410\t5420\t5430\t5440\t5450\t5460\t5470\t5480\t5490\t5500\t5510\t5520\t5530\t5540\t5550\t5560\t5570\t5580\t5590\t5600\t5610\t5620\t5630\t5640\t5650\t5660\t5670\t5680\t5690\t5700\t5710\t5720\t5730\t5740\t5750\t5760\t5770\t5780\t5790\t5800\t5810\t5820\t5830\t5840\t5850\t5860\t5870\t5880\t5890\t5900\t5910\t5920\t5930\t5940\t5950\t5960\t5970\t5980\t5990\t6000\t6010\t6020\t6030\t6040\t6050\t6060\t6070\t6080\t6090\t6100\t6110\t6120\t6130\t6140\t6150\t6160\t6170\t6180\t6190\t6200\t6210\t6220\t6230\t6240\t6250\t6260\t6270\t6280\t6290\t6300\t6310\t6320\t6330\t6340\t6350\t6360\t6370\t6380\t6390\t6400\t6410\t6420\t6430\t6440\t6450\t6460\t6470\t6480\t6490\t6500\t6510\t6520\t6530\t6540\t6550\t6560\t6570\t6580\t6590\t6600\t6610\t6620\t6630\t6640\t6650\t6660\t6670\t"
     ]
    }
   ],
   "source": [
    "ct = []\n",
    "encoder_output = []\n",
    "im = []\n",
    "\n",
    "for step, data in enumerate(test_loader):\n",
    "    data1 = data[1].permute(0, 3, 1, 2).to(device)\n",
    "    encoder_output += [model.encode(data1).mode().cpu().detach().numpy()]\n",
    "    ct += [s[s.rindex(\"/\")+1: s.rindex(\"_\")]for s in data[0]]\n",
    "#     encoder_output += [model.encode(data1).cpu().detach().numpy()]\n",
    "    im += data[0]\n",
    "    if step % 10 == 0:\n",
    "        print(step, end='\\t')\n",
    "        \n",
    "for step, data in enumerate(train_loader):\n",
    "    data1 = data[1].permute(0, 3, 1, 2).to(device)\n",
    "    encoder_output += [model.encode(data1).mode().cpu().detach().numpy()]\n",
    "    ct += [s[s.rindex(\"/\")+1: s.rindex(\"_\")]for s in data[0]]\n",
    "#     encoder_output += [model.encode(data1).cpu().detach().numpy()]\n",
    "    im += data[0]\n",
    "    if step % 10 == 0:\n",
    "        print(step, end='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e16038e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder_output = np.vstack(encoder_output)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ba90944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31420, 64, 8, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a627b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proj_dir+\"latent_space/SDAE_\"+zoomlevel+\"_kl-f32_ep20.pkl\", \"wb\") as f:\n",
    "\n",
    "    pkl.dump(encoder_output, f)\n",
    "    pkl.dump(im, f)\n",
    "    pkl.dump(ct, f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf81d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de56b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "ldm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
