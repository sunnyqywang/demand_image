{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook documents part 2 of the complementarity of image and demographic information: the ability of latent space extracted from Autoencoders to predict mode choice and trip generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"models/\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "from exp_version import get_hp_from_version_code\n",
    "\n",
    "\n",
    "from dataloader import SurveyDataset, load_aggregate_travel_behavior, load_demo\n",
    "from M1_util_train_test import load_model, test\n",
    "import linear_reg\n",
    "import mnl\n",
    "from setup import out_dir, data_dir, image_dir, model_dir, proj_dir\n",
    "\n",
    "plt.rcParams.update({\"font.size\":12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_model import parse_args\n",
    "\n",
    "zoomlevel = 'zoom15'\n",
    "latent_dim = 64*2\n",
    "demo_dim = 7\n",
    "demo_cols = [0,1,2,3,4,8,9]\n",
    "image_size = str(64)\n",
    "im_norm = '2'\n",
    "model_run_date = \"2210gan-7\"\n",
    "model_type = 'dcgan'\n",
    "loss_func = 'cosh'\n",
    "model_class = 'vae'\n",
    "base_lr = '0.0002'\n",
    "weight_decay = '0'\n",
    "sampling = 'clustered'\n",
    "\n",
    "args = parse_args(s=['--zoomlevel', zoomlevel, '--latent_dim', str(latent_dim), \n",
    "                     '--image_size', image_size, \\\n",
    "                     '--im_norm', im_norm, '--model_run_date', model_run_date, '--model_type', model_type, \\\n",
    "                     '--loss_func', loss_func, '--model_class', model_class, \\\n",
    "                     '--base_lr', base_lr, '--weight_decay', weight_decay, \\\n",
    "                     '--demo_channels', str(demo_dim),\n",
    "                     '--sampling', sampling])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Load Model Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proj_dir+\"latent_space/cVAE_\"+args.zoomlevel+\"_\"+str(args.latent_dim//2)+\"_\"+\\\n",
    "                      str(args.image_size)+\"_\"+str(int(args.im_norm))+\"_\"+str(args.model_run_date)+\\\n",
    "                   \"_\"+args.loss_func+\".pkl\", \"rb\") as f: \n",
    "    encoder_output = pkl.load(f)\n",
    "    im = pkl.load(f)\n",
    "    ct = pkl.load(f)\n",
    "    test_encoder_output = pkl.load(f)\n",
    "    test_im = pkl.load(f)\n",
    "    test_ct = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Embeddings\n",
    "unique_ct = list(set(ct))\n",
    "unique_ct.sort()\n",
    "ct = np.array(ct)\n",
    "aggregate_embeddings = []\n",
    "for i in unique_ct:\n",
    "    aggregate_embeddings.append(np.mean(encoder_output[ct == i], axis=0))\n",
    "aggregate_embeddings = np.array(aggregate_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Test Embeddings\n",
    "test_unique_ct = list(set(test_ct))\n",
    "test_unique_ct.sort()\n",
    "test_ct = np.array(test_ct)\n",
    "test_aggregate_embeddings = []\n",
    "for i in test_unique_ct:\n",
    "    test_aggregate_embeddings.append(np.mean(test_encoder_output[test_ct == i], axis=0))\n",
    "test_aggregate_embeddings = np.array(test_aggregate_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1414, 64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_aggregate_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Trip Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"origin_trip_behavior.csv\"\n",
    "df_pivot = load_aggregate_travel_behavior(file, data_version='1571')\n",
    "\n",
    "train_test_index = df_pivot['train_test'].astype(bool).to_numpy()\n",
    "variable_names = ['active','auto','mas','pt', 'trpgen']\n",
    "\n",
    "y = df_pivot[variable_names].to_numpy()\n",
    "y_train = y[~train_test_index,:4]\n",
    "y_test = y[train_test_index,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = aggregate_embeddings\n",
    "x_test = test_aggregate_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_train = y[~train_test_index,1]\n",
    "auto_test = y[train_test_index,1]\n",
    "\n",
    "pt_train = y[~train_test_index,3]\n",
    "pt_test = y[train_test_index,3]\n",
    "\n",
    "active_train = y[~train_test_index,0]\n",
    "active_test = y[train_test_index,0]\n",
    "\n",
    "trpgen_train = y[~train_test_index,-1]\n",
    "trpgen_test = y[train_test_index,-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Auto Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.2967 \t Test R2: 0.0067\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression without Regularization\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(x_train, auto_train)\n",
    "# with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#     f.write(\"%s,%s,%s,%.4f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], -1, \n",
    "#         lr.score(x_train, auto_train), lr.score(x_test, auto_test), 'lr', zoomlevel,\n",
    "#         np.sum(lr.coef_ != 0), len(lr.coef_)))\n",
    "print(\"Train R2: %.4f \\t Test R2: %.4f\" % (lr.score(x_train, auto_train), lr.score(x_test, auto_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.2967 \t Test R: 0.0067 \t Nonzero coef: 64\n",
      "Parameter: 1.00e-03 Train R2: 0.2781 \t Test R: 0.0832 \t Nonzero coef: 29\n",
      "Parameter: 2.00e-03 Train R2: 0.2602 \t Test R: 0.0974 \t Nonzero coef: 14\n",
      "Parameter: 4.00e-03 Train R2: 0.2447 \t Test R: 0.1055 \t Nonzero coef: 5\n",
      "Parameter: 6.00e-03 Train R2: 0.2383 \t Test R: 0.1104 \t Nonzero coef: 4\n",
      "Parameter: 8.00e-03 Train R2: 0.2336 \t Test R: 0.1219 \t Nonzero coef: 3\n",
      "Parameter: 1.00e-02 Train R2: 0.2289 \t Test R: 0.1301 \t Nonzero coef: 3\n",
      "Parameter: 2.00e-02 Train R2: 0.2025 \t Test R: 0.1571 \t Nonzero coef: 2\n",
      "Parameter: 3.00e-02 Train R2: 0.1654 \t Test R: 0.1497 \t Nonzero coef: 1\n",
      "Parameter: 4.00e-02 Train R2: 0.1518 \t Test R: 0.1394 \t Nonzero coef: 1\n",
      "Parameter: 5.00e-02 Train R2: 0.1342 \t Test R: 0.1246 \t Nonzero coef: 1\n",
      "Parameter: 6.00e-02 Train R2: 0.1127 \t Test R: 0.1055 \t Nonzero coef: 1\n",
      "Parameter: 7.00e-02 Train R2: 0.0874 \t Test R: 0.0820 \t Nonzero coef: 1\n",
      "Parameter: 8.00e-02 Train R2: 0.0581 \t Test R: 0.0540 \t Nonzero coef: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-ce4fb1c46106>:4: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso.fit(x_train, auto_train)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.709e+01, tolerance: 7.704e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "for a in (1e-2)*np.array([0,0.1,0.2,0.4,0.6,0.8,1,2,3,4,5,6,7,8]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, auto_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d\" % \\\n",
    "          (a, lasso.score(x_train, auto_train), lasso.score(x_test, auto_test), np.sum(lasso.coef_ != 0)))\n",
    "\n",
    "#     with open(out_dir+\"SAE_A_LR.csv\", \"a\") as f:\n",
    "#         f.write(\"%.2E,%.6f,%s,%.4f,%.4f,%s,%d,%d\\n\" % (weight,a,'auto',\n",
    "#             lasso.score(x_train, auto_train), lasso.score(x_test, auto_test), 'lasso', \n",
    "#             np.sum(lasso.coef_ != 0), len(lasso.coef_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "\n",
    "for a in (5e+0)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, auto_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, auto_train), \n",
    "                                                              ridge.score(x_test, auto_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-7120b7093049>:4: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso.fit(x_train, pt_train)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.2512 \t Test R: 0.0327 \t Nonzero coef: 64\n",
      "Parameter: 1.00e-03 Train R2: 0.2016 \t Test R: 0.1087 \t Nonzero coef: 10\n",
      "Parameter: 2.00e-03 Train R2: 0.1796 \t Test R: 0.1045 \t Nonzero coef: 4\n",
      "Parameter: 4.00e-03 Train R2: 0.1727 \t Test R: 0.1089 \t Nonzero coef: 2\n",
      "Parameter: 6.00e-03 Train R2: 0.1635 \t Test R: 0.1116 \t Nonzero coef: 2\n",
      "Parameter: 8.00e-03 Train R2: 0.1506 \t Test R: 0.1058 \t Nonzero coef: 2\n",
      "Parameter: 1.00e-02 Train R2: 0.1340 \t Test R: 0.0917 \t Nonzero coef: 2\n",
      "Parameter: 2.00e-02 Train R2: 0.0997 \t Test R: 0.0749 \t Nonzero coef: 1\n",
      "Parameter: 3.00e-02 Train R2: 0.0449 \t Test R: 0.0360 \t Nonzero coef: 1\n",
      "Parameter: 4.00e-02 Train R2: -0.0000 \t Test R: -0.0000 \t Nonzero coef: 0\n",
      "Parameter: 5.00e-02 Train R2: -0.0000 \t Test R: -0.0000 \t Nonzero coef: 0\n",
      "Parameter: 6.00e-02 Train R2: -0.0000 \t Test R: -0.0000 \t Nonzero coef: 0\n",
      "Parameter: 7.00e-02 Train R2: -0.0000 \t Test R: -0.0000 \t Nonzero coef: 0\n",
      "Parameter: 8.00e-02 Train R2: -0.0000 \t Test R: -0.0000 \t Nonzero coef: 0\n",
      "Parameter: 1.00e-01 Train R2: -0.0000 \t Test R: -0.0000 \t Nonzero coef: 0\n",
      "Parameter: 2.00e-01 Train R2: -0.0000 \t Test R: -0.0000 \t Nonzero coef: 0\n",
      "Parameter: 5.00e-01 Train R2: -0.0000 \t Test R: -0.0000 \t Nonzero coef: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.138e+00, tolerance: 1.373e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "for a in (1e-2)*np.array([0,0.1,0.2,0.4,0.6,0.8,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, pt_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d\" % (a, lasso.score(x_train, pt_train), \n",
    "                                                                                  lasso.score(x_test, pt_test), \n",
    "                                                                                  np.sum(lasso.coef_ != 0)))\n",
    "\n",
    "#     with open(out_dir+\"SAE_A_LR.csv\", \"a\") as f:\n",
    "#         f.write(\"%.2E,%.6f,%s,%.4f,%.4f,%s,%d,%d\\n\" % (weight,a,'pt',\n",
    "#             lasso.score(x_train, pt_train), lasso.score(x_test, pt_test), 'lasso', \n",
    "#             np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "\n",
    "for a in (1e+1)*np.array([0,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,10,20,50]):\n",
    "# for a in (5e0) * np.array([0, 4.5,4.6,4.7,4.8,4.9, 5, 5.1,5.2,5.3,5.4,5.5]):\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, pt_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, pt_train), \n",
    "                                                              ridge.score(x_test, pt_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.2502 \t Test R: -0.0880 \t Nonzero coef: 64\n",
      "Parameter: 1.00e-03 Train R2: 0.2250 \t Test R: 0.0072 \t Nonzero coef: 18\n",
      "Parameter: 2.00e-03 Train R2: 0.2138 \t Test R: 0.0361 \t Nonzero coef: 7\n",
      "Parameter: 4.00e-03 Train R2: 0.2022 \t Test R: 0.0563 \t Nonzero coef: 5\n",
      "Parameter: 6.00e-03 Train R2: 0.1938 \t Test R: 0.0751 \t Nonzero coef: 3\n",
      "Parameter: 8.00e-03 Train R2: 0.1863 \t Test R: 0.0922 \t Nonzero coef: 3\n",
      "Parameter: 1.00e-02 Train R2: 0.1771 \t Test R: 0.1050 \t Nonzero coef: 2\n",
      "Parameter: 2.00e-02 Train R2: 0.1289 \t Test R: 0.1307 \t Nonzero coef: 1\n",
      "Parameter: 3.00e-02 Train R2: 0.1090 \t Test R: 0.1121 \t Nonzero coef: 1\n",
      "Parameter: 4.00e-02 Train R2: 0.0813 \t Test R: 0.0844 \t Nonzero coef: 1\n",
      "Parameter: 5.00e-02 Train R2: 0.0456 \t Test R: 0.0476 \t Nonzero coef: 1\n",
      "Parameter: 6.00e-02 Train R2: 0.0019 \t Test R: 0.0018 \t Nonzero coef: 1\n",
      "Parameter: 7.00e-02 Train R2: -0.0000 \t Test R: -0.0003 \t Nonzero coef: 0\n",
      "Parameter: 8.00e-02 Train R2: -0.0000 \t Test R: -0.0003 \t Nonzero coef: 0\n",
      "Parameter: 1.00e-01 Train R2: -0.0000 \t Test R: -0.0003 \t Nonzero coef: 0\n",
      "Parameter: 2.00e-01 Train R2: -0.0000 \t Test R: -0.0003 \t Nonzero coef: 0\n",
      "Parameter: 5.00e-01 Train R2: -0.0000 \t Test R: -0.0003 \t Nonzero coef: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-0aabbf89e3f7>:3: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso.fit(x_train, active_train)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.421e+01, tolerance: 3.791e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "for a in (1e-2)*np.array([0,0.1,0.2,0.4,0.6,0.8,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, active_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d\" % (a, lasso.score(x_train, active_train), \n",
    "                                                                                  lasso.score(x_test, active_test), \n",
    "                                                                                  np.sum(lasso.coef_ != 0)))\n",
    "\n",
    "#     with open(out_dir+\"SAE_A_LR.csv\", \"a\") as f:\n",
    "#         f.write(\"%.2E,%.6f,%s,%.4f,%.4f,%s,%d,%d\\n\" % (weight,a,'active',\n",
    "#             lasso.score(x_train, active_train), lasso.score(x_test, active_test), 'lasso', \n",
    "#             np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "\n",
    "# for a in (1e+1)*np.array([0,0.1,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,10,20,50]):\n",
    "for a in (1e+1)*np.array([0,1, 1.5, 2, 1.5, 3]):\n",
    "    \n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, active_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, active_train), \n",
    "                                                              ridge.score(x_test, active_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Trip Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.0706 \t Test R: -0.2020 \t Nonzero coef: 64\n",
      "Parameter: 1.00e-04 Train R2: 0.0706 \t Test R: -0.2016 \t Nonzero coef: 64\n",
      "Parameter: 6.00e-03 Train R2: 0.0702 \t Test R: -0.1798 \t Nonzero coef: 62\n",
      "Parameter: 7.00e-03 Train R2: 0.0701 \t Test R: -0.1762 \t Nonzero coef: 62\n",
      "Parameter: 8.00e-03 Train R2: 0.0699 \t Test R: -0.1729 \t Nonzero coef: 58\n",
      "Parameter: 1.00e-02 Train R2: 0.0696 \t Test R: -0.1664 \t Nonzero coef: 56\n",
      "Parameter: 1.10e-02 Train R2: 0.0694 \t Test R: -0.1632 \t Nonzero coef: 55\n",
      "Parameter: 1.20e-02 Train R2: 0.0692 \t Test R: -0.1601 \t Nonzero coef: 55\n",
      "Parameter: 1.30e-02 Train R2: 0.0690 \t Test R: -0.1571 \t Nonzero coef: 54\n",
      "Parameter: 1.40e-02 Train R2: 0.0687 \t Test R: -0.1541 \t Nonzero coef: 53\n",
      "Parameter: 1.50e-02 Train R2: 0.0685 \t Test R: -0.1512 \t Nonzero coef: 52\n",
      "Parameter: 2.00e-02 Train R2: 0.0670 \t Test R: -0.1382 \t Nonzero coef: 50\n",
      "Parameter: 5.00e-02 Train R2: 0.0547 \t Test R: -0.0708 \t Nonzero coef: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-aff3c5ecbb93>:3: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso.fit(x_train, trpgen_train)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.687e+05, tolerance: 3.630e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "for a in (1e-3)*np.array([0,0.1,6,7,8,10,11,12,13,14,15,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, trpgen_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d\" % (a, lasso.score(x_train, trpgen_train), \n",
    "                                                                                  lasso.score(x_test, trpgen_test), \n",
    "                                                                                  np.sum(lasso.coef_ != 0)))\n",
    "#     with open(out_dir+\"BA_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%.6f,%.4f,%.4f,%s,%d,%d\\n\" % (a, \n",
    "#             lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test), 'lasso', \n",
    "#             np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.0706 \t Test R: -0.2020\n",
      "Parameter: 1.00e+00 Train R2: 0.0705 \t Test R: -0.1890\n",
      "Parameter: 1.00e+01 Train R2: 0.0664 \t Test R: -0.1155\n",
      "Parameter: 2.00e+01 Train R2: 0.0603 \t Test R: -0.0760\n",
      "Parameter: 3.00e+01 Train R2: 0.0549 \t Test R: -0.0538\n",
      "Parameter: 4.00e+01 Train R2: 0.0503 \t Test R: -0.0401\n",
      "Parameter: 5.00e+01 Train R2: 0.0465 \t Test R: -0.0309\n",
      "Parameter: 6.00e+01 Train R2: 0.0432 \t Test R: -0.0244\n",
      "Parameter: 7.00e+01 Train R2: 0.0404 \t Test R: -0.0198\n",
      "Parameter: 8.00e+01 Train R2: 0.0379 \t Test R: -0.0163\n",
      "Parameter: 1.00e+02 Train R2: 0.0339 \t Test R: -0.0117\n",
      "Parameter: 2.00e+02 Train R2: 0.0225 \t Test R: -0.0044\n",
      "Parameter: 5.00e+02 Train R2: 0.0119 \t Test R: -0.0034\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "\n",
    "for a in (1e+1)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, trpgen_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, trpgen_train), \n",
    "                                                              ridge.score(x_test, trpgen_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MNL for Mode Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader and model definition\n",
    "\n",
    "trainset = SurveyDataset(torch.tensor(x_train,  dtype=torch.float), torch.tensor(y_train, dtype=torch.float))\n",
    "trainloader = DataLoader(trainset, batch_size=len(trainset), shuffle=False)\n",
    "\n",
    "testset = SurveyDataset(torch.tensor(x_test, dtype=torch.float), torch.tensor(y_test, dtype=torch.float))\n",
    "testloader = DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "\n",
    "kldivloss = nn.KLDivLoss(reduction='sum')\n",
    "mseloss = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_train = np.sum(np.power(y_train - np.mean(y_train, axis=0), 2), axis=0)\n",
    "sst_test = np.sum(np.power(y_test - np.mean(y_test, axis=0), 2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mnl_torch(lr_list, wd_list):\n",
    "    \n",
    "    for (lr, wd) in itertools.product(lr_list, wd_list):\n",
    "        \n",
    "        print(f\"[lr: {lr:.4f}, wd: {wd:3.2e}]\")\n",
    "\n",
    "        # model setup\n",
    "        model = mnl.MNL(n_alts=4, n_features=x_train.shape[-1])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "#         print(optimizer)\n",
    "        # model training\n",
    "\n",
    "        converged = 0\n",
    "        ref1 = 0\n",
    "        ref2 = 0\n",
    "\n",
    "        for epoch in range(5000):\n",
    "\n",
    "            kl_ = 0\n",
    "            mse_ = 0\n",
    "            mse1_ = 0\n",
    "            mse2_ = 0\n",
    "            mse3_ = 0\n",
    "            mse4_ = 0\n",
    "\n",
    "            for batch, (x_batch, y_batch) in enumerate(trainloader):\n",
    "                \n",
    "                # Compute prediction and loss\n",
    "                util = model(x_batch)\n",
    "                probs = torch.log(nn.functional.softmax(util, dim=1))\n",
    "                kl = kldivloss(probs, y_batch)\n",
    "        #         kl = kldivloss(torch.log(util), y_batch)\n",
    "                kl_ += kl.item()\n",
    "\n",
    "                mse = mseloss(torch.exp(probs), y_batch)\n",
    "        #         mse = mseloss(util, y_batch)\n",
    "                mse_ += mse.sum().item()\n",
    "                mse1_ += mse[:,0].sum().item()\n",
    "                mse2_ += mse[:,1].sum().item()\n",
    "                mse3_ += mse[:,2].sum().item()\n",
    "                mse4_ += mse[:,3].sum().item()\n",
    "                mse = mse.sum()\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                kl.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_kl = kl_/len(trainset)\n",
    "            train_mse = np.sqrt(mse_/len(trainset))\n",
    "            train_mse1 = np.sqrt(mse1_/len(trainset))\n",
    "            train_mse2 = np.sqrt(mse2_/len(trainset))\n",
    "            train_mse3 = np.sqrt(mse3_/len(trainset))\n",
    "            train_mse4 = np.sqrt(mse4_/len(trainset))\n",
    "\n",
    "            train_r1 = 1-mse1_/sst_train[0]\n",
    "            train_r2 = 1-mse2_/sst_train[1]\n",
    "            train_r3 = 1-mse3_/sst_train[2]\n",
    "            train_r4 = 1-mse4_/sst_train[3]\n",
    "\n",
    "            loss_ = train_kl\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "\n",
    "                kl_ = 0\n",
    "                mse_ = 0 \n",
    "                mse1_ = 0\n",
    "                mse2_ = 0\n",
    "                mse3_ = 0\n",
    "                mse4_ = 0\n",
    "\n",
    "                for batch, (x_batch, y_batch) in enumerate(testloader):\n",
    "                    \n",
    "                    util = model(x_batch)\n",
    "                    probs = torch.log(nn.functional.softmax(util,dim=1))\n",
    "                    kl = kldivloss(probs, y_batch)\n",
    "            #         kl = kldivloss(torch.log(util), y_batch)\n",
    "                    kl_ += kl.item()\n",
    "\n",
    "                    mse = mseloss(torch.exp(probs), y_batch)\n",
    "            #         mse = mseloss(util, y_batch)\n",
    "                    mse_ += mse.sum().item()\n",
    "                    mse1_ += mse[:,0].sum().item()\n",
    "                    mse2_ += mse[:,1].sum().item()\n",
    "                    mse3_ += mse[:,2].sum().item()\n",
    "                    mse4_ += mse[:,3].sum().item()\n",
    "\n",
    "                test_kl = kl_/len(testset)\n",
    "                test_mse = np.sqrt(mse_/len(testset))\n",
    "                test_mse1 = np.sqrt(mse1_/len(testset))\n",
    "                test_mse2 = np.sqrt(mse2_/len(testset))\n",
    "                test_mse3 = np.sqrt(mse3_/len(testset))\n",
    "                test_mse4 = np.sqrt(mse4_/len(testset))\n",
    "\n",
    "                r1 = r2_score(y_batch.numpy()[:,0],torch.exp(probs).detach().numpy()[:,0])\n",
    "                r2 = r2_score(y_batch.numpy()[:,1],torch.exp(probs).detach().numpy()[:,1])\n",
    "                r3 = r2_score(y_batch.numpy()[:,2],torch.exp(probs).detach().numpy()[:,2])\n",
    "                r4 = r2_score(y_batch.numpy()[:,3],torch.exp(probs).detach().numpy()[:,3])\n",
    "\n",
    "                if epoch >= 40:\n",
    "                    if (np.abs(loss_ - ref1)/ref1<0.001) & (np.abs(loss_ - ref2)/ref2<0.001):\n",
    "                        converged = 1\n",
    "                        print(\"Early stopping at epoch\", epoch)\n",
    "                        break\n",
    "                    if (ref1 < loss_) & (ref1 < ref2):\n",
    "                        print(\"Diverging. stop.\")\n",
    "                        break\n",
    "                    if loss_ < best:\n",
    "                        best = loss_\n",
    "                        best_epoch = epoch\n",
    "                        output = (best_epoch, train_kl, train_mse, train_mse1, train_mse2, train_mse3, train_mse4,\n",
    "                                  test_kl, test_mse, test_mse1, test_mse2, test_mse3, test_mse4,\n",
    "                                  train_r1, train_r2, train_r3, train_r4, r1, r2, r3, r4)\n",
    "                else:\n",
    "                    best = loss_\n",
    "                    best_epoch = epoch\n",
    "                    output = (best_epoch, train_kl, train_mse, train_mse1, train_mse2, train_mse3, train_mse4,\n",
    "                                  test_kl, test_mse, test_mse1, test_mse2, test_mse3, test_mse4,\n",
    "                                  train_r1, train_r2, train_r3, train_r4, r1, r2, r3, r4)\n",
    "                ref2 = ref1\n",
    "                ref1 = loss_\n",
    "\n",
    "            if epoch % 300 == 0:\n",
    "\n",
    "                print(f\"[epoch: {epoch:>3d}] Train KL loss: {train_kl:.3f} RMSE {train_mse:.3f}\")\n",
    "                   # {train_mse1:.3f} {train_mse2:.3f} {train_mse3:.3f} {train_mse4:.3f}\")\n",
    "                print(f\"\\t\\t\\t\\t\\t\\t Train R2 score: {train_r1:.3f} {train_r2:.3f} {train_r3:.3f} {train_r4:.3f} \")\n",
    "                print(f\"[epoch: {epoch:>3d}] Test KL loss: {kl_/len(testset):.3f} RMSE {np.sqrt(mse_/len(testset)):.3f}\")\n",
    "                   #     {np.sqrt(mse1_/len(testset)):.3f} {np.sqrt(mse2_/len(testset)):.3f} {np.sqrt(mse3_/len(testset)):.3f} {np.sqrt(mse4_/len(testset)):.3f}\")\n",
    "                print(f\"\\t\\t\\t\\t\\t\\t Test R2 score: {r1:.3f} {r2:.3f} {r3:.3f} {r4:.3f} \")\n",
    "\n",
    "                print(f\"[epoch: {epoch:>3d}] Train KL loss: {train_kl:.3f} Train R2 score: {train_r1:.3f} {train_r2:.3f} {train_r3:.3f} {train_r4:.3f} \")\n",
    "                print(f\"[epoch: {epoch:>3d}] Test KL loss: {kl_/len(testset):.3f} Test R2 score: {r1:.3f} {r2:.3f} {r3:.3f} {r4:.3f} \")\n",
    "\n",
    "        with open(out_dir+\"SAE_A_MNL.csv\", \"a\") as f:\n",
    "            f.write(\"%.1E,%.1E,%.1E,%d,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%d\\n\" % \n",
    "                    ((weight,lr,wd)+output+(converged,)))\n",
    "\n",
    "        print(f\"[epoch: {best_epoch:>3d}] Train KL loss: {output[1]:.3f} Train R2 score: {output[13]:.3f} {output[14]:.3f} {output[15]:.3f} {output[16]:.3f} \")\n",
    "        print(f\"[epoch: {best_epoch:>3d}] Test KL loss: {output[7]:.3f} Test R2 score: {output[17]:.3f} {output[18]:.3f} {output[19]:.3f} {output[20]:.3f} \")\n",
    "        print()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lr: 0.0001, wd: 1.00e-01]\n",
      "[epoch:   0] Train KL loss: 0.782 RMSE 0.657\n",
      "\t\t\t\t\t\t Train R2 score: -1.691 -4.592 -6.467 -1.708 \n",
      "[epoch:   0] Test KL loss: 0.279 RMSE 0.346\n",
      "\t\t\t\t\t\t Test R2 score: -0.140 -0.483 -2.953 -0.106 \n",
      "[epoch:   0] Train KL loss: 0.782 Train R2 score: -1.691 -4.592 -6.467 -1.708 \n",
      "[epoch:   0] Test KL loss: 0.279 Test R2 score: -0.140 -0.483 -2.953 -0.106 \n",
      "[epoch: 300] Train KL loss: 0.150 RMSE 0.225\n",
      "\t\t\t\t\t\t Train R2 score: 0.420 0.528 0.004 0.437 \n",
      "[epoch: 300] Test KL loss: 0.125 RMSE 0.203\n",
      "\t\t\t\t\t\t Test R2 score: 0.479 0.578 -0.073 0.412 \n",
      "[epoch: 300] Train KL loss: 0.150 Train R2 score: 0.420 0.528 0.004 0.437 \n",
      "[epoch: 300] Test KL loss: 0.125 Test R2 score: 0.479 0.578 -0.073 0.412 \n",
      "[epoch: 600] Train KL loss: 0.141 RMSE 0.214\n",
      "\t\t\t\t\t\t Train R2 score: 0.466 0.582 0.010 0.487 \n",
      "[epoch: 600] Test KL loss: 0.116 RMSE 0.192\n",
      "\t\t\t\t\t\t Test R2 score: 0.532 0.633 -0.103 0.448 \n",
      "[epoch: 600] Train KL loss: 0.141 Train R2 score: 0.466 0.582 0.010 0.487 \n",
      "[epoch: 600] Test KL loss: 0.116 Test R2 score: 0.532 0.633 -0.103 0.448 \n",
      "Early stopping at epoch 665\n",
      "[epoch: 660] Train KL loss: 0.140 Train R2 score: 0.470 0.587 0.011 0.493 \n",
      "[epoch: 660] Test KL loss: 0.115 Test R2 score: 0.536 0.639 -0.106 0.454 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "#     mnl_torch(lr_list=[1e-4], wd_list=[1e-3]);\n",
    "#     mnl_torch(lr_list=[1e-4], wd_list=[1e-2]);\n",
    "\n",
    "    model = mnl_torch(lr_list=[1e-4], wd_list=[1e-1]);\n",
    "#     model = mnl_torch(lr_list=[1e-5], wd_list=[1e+0]);\n",
    "#     mnl_torch(lr_list=[1e-4], wd_list=[0.1,1,10,50,100,1000]);\n",
    "#     model = mnl_torch(lr_list=[1e-5], wd_list=[1e+1]);\n",
    "#     mnl_torch(lr_list=[1e-4], wd_list=[50]);\n",
    "\n",
    "#     mnl_torch(lr_list=[1e-4], wd_list=[1e+2]);\n",
    "\n",
    "#     mnl_torch(lr_list=[5e-5], wd_list=[1e+3]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), out_dir+\"sae_a_D_1_220829.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qingyi",
   "language": "python",
   "name": "qingyi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
