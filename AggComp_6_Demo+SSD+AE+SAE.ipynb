{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook documents part 3 of the **complementarity of image and demographic information**: combine the image latent spaces and demographics to predict mode choice and trip generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"models/\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import glob\n",
    "import itertools\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from M1_util_train_test import load_model, test\n",
    "import mnl\n",
    "import linear_reg\n",
    "from dataloader import SurveyDataset, load_aggregate_travel_behavior, load_demo, train_test_split_data\n",
    "from setup import out_dir, data_dir, image_dir, model_dir, proj_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_version = '1571'\n",
    "\n",
    "model_type = 'AE'\n",
    "sampling = 's'\n",
    "\n",
    "zoomlevel = 'zoom15'\n",
    "output_dim = 3\n",
    "model_run_date = '2208'\n",
    "v2 = 1\n",
    "\n",
    "variable_names = ['active','auto','mas','pt', 'trpgen']\n",
    "\n",
    "demo_variables = ['tot_population','pct25_34yrs','pct35_50yrs','pctover65yrs',\n",
    "         'pctwhite_alone','pct_nonwhite','pctblack_alone',\n",
    "         'pct_col_grad','avg_tt_to_work','inc_per_capita']\n",
    "\n",
    "with open(proj_dir+\"latent_space/\"+model_type+\"_\"+zoomlevel+\"_\"+str(output_dim**2*2048)+\n",
    "                       \"_\"+str(v2)+\"_\"+model_run_date+\".pkl\", \"rb\") as f: \n",
    "    encoder_output = pkl.load(f)\n",
    "    im = pkl.load(f)\n",
    "    ct = pkl.load(f)\n",
    "    \n",
    "# Aggregate Embeddings\n",
    "unique_ct = list(set(ct))\n",
    "unique_ct.sort()\n",
    "ct = np.array(ct)\n",
    "embed_ae = []\n",
    "for i in unique_ct:\n",
    "    embed_ae.append(np.mean(encoder_output[ct == i], axis=0))\n",
    "embed_ae = np.array(embed_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'SAE'\n",
    "sampling = 's'\n",
    "\n",
    "zoomlevel = 'zoom15'\n",
    "output_dim = 3\n",
    "model_run_date = '2208'\n",
    "\n",
    "v1 = 'F'\n",
    "v2 = 1\n",
    "\n",
    "with open(proj_dir+\"latent_space/\"+model_type+\"_\"+zoomlevel+\"_\"+str(output_dim**2*2048)+\"_\"+\n",
    "                       v1+\"_\"+str(v2)+\"_\"+model_run_date+\".pkl\", \"rb\") as f: \n",
    "    encoder_output = pkl.load(f)\n",
    "    im = pkl.load(f)\n",
    "    ct = pkl.load(f)\n",
    "    \n",
    "# Aggregate Embeddings\n",
    "unique_ct = list(set(ct))\n",
    "unique_ct.sort()\n",
    "ct = np.array(ct)\n",
    "embed_sae = []\n",
    "for i in unique_ct:\n",
    "    embed_sae.append(np.mean(encoder_output[ct == i], axis=0))\n",
    "embed_sae = np.array(embed_sae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'SSD'\n",
    "zoomlevel = 'zoom15'\n",
    "output_dim = 3\n",
    "model_run_date = '2208'\n",
    "\n",
    "with open(proj_dir+\"latent_space/SSD_\"+zoomlevel+\"_\"+str(output_dim**2*2048)+\"_\"+str(v2)+\"_\"+\n",
    "                       str(model_run_date)+\".pkl\", \"rb\") as f:\n",
    "    encoder_output = pkl.load(f)\n",
    "    im = pkl.load(f)\n",
    "    ct = pkl.load(f)\n",
    "    \n",
    "# Aggregate Embeddings\n",
    "unique_ct = list(set(ct))\n",
    "unique_ct.sort()\n",
    "ct = np.array(ct)\n",
    "embed_ssd = []\n",
    "for i in unique_ct:\n",
    "    embed_ssd.append(np.mean(encoder_output[ct == i], axis=0))\n",
    "embed_ssd = np.array(embed_ssd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demographic variables\n",
    "\n",
    "demo_variables = ['tot_population','pct25_34yrs','pct35_50yrs','pctover65yrs',\n",
    "         'pctwhite_alone','pct_nonwhite','pctblack_alone',\n",
    "         'pct_col_grad','avg_tt_to_work','inc_per_capita']\n",
    "\n",
    "demo_cs, demo_np = load_demo(data_dir)\n",
    "demo = np.hstack((np.array(demo_cs).reshape(-1,1), demo_np))\n",
    "demo = pd.DataFrame(demo, columns = ['geoid'] + demo_variables)\n",
    "demo_split = train_test_split_data(demo, data_version='1571')\n",
    "\n",
    "demo_train = demo_split[~demo_split['train_test'].astype(bool)][demo_variables].to_numpy(dtype=float)\n",
    "demo_test = demo_split[demo_split['train_test'].astype(bool)][demo_variables].to_numpy(dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Trip Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"origin_trip_behavior.csv\"\n",
    "df_pivot = load_aggregate_travel_behavior(file, str(len(unique_ct)))\n",
    "\n",
    "train_test_index = df_pivot['train_test'].astype(bool).to_numpy()\n",
    "\n",
    "y = df_pivot[variable_names].to_numpy()\n",
    "y_train = y[~train_test_index,:4]\n",
    "y_test = y[train_test_index,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ae_train = embed_ae[~train_test_index, :]\n",
    "embed_ae_test = embed_ae[train_test_index, :]\n",
    "\n",
    "embed_sae_train = embed_sae[~train_test_index, :]\n",
    "embed_sae_test = embed_sae[train_test_index, :]\n",
    "\n",
    "embed_ssd_train = embed_ssd[~train_test_index, :]\n",
    "embed_ssd_test = embed_ssd[train_test_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate([demo_train, embed_ssd_train, embed_ae_train, embed_sae_train], axis=1)\n",
    "x_test = np.concatenate([demo_test, embed_ssd_test, embed_ae_test, embed_sae_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_demo = demo_train.shape[1]\n",
    "dim_embed_ae = embed_ae.shape[1]\n",
    "dim_embed_sae = embed_sae.shape[1]\n",
    "dim_embed_ssd = embed_ssd.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_train = y[~train_test_index,1]\n",
    "auto_test = y[train_test_index,1]\n",
    "\n",
    "pt_train = y[~train_test_index,3]\n",
    "pt_test = y[train_test_index,3]\n",
    "\n",
    "active_train = y[~train_test_index,0]\n",
    "active_test = y[train_test_index,0]\n",
    "\n",
    "trpgen_train = y[~train_test_index,-1]\n",
    "trpgen_test = y[train_test_index,-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.393e-01, tolerance: 7.704e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 6.00e-04 Train R2: 0.7080 \t Test R: 0.7059 \t Nonzero coef: 1, 0, 2, 134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.331e-01, tolerance: 7.704e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 7.00e-04 Train R2: 0.6934 \t Test R: 0.7060 \t Nonzero coef: 1, 0, 1, 118\n",
      "Parameter: 8.00e-04 Train R2: 0.6821 \t Test R: 0.7059 \t Nonzero coef: 1, 0, 1, 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.330e-01, tolerance: 7.704e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "for a in (1e-4)*np.array([6,7,8]):#[0,0.1,0.2,0.4,0.6,0.8,1,2,3,4,5,10,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, auto_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d, %d, %d, %d\" % (a, lasso.score(x_train, auto_train), \n",
    "                                  lasso.score(x_test, auto_test), \n",
    "                                  np.sum(lasso.coef_[:dim_demo] != 0), \n",
    "                                  np.sum(lasso.coef_[dim_demo:dim_demo+dim_embed_ssd] != 0), \n",
    "                                  np.sum(lasso.coef_[dim_demo+dim_embed_ssd:dim_demo+dim_embed_ssd+dim_embed_ae] != 0),\n",
    "                                  np.sum(lasso.coef_[dim_demo+dim_embed_ssd+dim_embed_ae:] != 0)))\n",
    "    with open(out_dir+\"AllModels_A_LR.csv\", \"a\") as f:\n",
    "        f.write(\"%s,%.6f,%s,%.4f,%.4f,%s,%d,%d\\n\" % ('All',a,'auto',\n",
    "            lasso.score(x_train, auto_train), lasso.score(x_test, auto_test), 'LR', \n",
    "            np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "\n",
    "for a in (3e0)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, auto_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, auto_train), \n",
    "                                                              ridge.score(x_test, auto_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 PT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.372e-02, tolerance: 1.373e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 3.00e-04 Train R2: 0.6368 \t Test R: 0.5294 \t Nonzero coef: 1, 0, 2, 138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.903e-02, tolerance: 1.373e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 4.00e-04 Train R2: 0.5991 \t Test R: 0.5385 \t Nonzero coef: 0, 0, 2, 105\n",
      "Parameter: 5.00e-04 Train R2: 0.5749 \t Test R: 0.5366 \t Nonzero coef: 0, 0, 0, 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.827e-02, tolerance: 1.373e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "for a in (1e-4)*np.array([3,4,5]):#[0,0.1,0.2,0.4,0.6,0.8,1,2,3,4,5,10,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, pt_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d, %d, %d, %d\" % \n",
    "                                  (a, lasso.score(x_train, pt_train), \n",
    "                                  lasso.score(x_test, pt_test), \n",
    "                                  np.sum(lasso.coef_[:dim_demo] != 0), \n",
    "                                  np.sum(lasso.coef_[dim_demo:dim_demo+dim_embed_ssd] != 0), \n",
    "                                  np.sum(lasso.coef_[dim_demo+dim_embed_ssd:dim_demo+dim_embed_ssd+dim_embed_ae] != 0),\n",
    "                                  np.sum(lasso.coef_[dim_demo+dim_embed_ssd+dim_embed_ae:] != 0)))\n",
    "\n",
    "    with open(out_dir+\"AllModels_A_LR.csv\", \"a\") as f:\n",
    "        f.write(\"%s,%.6f,%s,%.4f,%.4f,%s,%d,%d\\n\" % ('All',a,'pt',\n",
    "            lasso.score(x_train, pt_train), lasso.score(x_test, pt_test), 'LR', \n",
    "            np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "\n",
    "for a in (5e0)*np.array([0,0.1,1,2,2.2,2.5,3,3.5,4,5,6,20,50]):\n",
    "\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, pt_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, pt_train), \n",
    "                                                              ridge.score(x_test, pt_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.350e-01, tolerance: 3.791e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 3.00e-04 Train R2: 0.7150 \t Test R: 0.5331 \t Nonzero coef: 3, 0, 12, 272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.346e-01, tolerance: 3.791e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 4.00e-04 Train R2: 0.6591 \t Test R: 0.5372 \t Nonzero coef: 3, 0, 4, 196\n",
      "Parameter: 5.00e-04 Train R2: 0.6200 \t Test R: 0.5452 \t Nonzero coef: 2, 0, 1, 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.293e-01, tolerance: 3.791e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "for a in (1e-4)*np.array([3,4,5]):#[0,0.1,0.2,0.4,0.6,0.8,1,2,3,4,5,10,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, active_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d, %d, %d, %d\" % \n",
    "                                  (a, lasso.score(x_train, active_train), \n",
    "                                  lasso.score(x_test, active_test), \n",
    "                                  np.sum(lasso.coef_[:dim_demo] != 0), \n",
    "                                  np.sum(lasso.coef_[dim_demo:dim_demo+dim_embed_ssd] != 0), \n",
    "                                  np.sum(lasso.coef_[dim_demo+dim_embed_ssd:dim_demo+dim_embed_ssd+dim_embed_ae] != 0),\n",
    "                                  np.sum(lasso.coef_[dim_demo+dim_embed_ssd+dim_embed_ae:] != 0)))\n",
    " \n",
    "    with open(out_dir+\"AllModels_A_LR.csv\", \"a\") as f:\n",
    "        f.write(\"%s,%.6f,%s,%.4f,%.4f,%s,%d,%d\\n\" % ('All',a,'active',\n",
    "            lasso.score(x_train, active_train), lasso.score(x_test, active_test), 'LR', \n",
    "            np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "\n",
    "for a in (5e0)*np.array([0,0.1,1,1.5,2,2.5,3,4,10,20,50]):\n",
    "\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, active_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, active_train), \n",
    "                                                              ridge.score(x_test, active_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Trip Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in (1e-3)*np.array([0,0.1,6,7,8,10,11,12,13,14,15,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, trpgen_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d, %d\" % (a, lasso.score(x_train, trpgen_train), \n",
    "                                                                                  lasso.score(x_test, trpgen_test), \n",
    "                                                                                  np.sum(lasso.coef_[:dim_demo] != 0), \n",
    "                                                                                  np.sum(lasso.coef_[dim_demo:] != 0)))\n",
    "\n",
    "#     with open(out_dir+\"BA_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%.6f,%.4f,%.4f,%s,%d,%d\\n\" % (a, \n",
    "#             lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test), 'lasso', \n",
    "#             np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader and model definition\n",
    "\n",
    "trainset = SurveyDataset(torch.tensor(x_train,  dtype=torch.float), torch.tensor(y_train, dtype=torch.float))\n",
    "trainloader = DataLoader(trainset, batch_size=len(trainset), shuffle=False)\n",
    "\n",
    "testset = SurveyDataset(torch.tensor(x_test, dtype=torch.float), torch.tensor(y_test, dtype=torch.float))\n",
    "testloader = DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "\n",
    "kldivloss = nn.KLDivLoss(reduction='sum')\n",
    "mseloss = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_train = np.sum(np.power(y_train - np.mean(y_train, axis=0), 2), axis=0)\n",
    "sst_test = np.sum(np.power(y_test - np.mean(y_test, axis=0), 2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mnl_torch(lr1_list, wd1_list, lr2_list, wd2_list, dim_demo, dim_embed):\n",
    "    \n",
    "    for (lr1, wd1, lr2, wd2) in itertools.product(lr1_list, wd1_list, lr2_list, wd2_list):\n",
    "        \n",
    "        print(f\"[lr1: {lr1:.3f}, wd1: {wd1:3.2e}, lr2: {lr2:.3f}, wd2: {wd2:3.2e}]\")\n",
    "\n",
    "        # model setup\n",
    "        model = mnl.MNL2(n_alts=4, dim_embed=dim_embed, dim_demo=dim_demo)\n",
    "        \n",
    "        embed_params = []\n",
    "        demo_params = []\n",
    "        other_params = []\n",
    "        for name, m in model.named_parameters():\n",
    "    #             print(name)\n",
    "            if 'embed' in name:\n",
    "                embed_params.append(m)\n",
    "            elif 'demo' in name:\n",
    "                demo_params.append(m)\n",
    "            else:\n",
    "                other_params.append(m)\n",
    "\n",
    "        optimizer = torch.optim.Adam([{'params':embed_params,'weight_decay':wd1,'lr':lr1},\n",
    "                                      {'params':demo_params,'weight_decay':wd2, 'lr':lr2}])\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        # model training\n",
    "        converged = 0\n",
    "        ref1 = 0\n",
    "        ref2 = 0\n",
    "\n",
    "        for epoch in range(5000):\n",
    "\n",
    "            kl_ = 0\n",
    "            mse_ = 0\n",
    "            mse1_ = 0\n",
    "            mse2_ = 0\n",
    "            mse3_ = 0\n",
    "            mse4_ = 0\n",
    "\n",
    "            for batch, (x_batch, y_batch) in enumerate(trainloader):\n",
    "                \n",
    "                # Compute prediction and loss\n",
    "                util = model(x_batch)\n",
    "                probs = torch.log(nn.functional.softmax(util, dim=1))\n",
    "                kl = kldivloss(probs, y_batch)\n",
    "        #         kl = kldivloss(torch.log(util), y_batch)\n",
    "                kl_ += kl.item()\n",
    "\n",
    "                mse = mseloss(torch.exp(probs), y_batch)\n",
    "        #         mse = mseloss(util, y_batch)\n",
    "                mse_ += mse.sum().item()\n",
    "                mse1_ += mse[:,0].sum().item()\n",
    "                mse2_ += mse[:,1].sum().item()\n",
    "                mse3_ += mse[:,2].sum().item()\n",
    "                mse4_ += mse[:,3].sum().item()\n",
    "                mse = mse.sum()\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                kl.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_kl = kl_/len(trainset)\n",
    "            train_mse = np.sqrt(mse_/len(trainset))\n",
    "            train_mse1 = np.sqrt(mse1_/len(trainset))\n",
    "            train_mse2 = np.sqrt(mse2_/len(trainset))\n",
    "            train_mse3 = np.sqrt(mse3_/len(trainset))\n",
    "            train_mse4 = np.sqrt(mse4_/len(trainset))\n",
    "\n",
    "            train_r1 = 1-mse1_/sst_train[0]\n",
    "            train_r2 = 1-mse2_/sst_train[1]\n",
    "            train_r3 = 1-mse3_/sst_train[2]\n",
    "            train_r4 = 1-mse4_/sst_train[3]\n",
    "\n",
    "            loss_ = train_kl\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "\n",
    "                kl_ = 0\n",
    "                mse_ = 0 \n",
    "                mse1_ = 0\n",
    "                mse2_ = 0\n",
    "                mse3_ = 0\n",
    "                mse4_ = 0\n",
    "\n",
    "                for batch, (x_batch, y_batch) in enumerate(testloader):\n",
    "                    \n",
    "                    util = model(x_batch)\n",
    "                    probs = torch.log(nn.functional.softmax(util,dim=1))\n",
    "                    kl = kldivloss(probs, y_batch)\n",
    "            #         kl = kldivloss(torch.log(util), y_batch)\n",
    "                    kl_ += kl.item()\n",
    "\n",
    "                    mse = mseloss(torch.exp(probs), y_batch)\n",
    "            #         mse = mseloss(util, y_batch)\n",
    "                    mse_ += mse.sum().item()\n",
    "                    mse1_ += mse[:,0].sum().item()\n",
    "                    mse2_ += mse[:,1].sum().item()\n",
    "                    mse3_ += mse[:,2].sum().item()\n",
    "                    mse4_ += mse[:,3].sum().item()\n",
    "\n",
    "                test_kl = kl_/len(testset)\n",
    "                test_mse = np.sqrt(mse_/len(testset))\n",
    "                test_mse1 = np.sqrt(mse1_/len(testset))\n",
    "                test_mse2 = np.sqrt(mse2_/len(testset))\n",
    "                test_mse3 = np.sqrt(mse3_/len(testset))\n",
    "                test_mse4 = np.sqrt(mse4_/len(testset))\n",
    "                \n",
    "                r1 = r2_score(y_batch.numpy()[:,0],torch.exp(probs).detach().numpy()[:,0])\n",
    "                r2 = r2_score(y_batch.numpy()[:,1],torch.exp(probs).detach().numpy()[:,1])\n",
    "                r3 = r2_score(y_batch.numpy()[:,2],torch.exp(probs).detach().numpy()[:,2])\n",
    "                r4 = r2_score(y_batch.numpy()[:,3],torch.exp(probs).detach().numpy()[:,3])\n",
    "\n",
    "                if epoch >= 40:\n",
    "                    if (np.abs(loss_ - ref1)/ref1<0.001) & (np.abs(loss_ - ref2)/ref2<0.001):\n",
    "                        print(\"Early stopping at epoch\", epoch)\n",
    "                        converged = 1\n",
    "                        break\n",
    "                    if (ref1 < loss_) & (ref1 < ref2):\n",
    "                        print(\"Diverging. stop.\")\n",
    "                        break\n",
    "                    if loss_ < best:\n",
    "                        best = loss_\n",
    "                        best_epoch = epoch\n",
    "                        output = (best_epoch, train_kl, train_mse, train_mse1, train_mse2, train_mse3, train_mse4,\n",
    "                                  test_kl, test_mse, test_mse1, test_mse2, test_mse3, test_mse4,\n",
    "                                  train_r1, train_r2, train_r3, train_r4, r1, r2, r3, r4)\n",
    "                else:\n",
    "                    best = loss_\n",
    "                    best_epoch = epoch\n",
    "                    output = (best_epoch, train_kl, train_mse, train_mse1, train_mse2, train_mse3, train_mse4,\n",
    "                                  test_kl, test_mse, test_mse1, test_mse2, test_mse3, test_mse4,\n",
    "                                  train_r1, train_r2, train_r3, train_r4, r1, r2, r3, r4)\n",
    "                ref2 = ref1\n",
    "                ref1 = loss_\n",
    "\n",
    "            if epoch % 300 == 0:\n",
    "\n",
    "                    print(f\"[epoch: {epoch:>3d}] Train KL loss: {train_kl:.3f} RMSE {train_mse:.3f}\")\n",
    "                       # {train_mse1:.3f} {train_mse2:.3f} {train_mse3:.3f} {train_mse4:.3f}\")\n",
    "                    print(f\"\\t\\t\\t\\t\\t\\t Train R2 score: {train_r1:.3f} {train_r2:.3f} {train_r3:.3f} {train_r4:.3f} \")\n",
    "                    print(f\"[epoch: {epoch:>3d}] Test KL loss: {kl_/len(testset):.3f} RMSE {np.sqrt(mse_/len(testset)):.3f}\")\n",
    "                       #     {np.sqrt(mse1_/len(testset)):.3f} {np.sqrt(mse2_/len(testset)):.3f} {np.sqrt(mse3_/len(testset)):.3f} {np.sqrt(mse4_/len(testset)):.3f}\")\n",
    "                    print(f\"\\t\\t\\t\\t\\t\\t Test R2 score: {r1:.3f} {r2:.3f} {r3:.3f} {r4:.3f} \")\n",
    "\n",
    "                    print(f\"[epoch: {epoch:>3d}] Train KL loss: {train_kl:.3f} Train R2 score: {train_r1:.3f} {train_r2:.3f} {train_r3:.3f} {train_r4:.3f} \")\n",
    "                    print(f\"[epoch: {epoch:>3d}] Test KL loss: {kl_/len(testset):.3f} Test R2 score: {r1:.3f} {r2:.3f} {r3:.3f} {r4:.3f} \")\n",
    "\n",
    "        with open(out_dir+\"AllModels_A_MNL.csv\", \"a\") as f:\n",
    "            f.write(\"%s,%.1E,%.1E,%d,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%d\\n\" % \n",
    "                    (('All',lr1,wd1)+output+(converged,)))\n",
    "\n",
    "        print(f\"[epoch: {best_epoch:>3d}] Train KL loss: {output[1]:.3f} Train R2 score: {output[13]:.3f} {output[14]:.3f} {output[15]:.3f} {output[16]:.3f} \")\n",
    "        print(f\"[epoch: {best_epoch:>3d}] Test KL loss: {output[7]:.3f} Test R2 score: {output[17]:.3f} {output[18]:.3f} {output[19]:.3f} {output[20]:.3f} \")\n",
    "        print()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lr1: 0.000, wd1: 1.00e-01, lr2: 0.010, wd2: 0.00e+00]\n",
      "Early stopping at epoch 1550\n",
      "[epoch: 1545] Train KL loss: 0.093 Train R2 score: 0.707 0.766 0.278 0.728 \n",
      "[epoch: 1545] Test KL loss: 0.104 Test R2 score: 0.578 0.712 -0.151 0.465 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+00, lr2: 0.010, wd2: 0.00e+00]\n",
      "Diverging. stop.\n",
      "[epoch: 1425] Train KL loss: 0.095 Train R2 score: 0.696 0.759 0.252 0.708 \n",
      "[epoch: 1425] Test KL loss: 0.104 Test R2 score: 0.573 0.707 -0.169 0.492 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+01, lr2: 0.010, wd2: 0.00e+00]\n",
      "Diverging. stop.\n",
      "[epoch: 1500] Train KL loss: 0.097 Train R2 score: 0.690 0.757 0.196 0.708 \n",
      "[epoch: 1500] Test KL loss: 0.104 Test R2 score: 0.577 0.710 -0.165 0.480 \n",
      "\n",
      "[lr1: 0.000, wd1: 5.00e+01, lr2: 0.010, wd2: 0.00e+00]\n",
      "Early stopping at epoch 1325\n",
      "[epoch: 1320] Train KL loss: 0.103 Train R2 score: 0.667 0.740 0.157 0.662 \n",
      "[epoch: 1320] Test KL loss: 0.102 Test R2 score: 0.581 0.719 -0.112 0.506 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+02, lr2: 0.010, wd2: 0.00e+00]\n",
      "Early stopping at epoch 1195\n",
      "[epoch: 1190] Train KL loss: 0.108 Train R2 score: 0.640 0.718 0.114 0.641 \n",
      "[epoch: 1190] Test KL loss: 0.103 Test R2 score: 0.570 0.712 -0.123 0.511 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+03, lr2: 0.010, wd2: 0.00e+00]\n",
      "Diverging. stop.\n",
      "[epoch:  35] Train KL loss: 0.152 Train R2 score: 0.465 0.537 -0.008 0.260 \n",
      "[epoch:  35] Test KL loss: 0.121 Test R2 score: 0.500 0.608 -0.050 0.381 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e-01, lr2: 0.010, wd2: 0.00e+00]\n",
      "Diverging. stop.\n",
      "[epoch:  45] Train KL loss: 0.145 Train R2 score: 0.465 0.562 -0.013 0.412 \n",
      "[epoch:  45] Test KL loss: 0.130 Test R2 score: 0.491 0.540 -0.372 0.351 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+00, lr2: 0.010, wd2: 0.00e+00]\n",
      "Diverging. stop.\n",
      "[epoch: 1340] Train KL loss: 0.097 Train R2 score: 0.687 0.755 0.236 0.715 \n",
      "[epoch: 1340] Test KL loss: 0.103 Test R2 score: 0.577 0.718 -0.146 0.487 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+01, lr2: 0.010, wd2: 0.00e+00]\n",
      "Diverging. stop.\n",
      "[epoch: 1555] Train KL loss: 0.097 Train R2 score: 0.700 0.759 0.185 0.720 \n",
      "[epoch: 1555] Test KL loss: 0.103 Test R2 score: 0.581 0.712 -0.148 0.476 \n",
      "\n",
      "[lr1: 0.000, wd1: 5.00e+01, lr2: 0.010, wd2: 0.00e+00]\n",
      "Diverging. stop.\n",
      "[epoch:  35] Train KL loss: 0.155 Train R2 score: 0.414 0.551 -0.050 0.402 \n",
      "[epoch:  35] Test KL loss: 0.122 Test R2 score: 0.516 0.599 -0.094 0.334 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+02, lr2: 0.010, wd2: 0.00e+00]\n",
      "Early stopping at epoch 1240\n",
      "[epoch: 1235] Train KL loss: 0.108 Train R2 score: 0.640 0.718 0.110 0.644 \n",
      "[epoch: 1235] Test KL loss: 0.103 Test R2 score: 0.573 0.709 -0.122 0.508 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+03, lr2: 0.010, wd2: 0.00e+00]\n",
      "Early stopping at epoch 430\n",
      "[epoch: 425] Train KL loss: 0.127 Train R2 score: 0.546 0.647 0.028 0.534 \n",
      "[epoch: 425] Test KL loss: 0.107 Test R2 score: 0.553 0.697 -0.112 0.539 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e-01, lr2: 0.010, wd2: 0.00e+00]\n",
      "Diverging. stop.\n",
      "[epoch:  45] Train KL loss: 0.146 Train R2 score: 0.466 0.555 -0.005 0.399 \n",
      "[epoch:  45] Test KL loss: 0.125 Test R2 score: 0.479 0.597 -0.362 0.426 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+00, lr2: 0.010, wd2: 0.00e+00]\n",
      "Diverging. stop.\n",
      "[epoch:  35] Train KL loss: 0.160 Train R2 score: 0.332 0.513 -0.055 0.440 \n",
      "[epoch:  35] Test KL loss: 0.140 Test R2 score: 0.372 0.499 -0.604 0.425 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+01, lr2: 0.010, wd2: 0.00e+00]\n",
      "Diverging. stop.\n",
      "[epoch: 1380] Train KL loss: 0.099 Train R2 score: 0.689 0.755 0.182 0.685 \n",
      "[epoch: 1380] Test KL loss: 0.103 Test R2 score: 0.580 0.717 -0.130 0.486 \n",
      "\n",
      "[lr1: 0.000, wd1: 5.00e+01, lr2: 0.010, wd2: 0.00e+00]\n",
      "Early stopping at epoch 1620\n",
      "[epoch: 1615] Train KL loss: 0.101 Train R2 score: 0.680 0.745 0.169 0.683 \n",
      "[epoch: 1615] Test KL loss: 0.103 Test R2 score: 0.581 0.713 -0.132 0.488 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+02, lr2: 0.010, wd2: 0.00e+00]\n",
      "Early stopping at epoch 1185\n",
      "[epoch: 1180] Train KL loss: 0.108 Train R2 score: 0.642 0.717 0.111 0.642 \n",
      "[epoch: 1180] Test KL loss: 0.103 Test R2 score: 0.575 0.714 -0.134 0.508 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+03, lr2: 0.010, wd2: 0.00e+00]\n",
      "Diverging. stop.\n",
      "[epoch:  35] Train KL loss: 0.150 Train R2 score: 0.437 0.543 -0.023 0.356 \n",
      "[epoch:  35] Test KL loss: 0.120 Test R2 score: 0.497 0.607 -0.065 0.392 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e-01, lr2: 0.010, wd2: 0.00e+00]\n",
      "Diverging. stop.\n",
      "[epoch: 1205] Train KL loss: 0.101 Train R2 score: 0.687 0.746 0.163 0.665 \n",
      "[epoch: 1205] Test KL loss: 0.103 Test R2 score: 0.577 0.708 -0.148 0.497 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+00, lr2: 0.010, wd2: 0.00e+00]\n",
      "Early stopping at epoch 1360\n",
      "[epoch: 1355] Train KL loss: 0.098 Train R2 score: 0.683 0.755 0.241 0.684 \n",
      "[epoch: 1355] Test KL loss: 0.104 Test R2 score: 0.572 0.711 -0.147 0.490 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+01, lr2: 0.010, wd2: 0.00e+00]\n",
      "Diverging. stop.\n",
      "[epoch: 1165] Train KL loss: 0.103 Train R2 score: 0.685 0.737 0.141 0.649 \n",
      "[epoch: 1165] Test KL loss: 0.103 Test R2 score: 0.573 0.712 -0.142 0.507 \n",
      "\n",
      "[lr1: 0.000, wd1: 5.00e+01, lr2: 0.010, wd2: 0.00e+00]\n",
      "Diverging. stop.\n",
      "[epoch:  35] Train KL loss: 0.150 Train R2 score: 0.421 0.538 -0.015 0.433 \n",
      "[epoch:  35] Test KL loss: 0.135 Test R2 score: 0.293 0.532 -0.056 0.430 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+02, lr2: 0.010, wd2: 0.00e+00]\n",
      "Early stopping at epoch 1220\n",
      "[epoch: 1215] Train KL loss: 0.108 Train R2 score: 0.649 0.722 0.105 0.644 \n",
      "[epoch: 1215] Test KL loss: 0.103 Test R2 score: 0.573 0.712 -0.124 0.509 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+03, lr2: 0.010, wd2: 0.00e+00]\n",
      "Early stopping at epoch 430\n",
      "[epoch: 425] Train KL loss: 0.127 Train R2 score: 0.546 0.646 0.026 0.540 \n",
      "[epoch: 425] Test KL loss: 0.107 Test R2 score: 0.551 0.701 -0.101 0.514 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e-01, lr2: 0.010, wd2: 0.00e+00]\n",
      "Diverging. stop.\n",
      "[epoch:  35] Train KL loss: 0.151 Train R2 score: 0.422 0.530 -0.008 0.431 \n",
      "[epoch:  35] Test KL loss: 0.134 Test R2 score: 0.350 0.538 -0.032 0.391 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+00, lr2: 0.010, wd2: 0.00e+00]\n",
      "Diverging. stop.\n",
      "[epoch: 1500] Train KL loss: 0.097 Train R2 score: 0.699 0.760 0.173 0.725 \n",
      "[epoch: 1500] Test KL loss: 0.103 Test R2 score: 0.577 0.717 -0.134 0.485 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+01, lr2: 0.010, wd2: 0.00e+00]\n",
      "Early stopping at epoch 1315\n",
      "[epoch: 1310] Train KL loss: 0.100 Train R2 score: 0.682 0.750 0.153 0.704 \n",
      "[epoch: 1310] Test KL loss: 0.102 Test R2 score: 0.583 0.720 -0.134 0.490 \n",
      "\n",
      "[lr1: 0.000, wd1: 5.00e+01, lr2: 0.010, wd2: 0.00e+00]\n",
      "Early stopping at epoch 1310\n",
      "[epoch: 1305] Train KL loss: 0.103 Train R2 score: 0.672 0.737 0.134 0.678 \n",
      "[epoch: 1305] Test KL loss: 0.104 Test R2 score: 0.571 0.708 -0.126 0.485 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+02, lr2: 0.010, wd2: 0.00e+00]\n",
      "Early stopping at epoch 1235\n",
      "[epoch: 1230] Train KL loss: 0.109 Train R2 score: 0.640 0.717 0.109 0.643 \n",
      "[epoch: 1230] Test KL loss: 0.104 Test R2 score: 0.572 0.710 -0.124 0.505 \n",
      "\n",
      "[lr1: 0.000, wd1: 1.00e+03, lr2: 0.010, wd2: 0.00e+00]\n",
      "Early stopping at epoch 415\n",
      "[epoch: 410] Train KL loss: 0.127 Train R2 score: 0.545 0.647 0.028 0.537 \n",
      "[epoch: 410] Test KL loss: 0.107 Test R2 score: 0.551 0.693 -0.110 0.515 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    mnl_torch(lr1_list=[1e-4], wd1_list=[0.1,1,10,50,100,1000], lr2_list=[1e-2], wd2_list=[0], \n",
    "              dim_demo=dim_demo, dim_embed=dim_embed_ae+dim_embed_sae+dim_embed_ssd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qingyi",
   "language": "python",
   "name": "qingyi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
