{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c8bee22",
   "metadata": {},
   "source": [
    "This notebook documents part 1 of the complementarity of image and demographic information: the ability of demographics to predict mode choice and trip generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dc2bd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"models/\")\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataloader import SurveyDataset, load_demo, load_aggregate_travel_behavior, train_test_split\n",
    "import mnl\n",
    "import linear_reg\n",
    "from setup import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343da39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_version = '1571'\n",
    "variable_names = ['active','auto','mas','pt','trpgen']\n",
    "\n",
    "demo_variables = ['tot_population','pct25_34yrs','pct35_50yrs','pctover65yrs',\n",
    "         'pctwhite_alone','pct_nonwhite','pctblack_alone',\n",
    "         'pct_col_grad','avg_tt_to_work','inc_per_capita']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8939bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target variables y\n",
    "\n",
    "file = \"origin_trip_behavior.csv\"\n",
    "df_pivot = load_aggregate_travel_behavior(file, data_version)\n",
    "\n",
    "train_test_index = df_pivot['train_test'].astype(bool)\n",
    "y = df_pivot[variable_names].to_numpy()\n",
    "y_train = y[~train_test_index, :4]\n",
    "y_test = y[train_test_index, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf49277",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_train = np.sum(np.power(y_train - np.mean(y_train, axis=0), 2), axis=0)\n",
    "sst_test = np.sum(np.power(y_test - np.mean(y_test, axis=0), 2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "684331c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanatory variables x\n",
    "\n",
    "demo_variables = ['tot_population','pct25_34yrs','pct35_50yrs','pctover65yrs',\n",
    "         'pctwhite_alone','pct_nonwhite','pctblack_alone',\n",
    "         'pct_col_grad','avg_tt_to_work','inc_per_capita']\n",
    "\n",
    "demo_cs, demo_np = load_demo(data_dir)\n",
    "demo = np.hstack((np.array(demo_cs).reshape(-1,1), demo_np))\n",
    "demo = pd.DataFrame(demo, columns = ['geoid'] + demo_variables)\n",
    "demo_split = train_test_split(demo, data_version)\n",
    "\n",
    "x_train = demo_split[~demo_split['train_test'].astype(bool)][demo_variables].to_numpy(dtype=float)\n",
    "x_test = demo_split[demo_split['train_test'].astype(bool)][demo_variables].to_numpy(dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00bca47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1173614 ,  0.77258279,  0.04028305,  0.06977276, 13.70586056])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33350f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_train = y[~train_test_index,1]\n",
    "auto_test = y[train_test_index,1]\n",
    "\n",
    "pt_train = y[~train_test_index,3]\n",
    "pt_test = y[train_test_index,3]\n",
    "\n",
    "active_train = y[~train_test_index,0]\n",
    "active_test = y[train_test_index,0]\n",
    "\n",
    "trpgen_train = y[~train_test_index,-1]\n",
    "trpgen_test = y[train_test_index,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2839a62",
   "metadata": {},
   "source": [
    "# 1. Linear Regresion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74e39a0",
   "metadata": {},
   "source": [
    "Highlight:\n",
    "\n",
    "Auto\n",
    "\n",
    "Parameter: 0.00e+00 Train R2: 0.4444 \t Test R: 0.5033\n",
    "\n",
    "PT\n",
    "\n",
    "Parameter: 0.00e+00 Train R2: 0.3893 \t Test R: 0.2894\n",
    "\n",
    "Active\n",
    "\n",
    "Parameter: 0.00e+00 Train R2: 0.3505 \t Test R: 0.4702\n",
    "\n",
    "Trip Generation\n",
    "\n",
    "Ridge Parameter: 4.00e+00 Train R2: 0.2159 \t Test R: 0.1730\n",
    "\n",
    "Lasso Parameter: 1.00e-02 Train R2: 0.2387 \t Test R: 0.1578 \t Nonzero coef: 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da12409f",
   "metadata": {},
   "source": [
    "### 1.1 Auto Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "404b8cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.4444 \t Test R2: 0.5033\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression without Regularization\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(x_train, auto_train)\n",
    "# with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#     f.write(\"%s,%s,%s,%.4f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], -1, \n",
    "#         lr.score(x_train, auto_train), lr.score(x_test, auto_test), 'lr', zoomlevel,\n",
    "#         np.sum(lr.coef_ != 0), len(lr.coef_)))\n",
    "print(\"Train R2: %.4f \\t Test R2: %.4f\" % (lr.score(x_train, auto_train), lr.score(x_test, auto_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e555b2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.4444 \t Test R: 0.5033 \t Nonzero coef: 10\n",
      "Parameter: 1.00e-05 Train R2: 0.4444 \t Test R: 0.5031 \t Nonzero coef: 10\n",
      "Parameter: 1.00e-04 Train R2: 0.4436 \t Test R: 0.5018 \t Nonzero coef: 10\n",
      "Parameter: 2.00e-04 Train R2: 0.4413 \t Test R: 0.4997 \t Nonzero coef: 10\n",
      "Parameter: 3.00e-04 Train R2: 0.4403 \t Test R: 0.4991 \t Nonzero coef: 8\n",
      "Parameter: 4.00e-04 Train R2: 0.4395 \t Test R: 0.4986 \t Nonzero coef: 8\n",
      "Parameter: 5.00e-04 Train R2: 0.4384 \t Test R: 0.4979 \t Nonzero coef: 8\n",
      "Parameter: 6.00e-04 Train R2: 0.4372 \t Test R: 0.4970 \t Nonzero coef: 7\n",
      "Parameter: 7.00e-04 Train R2: 0.4360 \t Test R: 0.4959 \t Nonzero coef: 7\n",
      "Parameter: 8.00e-04 Train R2: 0.4347 \t Test R: 0.4947 \t Nonzero coef: 7\n",
      "Parameter: 1.00e-03 Train R2: 0.4315 \t Test R: 0.4918 \t Nonzero coef: 7\n",
      "Parameter: 2.00e-03 Train R2: 0.4149 \t Test R: 0.4724 \t Nonzero coef: 5\n",
      "Parameter: 5.00e-03 Train R2: 0.3792 \t Test R: 0.4217 \t Nonzero coef: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  after removing the cwd from sys.path.\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.140e+01, tolerance: 7.704e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "for a in (1e-4)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, auto_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d\" % (a, lasso.score(x_train, auto_train), \n",
    "                                                                                  lasso.score(x_test, auto_test), \n",
    "                                                                                  np.sum(lasso.coef_ != 0)))\n",
    "\n",
    "#     with open(out_dir+\"BA_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%.6f,%.4f,%.4f,%s,%d,%d\\n\" % (a, \n",
    "#             lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test), 'lasso', \n",
    "#             np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e7afbb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.4444 \t Test R: 0.5033\n",
      "Parameter: 1.00e+00 Train R2: 0.4426 \t Test R: 0.5008\n",
      "Parameter: 1.00e+01 Train R2: 0.4197 \t Test R: 0.4778\n",
      "Parameter: 2.00e+01 Train R2: 0.3918 \t Test R: 0.4469\n",
      "Parameter: 3.00e+01 Train R2: 0.3660 \t Test R: 0.4171\n",
      "Parameter: 4.00e+01 Train R2: 0.3430 \t Test R: 0.3900\n",
      "Parameter: 5.00e+01 Train R2: 0.3224 \t Test R: 0.3655\n",
      "Parameter: 6.00e+01 Train R2: 0.3041 \t Test R: 0.3437\n",
      "Parameter: 7.00e+01 Train R2: 0.2878 \t Test R: 0.3241\n",
      "Parameter: 8.00e+01 Train R2: 0.2731 \t Test R: 0.3065\n",
      "Parameter: 1.00e+02 Train R2: 0.2478 \t Test R: 0.2763\n",
      "Parameter: 2.00e+02 Train R2: 0.1701 \t Test R: 0.1845\n",
      "Parameter: 5.00e+02 Train R2: 0.0888 \t Test R: 0.0919\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "\n",
    "for a in (1e+1)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, auto_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, auto_train), \n",
    "                                                              ridge.score(x_test, auto_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f57018a",
   "metadata": {},
   "source": [
    "### 1.2 PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "49eb787c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.3893 \t Test R2: 0.2894\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression without Regularization\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(x_train, pt_train)\n",
    "# with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#     f.write(\"%s,%s,%s,%.4f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], -1, \n",
    "#         lr.score(x_train, auto_train), lr.score(x_test, auto_test), 'lr', zoomlevel,\n",
    "#         np.sum(lr.coef_ != 0), len(lr.coef_)))\n",
    "print(\"Train R2: %.4f \\t Test R2: %.4f\" % (lr.score(x_train, pt_train), lr.score(x_test, pt_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eabe6903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.3893 \t Test R: 0.2894 \t Nonzero coef: 10\n",
      "Parameter: 1.00e-04 Train R2: 0.3852 \t Test R: 0.2904 \t Nonzero coef: 9\n",
      "Parameter: 1.00e-03 Train R2: 0.3498 \t Test R: 0.2752 \t Nonzero coef: 5\n",
      "Parameter: 2.00e-03 Train R2: 0.3167 \t Test R: 0.2463 \t Nonzero coef: 4\n",
      "Parameter: 3.00e-03 Train R2: 0.2785 \t Test R: 0.2165 \t Nonzero coef: 3\n",
      "Parameter: 4.00e-03 Train R2: 0.2351 \t Test R: 0.1876 \t Nonzero coef: 3\n",
      "Parameter: 5.00e-03 Train R2: 0.1792 \t Test R: 0.1461 \t Nonzero coef: 3\n",
      "Parameter: 6.00e-03 Train R2: 0.1109 \t Test R: 0.0920 \t Nonzero coef: 3\n",
      "Parameter: 7.00e-03 Train R2: 0.0302 \t Test R: 0.0252 \t Nonzero coef: 3\n",
      "Parameter: 8.00e-03 Train R2: 0.0000 \t Test R: -0.0000 \t Nonzero coef: 0\n",
      "Parameter: 1.00e-02 Train R2: 0.0000 \t Test R: -0.0000 \t Nonzero coef: 0\n",
      "Parameter: 2.00e-02 Train R2: 0.0000 \t Test R: -0.0000 \t Nonzero coef: 0\n",
      "Parameter: 5.00e-02 Train R2: 0.0000 \t Test R: -0.0000 \t Nonzero coef: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  import sys\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.191e+00, tolerance: 1.373e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "for a in (1e-3)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, pt_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d\" % (a, lasso.score(x_train, pt_train), \n",
    "                                                                                  lasso.score(x_test, pt_test), \n",
    "                                                                                  np.sum(lasso.coef_ != 0)))\n",
    "\n",
    "#     with open(out_dir+\"BA_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%.6f,%.4f,%.4f,%s,%d,%d\\n\" % (a, \n",
    "#             lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test), 'lasso', \n",
    "#             np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62e75a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.3893 \t Test R: 0.2894\n",
      "Parameter: 1.00e+00 Train R2: 0.3838 \t Test R: 0.2934\n",
      "Parameter: 1.00e+01 Train R2: 0.3567 \t Test R: 0.2929\n",
      "Parameter: 2.00e+01 Train R2: 0.3327 \t Test R: 0.2810\n",
      "Parameter: 3.00e+01 Train R2: 0.3117 \t Test R: 0.2672\n",
      "Parameter: 4.00e+01 Train R2: 0.2934 \t Test R: 0.2537\n",
      "Parameter: 5.00e+01 Train R2: 0.2774 \t Test R: 0.2411\n",
      "Parameter: 6.00e+01 Train R2: 0.2632 \t Test R: 0.2295\n",
      "Parameter: 7.00e+01 Train R2: 0.2507 \t Test R: 0.2189\n",
      "Parameter: 8.00e+01 Train R2: 0.2394 \t Test R: 0.2093\n",
      "Parameter: 1.00e+02 Train R2: 0.2200 \t Test R: 0.1924\n",
      "Parameter: 2.00e+02 Train R2: 0.1594 \t Test R: 0.1386\n",
      "Parameter: 5.00e+02 Train R2: 0.0912 \t Test R: 0.0782\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "\n",
    "for a in (1e+1)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, pt_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, pt_train), \n",
    "                                                              ridge.score(x_test, pt_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f24acf",
   "metadata": {},
   "source": [
    "### 1.3 Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1937c2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.3505 \t Test R: 0.4702 \t Nonzero coef: 10\n",
      "Parameter: 1.00e-04 Train R2: 0.3500 \t Test R: 0.4715 \t Nonzero coef: 9\n",
      "Parameter: 1.00e-03 Train R2: 0.3310 \t Test R: 0.4598 \t Nonzero coef: 6\n",
      "Parameter: 2.00e-03 Train R2: 0.3152 \t Test R: 0.4355 \t Nonzero coef: 4\n",
      "Parameter: 3.00e-03 Train R2: 0.2995 \t Test R: 0.4077 \t Nonzero coef: 4\n",
      "Parameter: 4.00e-03 Train R2: 0.2776 \t Test R: 0.3734 \t Nonzero coef: 4\n",
      "Parameter: 5.00e-03 Train R2: 0.2495 \t Test R: 0.3325 \t Nonzero coef: 3\n",
      "Parameter: 6.00e-03 Train R2: 0.2281 \t Test R: 0.3004 \t Nonzero coef: 2\n",
      "Parameter: 7.00e-03 Train R2: 0.2030 \t Test R: 0.2641 \t Nonzero coef: 2\n",
      "Parameter: 8.00e-03 Train R2: 0.1739 \t Test R: 0.2237 \t Nonzero coef: 2\n",
      "Parameter: 1.00e-02 Train R2: 0.1043 \t Test R: 0.1304 \t Nonzero coef: 2\n",
      "Parameter: 2.00e-02 Train R2: 0.0000 \t Test R: -0.0003 \t Nonzero coef: 0\n",
      "Parameter: 5.00e-02 Train R2: 0.0000 \t Test R: -0.0003 \t Nonzero coef: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  \n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.231e+01, tolerance: 3.791e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    }
   ],
   "source": [
    "for a in (1e-3)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, active_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d\" % (a, lasso.score(x_train, active_train), \n",
    "                                                                                  lasso.score(x_test, active_test), \n",
    "                                                                                  np.sum(lasso.coef_ != 0)))\n",
    "\n",
    "#     with open(out_dir+\"BA_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%.6f,%.4f,%.4f,%s,%d,%d\\n\" % (a, \n",
    "#             lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test), 'lasso', \n",
    "#             np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea131a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.3505 \t Test R: 0.4702\n",
      "Parameter: 1.00e+00 Train R2: 0.3500 \t Test R: 0.4696\n",
      "Parameter: 1.00e+01 Train R2: 0.3322 \t Test R: 0.4478\n",
      "Parameter: 2.00e+01 Train R2: 0.3095 \t Test R: 0.4170\n",
      "Parameter: 3.00e+01 Train R2: 0.2886 \t Test R: 0.3879\n",
      "Parameter: 4.00e+01 Train R2: 0.2698 \t Test R: 0.3617\n",
      "Parameter: 5.00e+01 Train R2: 0.2531 \t Test R: 0.3384\n",
      "Parameter: 6.00e+01 Train R2: 0.2382 \t Test R: 0.3176\n",
      "Parameter: 7.00e+01 Train R2: 0.2249 \t Test R: 0.2992\n",
      "Parameter: 8.00e+01 Train R2: 0.2129 \t Test R: 0.2826\n",
      "Parameter: 1.00e+02 Train R2: 0.1923 \t Test R: 0.2543\n",
      "Parameter: 2.00e+02 Train R2: 0.1292 \t Test R: 0.1688\n",
      "Parameter: 5.00e+02 Train R2: 0.0649 \t Test R: 0.0836\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "\n",
    "for a in (1e+1)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, active_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, active_train), \n",
    "                                                              ridge.score(x_test, active_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b7d2b3",
   "metadata": {},
   "source": [
    "### 1.4 Trip Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "086e5033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.2404 \t Test R: 0.1470 \t Nonzero coef: 10\n",
      "Parameter: 1.00e-03 Train R2: 0.2404 \t Test R: 0.1481 \t Nonzero coef: 10\n",
      "Parameter: 5.00e-03 Train R2: 0.2399 \t Test R: 0.1534 \t Nonzero coef: 9\n",
      "Parameter: 8.00e-03 Train R2: 0.2392 \t Test R: 0.1572 \t Nonzero coef: 8\n",
      "Parameter: 9.00e-03 Train R2: 0.2389 \t Test R: 0.1575 \t Nonzero coef: 8\n",
      "Parameter: 1.00e-02 Train R2: 0.2387 \t Test R: 0.1578 \t Nonzero coef: 7\n",
      "Parameter: 1.50e-02 Train R2: 0.2383 \t Test R: 0.1578 \t Nonzero coef: 7\n",
      "Parameter: 2.00e-02 Train R2: 0.2379 \t Test R: 0.1576 \t Nonzero coef: 7\n",
      "Parameter: 2.00e-01 Train R2: 0.1799 \t Test R: 0.1072 \t Nonzero coef: 2\n",
      "Parameter: 5.00e-01 Train R2: 0.0280 \t Test R: 0.0591 \t Nonzero coef: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.379e+05, tolerance: 3.630e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    }
   ],
   "source": [
    "for a in (1e-2)*np.array([0,0.1,0.5,0.8,0.9,1,1.5,2,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, trpgen_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d\" % (a, lasso.score(x_train, trpgen_train), \n",
    "                                                                                  lasso.score(x_test, trpgen_test), \n",
    "                                                                                  np.sum(lasso.coef_ != 0)))\n",
    "#     with open(out_dir+\"BA_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%.6f,%.4f,%.4f,%s,%d,%d\\n\" % (a, \n",
    "#             lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test), 'lasso', \n",
    "#             np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9b1e8abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.2404 \t Test R: 0.1470\n",
      "Parameter: 1.00e-01 Train R2: 0.2404 \t Test R: 0.1495\n",
      "Parameter: 1.00e+00 Train R2: 0.2372 \t Test R: 0.1633\n",
      "Parameter: 2.00e+00 Train R2: 0.2309 \t Test R: 0.1700\n",
      "Parameter: 3.00e+00 Train R2: 0.2236 \t Test R: 0.1727\n",
      "Parameter: 4.00e+00 Train R2: 0.2159 \t Test R: 0.1730\n",
      "Parameter: 5.00e+00 Train R2: 0.2084 \t Test R: 0.1721\n",
      "Parameter: 6.00e+00 Train R2: 0.2012 \t Test R: 0.1705\n",
      "Parameter: 7.00e+00 Train R2: 0.1944 \t Test R: 0.1684\n",
      "Parameter: 8.00e+00 Train R2: 0.1880 \t Test R: 0.1660\n",
      "Parameter: 1.00e+01 Train R2: 0.1764 \t Test R: 0.1611\n",
      "Parameter: 2.00e+01 Train R2: 0.1365 \t Test R: 0.1386\n",
      "Parameter: 5.00e+01 Train R2: 0.0881 \t Test R: 0.1011\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "\n",
    "for a in (1e0)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, trpgen_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, trpgen_train), \n",
    "                                                              ridge.score(x_test, trpgen_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe9f73",
   "metadata": {},
   "source": [
    "# 2. Linear Regression (PyTorch)\n",
    "\n",
    "Learning Rate very sensitive to data magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "34ceb982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_lr(w2_list, lr_list, x1, x2, y1, y2, reg_type='L2'):\n",
    "    \n",
    "    mseloss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    trainset = SurveyDataset(torch.tensor(x1,  dtype=torch.float), torch.tensor(y1, dtype=torch.float))\n",
    "    trainloader = DataLoader(trainset, batch_size=len(trainset), shuffle=True)\n",
    "\n",
    "    testset = SurveyDataset(torch.tensor(x2, dtype=torch.float), torch.tensor(y2, dtype=torch.float))\n",
    "    testloader = DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "\n",
    "    # decay rates for embedding\n",
    "    w1_list = [0]\n",
    "    # decay rates for demo (There is no demo in this case)\n",
    "#     w2_list = [0]\n",
    "    # lr_list = [0.005,0.01, 0.02]\n",
    "#     lr_list = [0.002]\n",
    "\n",
    "    dim_demo = x1.shape[1]\n",
    "    dim_embed = 0\n",
    "\n",
    "    for lr in lr_list:\n",
    "\n",
    "        for w1, w2 in itertools.product(w1_list, w2_list):\n",
    "\n",
    "            # model setup\n",
    "            model = linear_reg.LR(dim_embed=dim_embed, dim_demo=dim_demo)\n",
    "\n",
    "#             print(model)\n",
    "            embed_params = []\n",
    "            demo_params = []\n",
    "            other_params = []\n",
    "            for name, m in model.named_parameters():\n",
    "        #             print(name)\n",
    "                if 'embed' in name:\n",
    "                    embed_params.append(m)\n",
    "                elif 'demo' in name:\n",
    "                    demo_params.append(m)\n",
    "                else:\n",
    "                    other_params.append(m)\n",
    "\n",
    "#             optimizer = torch.optim.Adam([{'params':demo_params,'lr':lr}])\n",
    "            if reg_type == 'L2':\n",
    "                optimizer = torch.optim.Adam([{'params':embed_params,'weight_decay':w1,'lr':lr},\n",
    "                                          {'params':demo_params,'weight_decay':w2, 'lr':lr},\n",
    "                                          {'params':other_params,'weight_decay':0, 'lr':lr}])\n",
    "            else:\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#             print(optimizer)\n",
    "#             print(demo_params)\n",
    "            \n",
    "            # model training\n",
    "            ref1 = 0\n",
    "            ref2 = 0\n",
    "\n",
    "            for epoch in range(5000):\n",
    "\n",
    "                mse_ = 0\n",
    "\n",
    "                for batch, (x_batch, y_batch) in enumerate(trainloader):\n",
    "                    # Compute prediction and loss\n",
    "                    pred = model(None, x_batch)\n",
    "                    pred = F.relu(pred).squeeze()\n",
    "\n",
    "                    mse = mseloss(pred, y_batch)\n",
    "                    mse_ += mse.item()\n",
    "\n",
    "                    if reg_type == 'L1':\n",
    "                        l1_reg = w2 * torch.norm(demo_params, 1)\n",
    "                        mse += l1_reg\n",
    "                    \n",
    "                    # Backpropagation\n",
    "                    optimizer.zero_grad()\n",
    "                    mse.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                train_r = r2_score(y_batch.numpy(), pred.detach().numpy())\n",
    "                train_mse = mse_/len(trainset)\n",
    "\n",
    "                if epoch % 20 == 0:\n",
    "                    print(f\"[epoch: {epoch:>3d}] Train MSE : {train_mse:.4f} R2 score: {train_r:.3f} \")\n",
    "                loss_ = train_mse\n",
    "\n",
    "                if epoch % 5 == 0:\n",
    "                    if epoch >=40:\n",
    "                        if (np.abs(loss_ - ref1)/ref1<0.0005) & (np.abs(loss_ - ref2)/ref2<0.0005):\n",
    "                            print(\"Early stopping at epoch\", epoch)\n",
    "                            print(ref2, ref1, loss_)\n",
    "                            break\n",
    "                        if (ref1 < loss_) & (ref1 < ref2):\n",
    "                            print(\"Diverging. stop.\")\n",
    "                            break\n",
    "                        if loss_ < best:\n",
    "                            best = loss_\n",
    "                            best_epoch = epoch\n",
    "                    else:\n",
    "                        best = loss_\n",
    "                        best_epoch = epoch\n",
    "\n",
    "                    ref2 = ref1\n",
    "                    ref1 = loss_\n",
    "\n",
    "                if epoch % 20 == 0:\n",
    "\n",
    "                    mse_ = 0 \n",
    "\n",
    "                    for batch, (x_batch, y_batch) in enumerate(testloader):\n",
    "                        pred = model(None, x_batch)\n",
    "                        pred = F.relu(pred).squeeze()\n",
    "\n",
    "                        mse = mseloss(pred, y_batch)\n",
    "                        mse_ += mse.item()\n",
    "                        \n",
    "#                     print(len(testset))\n",
    "\n",
    "                    test_mse = mse_/len(testset)\n",
    "                    test_r = r2_score(y_batch.numpy(),pred.detach().numpy())\n",
    "\n",
    "                    print(f\"[epoch: {epoch:>3d}] Test MSE {test_mse:.4f} R2 score: {test_r:.3f} \")\n",
    "    return model\n",
    "    #         with open(out_dir+model_code+\"_regression_trpgen.csv\", \"a\") as f:\n",
    "    #             f.write(\"%s,%s,%s,%s,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f\\n\" % \\\n",
    "    #                 (model_run_date, model_type, zoomlevel, \"LR\", lr, w1, \n",
    "    #                   train_rmse, train_r, test_rmse, test_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "94db67ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:   0] Train MSE : 0.6450 R2 score: -10.838 \n",
      "[epoch:   0] Test MSE 0.6247 R2 score: -11.327 \n",
      "[epoch:  20] Train MSE : 0.1336 R2 score: -1.452 \n",
      "[epoch:  20] Test MSE 0.1245 R2 score: -1.457 \n",
      "[epoch:  40] Train MSE : 0.0758 R2 score: -0.391 \n",
      "[epoch:  40] Test MSE 0.0762 R2 score: -0.503 \n",
      "[epoch:  60] Train MSE : 0.0605 R2 score: -0.110 \n",
      "[epoch:  60] Test MSE 0.0610 R2 score: -0.204 \n",
      "[epoch:  80] Train MSE : 0.0496 R2 score: 0.089 \n",
      "[epoch:  80] Test MSE 0.0491 R2 score: 0.032 \n",
      "[epoch: 100] Train MSE : 0.0433 R2 score: 0.205 \n",
      "[epoch: 100] Test MSE 0.0421 R2 score: 0.170 \n",
      "[epoch: 120] Train MSE : 0.0397 R2 score: 0.271 \n",
      "[epoch: 120] Test MSE 0.0379 R2 score: 0.253 \n",
      "[epoch: 140] Train MSE : 0.0377 R2 score: 0.308 \n",
      "[epoch: 140] Test MSE 0.0354 R2 score: 0.302 \n",
      "[epoch: 160] Train MSE : 0.0365 R2 score: 0.330 \n",
      "[epoch: 160] Test MSE 0.0338 R2 score: 0.332 \n",
      "[epoch: 180] Train MSE : 0.0357 R2 score: 0.344 \n",
      "[epoch: 180] Test MSE 0.0328 R2 score: 0.354 \n",
      "[epoch: 200] Train MSE : 0.0351 R2 score: 0.356 \n",
      "[epoch: 200] Test MSE 0.0320 R2 score: 0.369 \n",
      "[epoch: 220] Train MSE : 0.0346 R2 score: 0.366 \n",
      "[epoch: 220] Test MSE 0.0313 R2 score: 0.382 \n",
      "[epoch: 240] Train MSE : 0.0341 R2 score: 0.374 \n",
      "[epoch: 240] Test MSE 0.0307 R2 score: 0.393 \n",
      "[epoch: 260] Train MSE : 0.0337 R2 score: 0.382 \n",
      "[epoch: 260] Test MSE 0.0303 R2 score: 0.403 \n",
      "[epoch: 280] Train MSE : 0.0333 R2 score: 0.389 \n",
      "[epoch: 280] Test MSE 0.0298 R2 score: 0.412 \n",
      "[epoch: 300] Train MSE : 0.0330 R2 score: 0.395 \n",
      "[epoch: 300] Test MSE 0.0294 R2 score: 0.420 \n",
      "[epoch: 320] Train MSE : 0.0327 R2 score: 0.400 \n",
      "[epoch: 320] Test MSE 0.0291 R2 score: 0.427 \n",
      "[epoch: 340] Train MSE : 0.0324 R2 score: 0.405 \n",
      "[epoch: 340] Test MSE 0.0287 R2 score: 0.433 \n",
      "[epoch: 360] Train MSE : 0.0322 R2 score: 0.410 \n",
      "[epoch: 360] Test MSE 0.0284 R2 score: 0.439 \n",
      "[epoch: 380] Train MSE : 0.0319 R2 score: 0.414 \n",
      "[epoch: 380] Test MSE 0.0281 R2 score: 0.445 \n",
      "[epoch: 400] Train MSE : 0.0318 R2 score: 0.417 \n",
      "[epoch: 400] Test MSE 0.0279 R2 score: 0.450 \n",
      "[epoch: 420] Train MSE : 0.0316 R2 score: 0.420 \n",
      "[epoch: 420] Test MSE 0.0276 R2 score: 0.455 \n",
      "[epoch: 440] Train MSE : 0.0314 R2 score: 0.423 \n",
      "[epoch: 440] Test MSE 0.0274 R2 score: 0.459 \n",
      "[epoch: 460] Train MSE : 0.0313 R2 score: 0.426 \n",
      "[epoch: 460] Test MSE 0.0272 R2 score: 0.463 \n",
      "[epoch: 480] Train MSE : 0.0312 R2 score: 0.428 \n",
      "[epoch: 480] Test MSE 0.0270 R2 score: 0.467 \n",
      "[epoch: 500] Train MSE : 0.0310 R2 score: 0.430 \n",
      "[epoch: 500] Test MSE 0.0268 R2 score: 0.471 \n",
      "[epoch: 520] Train MSE : 0.0309 R2 score: 0.432 \n",
      "[epoch: 520] Test MSE 0.0267 R2 score: 0.474 \n",
      "[epoch: 540] Train MSE : 0.0309 R2 score: 0.434 \n",
      "[epoch: 540] Test MSE 0.0265 R2 score: 0.477 \n",
      "[epoch: 560] Train MSE : 0.0308 R2 score: 0.435 \n",
      "[epoch: 560] Test MSE 0.0264 R2 score: 0.479 \n",
      "[epoch: 580] Train MSE : 0.0307 R2 score: 0.436 \n",
      "[epoch: 580] Test MSE 0.0263 R2 score: 0.481 \n",
      "[epoch: 600] Train MSE : 0.0306 R2 score: 0.437 \n",
      "[epoch: 600] Test MSE 0.0262 R2 score: 0.484 \n",
      "[epoch: 620] Train MSE : 0.0306 R2 score: 0.438 \n",
      "[epoch: 620] Test MSE 0.0261 R2 score: 0.486 \n",
      "[epoch: 640] Train MSE : 0.0305 R2 score: 0.439 \n",
      "[epoch: 640] Test MSE 0.0260 R2 score: 0.487 \n",
      "[epoch: 660] Train MSE : 0.0305 R2 score: 0.440 \n",
      "[epoch: 660] Test MSE 0.0259 R2 score: 0.489 \n",
      "[epoch: 680] Train MSE : 0.0305 R2 score: 0.441 \n",
      "[epoch: 680] Test MSE 0.0258 R2 score: 0.490 \n",
      "[epoch: 700] Train MSE : 0.0304 R2 score: 0.441 \n",
      "Early stopping at epoch 700\n",
      "0.030457972805773184 0.030450475603721435 0.030443253577851506\n"
     ]
    }
   ],
   "source": [
    "model = pytorch_lr(w2_list=[0], lr_list=[0.02], x1=x_train, x2=x_test, y1=auto_train, y2=auto_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e803a55c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:   0] Train MSE : 0.0209 R2 score: -1.151 \n",
      "[epoch:   0] Test MSE 0.0165 R2 score: -0.770 \n",
      "[epoch:  20] Train MSE : 0.0128 R2 score: -0.324 \n",
      "[epoch:  20] Test MSE 0.0119 R2 score: -0.276 \n",
      "[epoch:  40] Train MSE : 0.0119 R2 score: -0.231 \n",
      "[epoch:  40] Test MSE 0.0112 R2 score: -0.205 \n",
      "[epoch:  60] Train MSE : 0.0110 R2 score: -0.131 \n",
      "[epoch:  60] Test MSE 0.0098 R2 score: -0.050 \n",
      "[epoch:  80] Train MSE : 0.0097 R2 score: -0.004 \n",
      "[epoch:  80] Test MSE 0.0085 R2 score: 0.085 \n",
      "[epoch: 100] Train MSE : 0.0088 R2 score: 0.092 \n",
      "[epoch: 100] Test MSE 0.0079 R2 score: 0.154 \n",
      "[epoch: 120] Train MSE : 0.0082 R2 score: 0.160 \n",
      "[epoch: 120] Test MSE 0.0075 R2 score: 0.194 \n",
      "[epoch: 140] Train MSE : 0.0077 R2 score: 0.209 \n",
      "[epoch: 140] Test MSE 0.0073 R2 score: 0.221 \n",
      "[epoch: 160] Train MSE : 0.0073 R2 score: 0.246 \n",
      "[epoch: 160] Test MSE 0.0071 R2 score: 0.237 \n",
      "[epoch: 180] Train MSE : 0.0071 R2 score: 0.273 \n",
      "[epoch: 180] Test MSE 0.0070 R2 score: 0.247 \n",
      "[epoch: 200] Train MSE : 0.0068 R2 score: 0.296 \n",
      "[epoch: 200] Test MSE 0.0069 R2 score: 0.256 \n",
      "[epoch: 220] Train MSE : 0.0067 R2 score: 0.314 \n",
      "[epoch: 220] Test MSE 0.0069 R2 score: 0.262 \n",
      "[epoch: 240] Train MSE : 0.0065 R2 score: 0.329 \n",
      "[epoch: 240] Test MSE 0.0068 R2 score: 0.268 \n",
      "[epoch: 260] Train MSE : 0.0064 R2 score: 0.341 \n",
      "[epoch: 260] Test MSE 0.0068 R2 score: 0.273 \n",
      "[epoch: 280] Train MSE : 0.0063 R2 score: 0.351 \n",
      "[epoch: 280] Test MSE 0.0067 R2 score: 0.277 \n",
      "[epoch: 300] Train MSE : 0.0062 R2 score: 0.358 \n",
      "[epoch: 300] Test MSE 0.0067 R2 score: 0.280 \n",
      "[epoch: 320] Train MSE : 0.0062 R2 score: 0.364 \n",
      "[epoch: 320] Test MSE 0.0067 R2 score: 0.283 \n",
      "[epoch: 340] Train MSE : 0.0061 R2 score: 0.369 \n",
      "[epoch: 340] Test MSE 0.0067 R2 score: 0.285 \n",
      "[epoch: 360] Train MSE : 0.0061 R2 score: 0.373 \n",
      "[epoch: 360] Test MSE 0.0067 R2 score: 0.286 \n",
      "[epoch: 380] Train MSE : 0.0061 R2 score: 0.376 \n",
      "[epoch: 380] Test MSE 0.0066 R2 score: 0.287 \n",
      "[epoch: 400] Train MSE : 0.0060 R2 score: 0.379 \n",
      "[epoch: 400] Test MSE 0.0066 R2 score: 0.288 \n",
      "[epoch: 420] Train MSE : 0.0060 R2 score: 0.381 \n",
      "[epoch: 420] Test MSE 0.0066 R2 score: 0.289 \n",
      "[epoch: 440] Train MSE : 0.0060 R2 score: 0.383 \n",
      "[epoch: 440] Test MSE 0.0066 R2 score: 0.290 \n",
      "[epoch: 460] Train MSE : 0.0060 R2 score: 0.384 \n",
      "[epoch: 460] Test MSE 0.0066 R2 score: 0.290 \n",
      "[epoch: 480] Train MSE : 0.0060 R2 score: 0.385 \n",
      "[epoch: 480] Test MSE 0.0066 R2 score: 0.291 \n",
      "[epoch: 500] Train MSE : 0.0060 R2 score: 0.387 \n",
      "[epoch: 500] Test MSE 0.0066 R2 score: 0.291 \n",
      "[epoch: 520] Train MSE : 0.0059 R2 score: 0.387 \n",
      "[epoch: 520] Test MSE 0.0066 R2 score: 0.291 \n",
      "[epoch: 540] Train MSE : 0.0059 R2 score: 0.388 \n",
      "[epoch: 540] Test MSE 0.0066 R2 score: 0.292 \n",
      "[epoch: 560] Train MSE : 0.0059 R2 score: 0.389 \n",
      "[epoch: 560] Test MSE 0.0066 R2 score: 0.292 \n",
      "Early stopping at epoch 565\n",
      "0.005933469559212217 0.00593199453394349 0.005930584930463087\n"
     ]
    }
   ],
   "source": [
    "model = pytorch_lr(w2_list=[0], lr_list=[0.003], x1=x_train, x2=x_test, y1=pt_train, y2=pt_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e5ff054a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:   0] Train MSE : 0.0406 R2 score: -0.514 \n",
      "[epoch:   0] Test MSE 0.0378 R2 score: -0.537 \n",
      "[epoch:  20] Train MSE : 0.0265 R2 score: 0.011 \n",
      "[epoch:  20] Test MSE 0.0239 R2 score: 0.031 \n",
      "[epoch:  40] Train MSE : 0.0222 R2 score: 0.172 \n",
      "[epoch:  40] Test MSE 0.0196 R2 score: 0.201 \n",
      "[epoch:  60] Train MSE : 0.0199 R2 score: 0.259 \n",
      "[epoch:  60] Test MSE 0.0163 R2 score: 0.336 \n",
      "[epoch:  80] Train MSE : 0.0188 R2 score: 0.297 \n",
      "[epoch:  80] Test MSE 0.0147 R2 score: 0.401 \n",
      "[epoch: 100] Train MSE : 0.0184 R2 score: 0.313 \n",
      "[epoch: 100] Test MSE 0.0140 R2 score: 0.432 \n",
      "[epoch: 120] Train MSE : 0.0182 R2 score: 0.321 \n",
      "[epoch: 120] Test MSE 0.0136 R2 score: 0.447 \n",
      "[epoch: 140] Train MSE : 0.0181 R2 score: 0.326 \n",
      "[epoch: 140] Test MSE 0.0134 R2 score: 0.456 \n",
      "[epoch: 160] Train MSE : 0.0179 R2 score: 0.331 \n",
      "[epoch: 160] Test MSE 0.0132 R2 score: 0.462 \n",
      "[epoch: 180] Train MSE : 0.0178 R2 score: 0.335 \n",
      "[epoch: 180] Test MSE 0.0131 R2 score: 0.466 \n",
      "[epoch: 200] Train MSE : 0.0177 R2 score: 0.338 \n",
      "[epoch: 200] Test MSE 0.0131 R2 score: 0.469 \n",
      "[epoch: 220] Train MSE : 0.0177 R2 score: 0.341 \n",
      "[epoch: 220] Test MSE 0.0130 R2 score: 0.472 \n",
      "[epoch: 240] Train MSE : 0.0176 R2 score: 0.344 \n",
      "[epoch: 240] Test MSE 0.0130 R2 score: 0.473 \n",
      "[epoch: 260] Train MSE : 0.0175 R2 score: 0.346 \n",
      "[epoch: 260] Test MSE 0.0129 R2 score: 0.475 \n",
      "[epoch: 280] Train MSE : 0.0175 R2 score: 0.348 \n",
      "[epoch: 280] Test MSE 0.0129 R2 score: 0.476 \n",
      "[epoch: 300] Train MSE : 0.0174 R2 score: 0.349 \n",
      "[epoch: 300] Test MSE 0.0129 R2 score: 0.477 \n",
      "[epoch: 320] Train MSE : 0.0174 R2 score: 0.351 \n",
      "[epoch: 320] Test MSE 0.0129 R2 score: 0.477 \n",
      "[epoch: 340] Train MSE : 0.0174 R2 score: 0.352 \n",
      "[epoch: 340] Test MSE 0.0129 R2 score: 0.477 \n",
      "[epoch: 360] Train MSE : 0.0174 R2 score: 0.353 \n",
      "[epoch: 360] Test MSE 0.0129 R2 score: 0.477 \n",
      "[epoch: 380] Train MSE : 0.0173 R2 score: 0.353 \n",
      "[epoch: 380] Test MSE 0.0129 R2 score: 0.478 \n",
      "Early stopping at epoch 395\n",
      "0.01733011572162029 0.017325835652870707 0.01732179029321738\n"
     ]
    }
   ],
   "source": [
    "model = pytorch_lr(w2_list=[0], lr_list=[0.005], x1=x_train, x2=x_test, y1=active_train, y2=active_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "dfdb6b4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:   0] Train MSE : 442.0938 R2 score: -0.722 \n",
      "[epoch:   0] Test MSE 380.1566 R2 score: -1.005 \n",
      "[epoch:  20] Train MSE : 258.4086 R2 score: -0.007 \n",
      "[epoch:  20] Test MSE 184.4423 R2 score: 0.027 \n",
      "[epoch:  40] Train MSE : 241.0255 R2 score: 0.061 \n",
      "[epoch:  40] Test MSE 179.1828 R2 score: 0.055 \n",
      "[epoch:  60] Train MSE : 236.4879 R2 score: 0.079 \n",
      "[epoch:  60] Test MSE 177.2693 R2 score: 0.065 \n",
      "[epoch:  80] Train MSE : 231.7684 R2 score: 0.097 \n",
      "[epoch:  80] Test MSE 173.2355 R2 score: 0.087 \n",
      "[epoch: 100] Train MSE : 227.4779 R2 score: 0.114 \n",
      "[epoch: 100] Test MSE 170.7736 R2 score: 0.100 \n",
      "[epoch: 120] Train MSE : 223.5468 R2 score: 0.129 \n",
      "[epoch: 120] Test MSE 168.7242 R2 score: 0.110 \n",
      "[epoch: 140] Train MSE : 220.0191 R2 score: 0.143 \n",
      "[epoch: 140] Test MSE 166.9102 R2 score: 0.120 \n",
      "[epoch: 160] Train MSE : 216.8922 R2 score: 0.155 \n",
      "[epoch: 160] Test MSE 165.3753 R2 score: 0.128 \n",
      "[epoch: 180] Train MSE : 214.1471 R2 score: 0.166 \n",
      "[epoch: 180] Test MSE 164.1018 R2 score: 0.135 \n",
      "[epoch: 200] Train MSE : 211.7547 R2 score: 0.175 \n",
      "[epoch: 200] Test MSE 163.0520 R2 score: 0.140 \n",
      "[epoch: 220] Train MSE : 209.6813 R2 score: 0.183 \n",
      "[epoch: 220] Test MSE 162.1890 R2 score: 0.145 \n",
      "[epoch: 240] Train MSE : 207.8925 R2 score: 0.190 \n",
      "[epoch: 240] Test MSE 161.4787 R2 score: 0.149 \n",
      "[epoch: 260] Train MSE : 206.3550 R2 score: 0.196 \n",
      "[epoch: 260] Test MSE 160.8930 R2 score: 0.152 \n",
      "[epoch: 280] Train MSE : 205.0376 R2 score: 0.201 \n",
      "[epoch: 280] Test MSE 160.4090 R2 score: 0.154 \n",
      "[epoch: 300] Train MSE : 203.9117 R2 score: 0.206 \n",
      "[epoch: 300] Test MSE 160.0083 R2 score: 0.156 \n",
      "[epoch: 320] Train MSE : 202.9515 R2 score: 0.209 \n",
      "[epoch: 320] Test MSE 159.6758 R2 score: 0.158 \n",
      "[epoch: 340] Train MSE : 202.1312 R2 score: 0.213 \n",
      "[epoch: 340] Test MSE 159.4151 R2 score: 0.159 \n",
      "[epoch: 360] Train MSE : 201.4263 R2 score: 0.215 \n",
      "[epoch: 360] Test MSE 159.1787 R2 score: 0.161 \n",
      "[epoch: 380] Train MSE : 200.8200 R2 score: 0.218 \n",
      "[epoch: 380] Test MSE 158.9628 R2 score: 0.162 \n",
      "[epoch: 400] Train MSE : 200.3006 R2 score: 0.220 \n",
      "[epoch: 400] Test MSE 158.7681 R2 score: 0.163 \n",
      "[epoch: 420] Train MSE : 199.8582 R2 score: 0.222 \n",
      "[epoch: 420] Test MSE 158.6092 R2 score: 0.164 \n",
      "[epoch: 440] Train MSE : 199.4816 R2 score: 0.223 \n",
      "[epoch: 440] Test MSE 158.4843 R2 score: 0.164 \n",
      "[epoch: 460] Train MSE : 199.1582 R2 score: 0.224 \n",
      "[epoch: 460] Test MSE 158.3613 R2 score: 0.165 \n",
      "[epoch: 480] Train MSE : 198.8782 R2 score: 0.225 \n",
      "[epoch: 480] Test MSE 158.2319 R2 score: 0.166 \n",
      "[epoch: 500] Train MSE : 198.6384 R2 score: 0.226 \n",
      "[epoch: 500] Test MSE 158.1177 R2 score: 0.166 \n",
      "[epoch: 520] Train MSE : 198.4348 R2 score: 0.227 \n",
      "Early stopping at epoch 520\n",
      "198.53244342291373 198.48260696605374 198.43480374823196\n"
     ]
    }
   ],
   "source": [
    "model = pytorch_lr(w2_list=[4], lr_list=[0.4], x1=x_train, x2=x_test, y1=trpgen_train, y2=trpgen_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be50f468",
   "metadata": {},
   "source": [
    "# 3. MNL for mode choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5b6ee503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader and model definition\n",
    "\n",
    "trainset = SurveyDataset(torch.tensor(x_train,  dtype=torch.float), torch.tensor(y_train, dtype=torch.float))\n",
    "trainloader = DataLoader(trainset, batch_size=len(trainset), shuffle=False)\n",
    "\n",
    "testset = SurveyDataset(torch.tensor(x_test, dtype=torch.float), torch.tensor(y_test, dtype=torch.float))\n",
    "testloader = DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "\n",
    "kldivloss = nn.KLDivLoss(reduction='sum')\n",
    "mseloss = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "71baf8c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mnl_torch(lr_list, wd_list):\n",
    "    \n",
    "    for (lr, wd) in itertools.product(lr_list, wd_list):\n",
    "        print(f\"[lr: {lr:.3f}, wd: {wd:3.4f}]\")\n",
    "\n",
    "        # model setup\n",
    "        model = mnl.MNL(n_alts=4, n_features=x_train.shape[-1])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        # model training\n",
    "\n",
    "        ref1 = 0\n",
    "        ref2 = 0\n",
    "\n",
    "        for epoch in range(1000):\n",
    "\n",
    "            kl_ = 0\n",
    "            mse_ = 0\n",
    "            mse1_ = 0\n",
    "            mse2_ = 0\n",
    "            mse3_ = 0\n",
    "            mse4_ = 0\n",
    "\n",
    "            for batch, (x_batch, y_batch) in enumerate(trainloader):\n",
    "                # Compute prediction and loss\n",
    "                util = model(x_batch)\n",
    "                probs = torch.log(nn.functional.softmax(util, dim=1))\n",
    "                kl = kldivloss(probs, y_batch)\n",
    "        #         kl = kldivloss(torch.log(util), y_batch)\n",
    "                kl_ += kl.item()\n",
    "\n",
    "                mse = mseloss(torch.exp(probs), y_batch)\n",
    "        #         mse = mseloss(util, y_batch)\n",
    "                mse_ += mse.sum().item()\n",
    "                mse1_ += mse[:,0].sum().item()\n",
    "                mse2_ += mse[:,1].sum().item()\n",
    "                mse3_ += mse[:,2].sum().item()\n",
    "                mse4_ += mse[:,3].sum().item()\n",
    "                mse = mse.sum()\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                kl.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_kl = kl_/len(trainset)\n",
    "            train_mse = np.sqrt(mse_/len(trainset))\n",
    "            train_mse1 = np.sqrt(mse1_/len(trainset))\n",
    "            train_mse2 = np.sqrt(mse2_/len(trainset))\n",
    "            train_mse3 = np.sqrt(mse3_/len(trainset))\n",
    "            train_mse4 = np.sqrt(mse4_/len(trainset))\n",
    "\n",
    "            train_r1 = 1-mse1_/sst_train[0]\n",
    "            train_r2 = 1-mse2_/sst_train[1]\n",
    "            train_r3 = 1-mse3_/sst_train[2]\n",
    "            train_r4 = 1-mse4_/sst_train[3]\n",
    "\n",
    "            loss_ = train_kl\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "\n",
    "                kl_ = 0\n",
    "                mse_ = 0 \n",
    "                mse1_ = 0\n",
    "                mse2_ = 0\n",
    "                mse3_ = 0\n",
    "                mse4_ = 0\n",
    "\n",
    "                for batch, (x_batch, y_batch) in enumerate(testloader):\n",
    "                    util = model(x_batch)\n",
    "                    probs = torch.log(nn.functional.softmax(util,dim=1))\n",
    "                    kl = kldivloss(probs, y_batch)\n",
    "            #         kl = kldivloss(torch.log(util), y_batch)\n",
    "                    kl_ += kl.item()\n",
    "\n",
    "                    mse = mseloss(torch.exp(probs), y_batch)\n",
    "            #         mse = mseloss(util, y_batch)\n",
    "                    mse_ += mse.sum().item()\n",
    "                    mse1_ += mse[:,0].sum().item()\n",
    "                    mse2_ += mse[:,1].sum().item()\n",
    "                    mse3_ += mse[:,2].sum().item()\n",
    "                    mse4_ += mse[:,3].sum().item()\n",
    "\n",
    "                test_kl = kl_/len(testset)\n",
    "                test_mse = np.sqrt(mse_/len(testset))\n",
    "                test_mse1 = np.sqrt(mse1_/len(testset))\n",
    "                test_mse2 = np.sqrt(mse2_/len(testset))\n",
    "                test_mse3 = np.sqrt(mse3_/len(testset))\n",
    "                test_mse4 = np.sqrt(mse4_/len(testset))\n",
    "\n",
    "                r1 = r2_score(y_batch.numpy()[:,0],torch.exp(probs).detach().numpy()[:,0])\n",
    "                r2 = r2_score(y_batch.numpy()[:,1],torch.exp(probs).detach().numpy()[:,1])\n",
    "                r3 = r2_score(y_batch.numpy()[:,2],torch.exp(probs).detach().numpy()[:,2])\n",
    "                r4 = r2_score(y_batch.numpy()[:,3],torch.exp(probs).detach().numpy()[:,3])\n",
    "\n",
    "                if epoch >= 40:\n",
    "                    if (np.abs(loss_ - ref1)/ref1<0.001) & (np.abs(loss_ - ref2)/ref2<0.001):\n",
    "                        print(\"Early stopping at epoch\", epoch)\n",
    "                        break\n",
    "                    if (ref1 < loss_) & (ref1 < ref2):\n",
    "                        print(\"Diverging. stop.\")\n",
    "                        break\n",
    "                    if loss_ < best:\n",
    "                        best = loss_\n",
    "                        best_epoch = epoch\n",
    "                        output = (train_kl, train_mse, train_mse1, train_mse2, train_mse3, train_mse4,\n",
    "                                  test_kl, test_mse, test_mse1, test_mse2, test_mse3, test_mse4,\n",
    "                                  r1, r2, r3, r4, train_r1, train_r2, train_r3, train_r4)\n",
    "                else:\n",
    "                    best = loss_\n",
    "                    best_epoch = epoch\n",
    "                    output = (train_kl, train_mse, train_mse1, train_mse2, train_mse3, train_mse4,\n",
    "                                  test_kl, test_mse, test_mse1, test_mse2, test_mse3, test_mse4,\n",
    "                                  r1, r2, r3, r4, train_r1, train_r2, train_r3, train_r4)\n",
    "                ref2 = ref1\n",
    "                ref1 = loss_\n",
    "\n",
    "#             if epoch % 20 == 0:\n",
    "\n",
    "    #             print(f\"[epoch: {epoch:>3d}] Train KL loss: {train_kl:.3f} RMSE {train_mse:.3f} \\\n",
    "    #                 {train_mse1:.3f} {train_mse2:.3f} {train_mse3:.3f} {train_mse4:.3f}\")\n",
    "    #             print(f\"\\t\\t\\t\\t\\t\\t Train R2 score: {train_r1:.3f} {train_r2:.3f} {train_r3:.3f} {train_r4:.3f} \")\n",
    "    #             print(f\"[epoch: {epoch:>3d}] Test KL loss: {kl_/len(testset):.3f} RMSE {np.sqrt(mse_/len(testset)):.3f} \\\n",
    "    #                     {np.sqrt(mse1_/len(testset)):.3f} {np.sqrt(mse2_/len(testset)):.3f} {np.sqrt(mse3_/len(testset)):.3f} {np.sqrt(mse4_/len(testset)):.3f}\")\n",
    "    #             print(f\"\\t\\t\\t\\t\\t\\t Test R2 score: {r1:.3f} {r2:.3f} {r3:.3f} {r4:.3f} \")\n",
    "\n",
    "#                 print(f\"[epoch: {epoch:>3d}] Train KL loss: {train_kl:.3f} Train R2 score: {train_r1:.3f} {train_r2:.3f} {train_r3:.3f} {train_r4:.3f} \")\n",
    "#                 print(f\"[epoch: {epoch:>3d}] Test KL loss: {kl_/len(testset):.3f} Test R2 score: {r1:.3f} {r2:.3f} {r3:.3f} {r4:.3f} \")\n",
    "\n",
    "    #     with open(out_dir+\"BA_mode_choice.csv\", \"a\") as f:\n",
    "    #         f.write(\"%s,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f\\n\" % \n",
    "    #                 ((\"MNL\",lr, wd)+output))\n",
    "    \n",
    "        print(f\"[epoch: {best_epoch:>3d}] Train KL loss: {output[0]:.3f} Train R2 score: {output[12]:.3f} {output[13]:.3f} {output[14]:.3f} {output[15]:.3f} \")\n",
    "        print(f\"[epoch: {best_epoch:>3d}] Test KL loss: {output[6]:.3f} Test R2 score: {output[16]:.3f} {output[17]:.3f} {output[18]:.3f} {output[19]:.3f} \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "4de1d22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lr: 0.010, wd: 0.0000]\n",
      "Early stopping at epoch 505\n",
      "[epoch: 500] Train KL loss: 0.168 Train R2 score: 0.439 0.445 -0.151 0.230 \n",
      "[epoch: 500] Test KL loss: 0.145 Test R2 score: 0.339 0.414 -0.019 0.337 \n",
      "\n",
      "[lr: 0.010, wd: 0.0005]\n",
      "Early stopping at epoch 640\n",
      "[epoch: 635] Train KL loss: 0.170 Train R2 score: 0.440 0.445 -0.133 0.241 \n",
      "[epoch: 635] Test KL loss: 0.145 Test R2 score: 0.331 0.409 -0.013 0.323 \n",
      "\n",
      "[lr: 0.010, wd: 0.0010]\n",
      "Early stopping at epoch 715\n",
      "[epoch: 710] Train KL loss: 0.169 Train R2 score: 0.444 0.442 -0.149 0.221 \n",
      "[epoch: 710] Test KL loss: 0.145 Test R2 score: 0.341 0.415 -0.012 0.312 \n",
      "\n",
      "[lr: 0.010, wd: 0.0020]\n",
      "Early stopping at epoch 580\n",
      "[epoch: 575] Train KL loss: 0.169 Train R2 score: 0.436 0.442 -0.132 0.232 \n",
      "[epoch: 575] Test KL loss: 0.145 Test R2 score: 0.334 0.410 -0.015 0.333 \n",
      "\n",
      "[lr: 0.010, wd: 0.0100]\n",
      "Early stopping at epoch 680\n",
      "[epoch: 675] Train KL loss: 0.168 Train R2 score: 0.442 0.449 -0.134 0.234 \n",
      "[epoch: 675] Test KL loss: 0.145 Test R2 score: 0.342 0.418 -0.006 0.321 \n",
      "\n",
      "[lr: 0.010, wd: 0.1000]\n",
      "Early stopping at epoch 705\n",
      "[epoch: 700] Train KL loss: 0.170 Train R2 score: 0.434 0.431 -0.159 0.225 \n",
      "[epoch: 700] Test KL loss: 0.147 Test R2 score: 0.333 0.406 -0.012 0.322 \n",
      "\n",
      "[lr: 0.010, wd: 1.0000]\n",
      "Early stopping at epoch 555\n",
      "[epoch: 550] Train KL loss: 0.171 Train R2 score: 0.416 0.427 -0.135 0.251 \n",
      "[epoch: 550] Test KL loss: 0.148 Test R2 score: 0.320 0.397 -0.011 0.334 \n",
      "\n",
      "[lr: 0.010, wd: 10.0000]\n",
      "Early stopping at epoch 585\n",
      "[epoch: 580] Train KL loss: 0.189 Train R2 score: 0.320 0.305 -0.194 0.174 \n",
      "[epoch: 580] Test KL loss: 0.167 Test R2 score: 0.251 0.296 -0.012 0.221 \n",
      "\n",
      "[lr: 0.010, wd: 100.0000]\n",
      "Early stopping at epoch 230\n",
      "[epoch: 225] Train KL loss: 0.238 Train R2 score: 0.035 -0.065 -0.821 -0.010 \n",
      "[epoch: 225] Test KL loss: 0.224 Test R2 score: 0.035 -0.010 -0.155 0.014 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnl_torch(lr_list=[0.01], wd_list=[0, 0.0005, 0.001, 0.002, 0.01, 0.1, 1, 10, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "0f30e930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
    "for i in range(4):\n",
    "    ax.scatter(y_batch.detach().numpy()[:,i], torch.exp(probs).detach().numpy()[:,i], s=10)\n",
    "\n",
    "ax.set_xlabel(\"True Probs\")\n",
    "ax.set_ylabel(\"Predicted Probs\")\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_ylim([0,1]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1ee3db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), out_dir+\"demo_weights.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
