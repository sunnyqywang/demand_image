{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022/02/22 10:43:37 matplotlib.backends DEBUG] - backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"models/\")\n",
    "from setup import out_dir, data_dir, image_dir, model_dir\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn\n",
    "import torchvision.utils\n",
    "import torchvision.transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    from tensorboardX import SummaryWriter\n",
    "    is_tensorboard_available = True\n",
    "except Exception:\n",
    "    is_tensorboard_available = False\n",
    "\n",
    "from dataloader import image_loader, load_demo\n",
    "from autoencoder import Autoencoder\n",
    "from M1_util_train_test import load_model, train, test, AverageMeter\n",
    "from util_model import my_loss\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='[%(asctime)s %(name)s %(levelname)s] - %(message)s',\n",
    "    datefmt='%Y/%m/%d %H:%M:%S',\n",
    "    level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoomlevel = 'zoom15'\n",
    "output_dim = 3\n",
    "model_run_date = \"22021407\"\n",
    "sampling='stratified'\n",
    "normalization = 'minmax'\n",
    "data_version = '1571'\n",
    "variable_names = ['tot_population','pct25_34yrs','pct35_50yrs','pctover65yrs',\n",
    "         'pctwhite_alone','pct_nonwhite','pctblack_alone',\n",
    "         'pct_col_grad','avg_tt_to_work','inc_per_capita']\n",
    "model_save_variable_names = ['totpop','pct25-34','pct35-50','pctsenior',\n",
    "         'pctwhite_alone','pct_nonwhite','pctblack_alone',\n",
    "         'pctcolgrad','avg_tt_to_work','inc']\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022/02/22 10:43:39 numexpr.utils INFO] - Note: NumExpr detected 20 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[2022/02/22 10:43:39 numexpr.utils INFO] - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "demo_cs, demo_np = load_demo(data_dir, norm=normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'weight':50,\n",
    "        'image_size': 224, \n",
    "        'depth': -1,\n",
    "       'base_channels':64,\n",
    "       'output_dim':output_dim,\n",
    "       'num_demo_vars':len(variable_names),\n",
    "       'demo_norm': normalization,\n",
    "       'cardinality':1,\n",
    "       'epochs':200,\n",
    "       'batch_size':16,\n",
    "       'base_lr':0.005,\n",
    "       'weight_decay':0.0005,\n",
    "       'momentum': 0.9,\n",
    "       'nesterov': True,\n",
    "       'milestones': '[50,100]',\n",
    "       'lr_decay':0.1,\n",
    "       'seed': 1234,\n",
    "       'outdir':out_dir,\n",
    "       'num_workers':8,\n",
    "       'tensorboard':False,\n",
    "       'save':True}\n",
    "\n",
    "model_config = OrderedDict([\n",
    "    ('arch', 'resnext'),\n",
    "    ('depth', args['depth']),\n",
    "    ('base_channels', args['base_channels']),\n",
    "    ('cardinality', args['cardinality']),\n",
    "    ('input_shape', (1, 3, 32, 32)),\n",
    "    ('output_dim', args['output_dim']),\n",
    "    ('num_demo_vars', args['num_demo_vars'])\n",
    "])\n",
    "\n",
    "optim_config = OrderedDict([\n",
    "    ('epochs', args['epochs']),\n",
    "    ('batch_size', args['batch_size']),\n",
    "    ('base_lr', args['base_lr']),\n",
    "    ('weight_decay', args['weight_decay']),\n",
    "    ('momentum', args['momentum']),\n",
    "    ('nesterov', args['nesterov']),\n",
    "    ('milestones', json.loads(args['milestones'])),\n",
    "    ('lr_decay', args['lr_decay']),\n",
    "])\n",
    "\n",
    "data_config = OrderedDict([\n",
    "    ('dataset', 'CIFAR10'),\n",
    "    ('image_size', args['image_size']),\n",
    "    ('demo_norm', args['demo_norm'])\n",
    "])\n",
    "\n",
    "run_config = OrderedDict([\n",
    "    ('weight', args['weight']),\n",
    "    ('seed', args['seed']),\n",
    "    ('outdir', args['outdir']),\n",
    "    ('save', args['save']),\n",
    "    ('num_workers', args['num_workers']),\n",
    "    ('tensorboard', args['tensorboard']),\n",
    "])\n",
    "\n",
    "config = OrderedDict([\n",
    "    ('model_config', model_config),\n",
    "    ('optim_config', optim_config),\n",
    "    ('data_config', data_config),\n",
    "    ('run_config', run_config),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse command line arguments\n",
    "#config = parse_args()\n",
    "#logger.info(json.dumps(config, indent=2))\n",
    "\n",
    "model_name = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "\n",
    "run_config = config['run_config']\n",
    "optim_config = config['optim_config']\n",
    "\n",
    "# TensorBoard SummaryWriter\n",
    "writer = SummaryWriter(model_name) if run_config['tensorboard'] else None\n",
    "\n",
    "# set random seed\n",
    "seed = run_config['seed']\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# create output directory\n",
    "outdir = run_config['outdir']\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# save config as json file in output directory\n",
    "outpath = os.path.join(outdir, 'config.json')\n",
    "with open(outpath, 'w') as fout:\n",
    "    json.dump(config, fout, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28278 images in dataset\n",
      "3142 images in dataset\n"
     ]
    }
   ],
   "source": [
    "# data loaders\n",
    "# train_loader, test_loader = get_loader(optim_config['batch_size'], run_config['num_workers'])\n",
    "train_loader, test_loader = image_loader(image_dir+zoomlevel+\"/\", data_dir, optim_config['batch_size'], \n",
    "                                         run_config['num_workers'], \n",
    "                                         data_config['image_size'], \n",
    "                                         data_version=data_version, sampling=sampling, \n",
    "                                         recalculate_normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.MSELoss(reduction='mean')\n",
    "criterion = my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "config['model_config']['input_shape'] = (1,3,data_config['image_size'],data_config['image_size'])\n",
    "\n",
    "encoder = load_model(config['model_config']['arch'], 'Encoder', config['model_config'])\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "config['model_config']['input_shape'] = [1,2048,config['model_config']['output_dim'],config['model_config']['output_dim']]\n",
    "\n",
    "config['model_config']['conv_shape'] = [data_config['image_size']//32,data_config['image_size']//32]\n",
    "config['model_config']['output_channels'] = 3\n",
    "\n",
    "decoder = load_model(config['model_config']['arch'], 'Decoder', config['model_config'])\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "config['encoder'] = encoder\n",
    "config['decoder'] = decoder\n",
    "model = load_model('autoencoder','Autoencoder', config)\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/qingyi/image_chicago/models/SAE_zoom15_18432_22021407_95.pt loaded.\n"
     ]
    }
   ],
   "source": [
    "# Check one model exists for this config\n",
    "model_type = 'SAE'\n",
    "model_path = glob.glob(model_dir+model_type+\"_\"+zoomlevel+\"_\"+str(model_config['output_dim']**2*2048)+\"_\"+\n",
    "                       model_run_date+\"_95.pt\")\n",
    "#\n",
    "if len(model_path) == 1:\n",
    "    saved = torch.load(model_path[0])\n",
    "    print(model_path[0], \"loaded.\")\n",
    "else:\n",
    "    print(\"Error. More than one model or no model exists.\")\n",
    "    print(model_path)\n",
    "    print(model_dir+model_type+\"_\"+zoomlevel+\"_\"+str(model_config['output_dim']**2*2048)+\"_\"\n",
    "                      +model_run_date+\"_*.pt\")\n",
    "\n",
    "model.load_state_dict(saved['model_state_dict']);\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022/02/22 10:46:16 __main__ INFO] - n_params: 31961862\n"
     ]
    }
   ],
   "source": [
    "n_params = sum([param.view(-1).size()[0] for param in encoder.parameters()]) +\\\n",
    "           sum([param.view(-1).size()[0] for param in decoder.parameters()])\n",
    "logger.info('n_params: {}'.format(n_params))\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=optim_config['base_lr'],\n",
    "    momentum=optim_config['momentum'],\n",
    "    weight_decay=optim_config['weight_decay'],\n",
    "    nesterov=optim_config['nesterov'])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones=optim_config['milestones'],\n",
    "    gamma=optim_config['lr_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.load_state_dict(saved['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022/02/22 10:50:42 __main__ INFO] - Epoch 96 Step 1767/1768 Train Loss 0.48121082\n",
      "[2022/02/22 10:50:51 __main__ INFO] - Epoch 96 Test Loss 0.94721786\n",
      "[2022/02/22 10:50:51 __main__ INFO] - Elapsed 9.32\n",
      "[2022/02/22 10:55:00 __main__ INFO] - Epoch 97 Step 1767/1768 Train Loss 0.47712164\n",
      "[2022/02/22 10:55:09 __main__ INFO] - Epoch 97 Test Loss 0.94611216\n",
      "[2022/02/22 10:55:09 __main__ INFO] - Elapsed 9.33\n",
      "[2022/02/22 10:59:18 __main__ INFO] - Epoch 98 Step 1767/1768 Train Loss 0.47572016\n",
      "[2022/02/22 10:59:27 __main__ INFO] - Epoch 98 Test Loss 0.94487242\n",
      "[2022/02/22 10:59:27 __main__ INFO] - Elapsed 9.35\n",
      "[2022/02/22 11:03:36 __main__ INFO] - Epoch 99 Step 1767/1768 Train Loss 0.47672247\n",
      "[2022/02/22 11:03:45 __main__ INFO] - Epoch 99 Test Loss 0.95369453\n",
      "[2022/02/22 11:03:45 __main__ INFO] - Elapsed 9.31\n",
      "[2022/02/22 11:07:54 __main__ INFO] - Epoch 100 Step 1767/1768 Train Loss 0.47499665\n",
      "[2022/02/22 11:08:03 __main__ INFO] - Epoch 100 Test Loss 0.95603716\n",
      "[2022/02/22 11:08:03 __main__ INFO] - Elapsed 9.33\n",
      "[2022/02/22 11:12:14 __main__ INFO] - Epoch 101 Step 1767/1768 Train Loss 0.47313623\n",
      "[2022/02/22 11:12:23 __main__ INFO] - Epoch 101 Test Loss 0.95227605\n",
      "[2022/02/22 11:12:23 __main__ INFO] - Elapsed 9.27\n",
      "[2022/02/22 11:16:31 __main__ INFO] - Epoch 102 Step 1767/1768 Train Loss 0.47280358\n",
      "[2022/02/22 11:16:41 __main__ INFO] - Epoch 102 Test Loss 0.94593182\n",
      "[2022/02/22 11:16:41 __main__ INFO] - Elapsed 9.33\n",
      "[2022/02/22 11:20:49 __main__ INFO] - Epoch 103 Step 1767/1768 Train Loss 0.47239841\n",
      "[2022/02/22 11:20:59 __main__ INFO] - Epoch 103 Test Loss 0.95188622\n",
      "[2022/02/22 11:20:59 __main__ INFO] - Elapsed 9.34\n",
      "[2022/02/22 11:25:07 __main__ INFO] - Epoch 104 Step 1767/1768 Train Loss 0.47039249\n",
      "[2022/02/22 11:25:17 __main__ INFO] - Epoch 104 Test Loss 0.95226614\n",
      "[2022/02/22 11:25:17 __main__ INFO] - Elapsed 9.26\n",
      "[2022/02/22 11:29:25 __main__ INFO] - Epoch 105 Step 1767/1768 Train Loss 0.46943461\n",
      "[2022/02/22 11:29:35 __main__ INFO] - Epoch 105 Test Loss 0.94754670\n",
      "[2022/02/22 11:29:35 __main__ INFO] - Elapsed 9.34\n",
      "[2022/02/22 11:33:44 __main__ INFO] - Epoch 106 Step 1767/1768 Train Loss 0.46801910\n",
      "[2022/02/22 11:33:53 __main__ INFO] - Epoch 106 Test Loss 0.95277259\n",
      "[2022/02/22 11:33:53 __main__ INFO] - Elapsed 9.32\n",
      "[2022/02/22 11:38:02 __main__ INFO] - Epoch 107 Step 1767/1768 Train Loss 0.46740648\n",
      "[2022/02/22 11:38:11 __main__ INFO] - Epoch 107 Test Loss 0.94945885\n",
      "[2022/02/22 11:38:11 __main__ INFO] - Elapsed 9.29\n",
      "[2022/02/22 11:42:20 __main__ INFO] - Epoch 108 Step 1767/1768 Train Loss 0.46579838\n",
      "[2022/02/22 11:42:29 __main__ INFO] - Epoch 108 Test Loss 0.94561820\n",
      "[2022/02/22 11:42:29 __main__ INFO] - Elapsed 9.32\n",
      "[2022/02/22 11:46:38 __main__ INFO] - Epoch 109 Step 1767/1768 Train Loss 0.46639517\n",
      "[2022/02/22 11:46:47 __main__ INFO] - Epoch 109 Test Loss 0.93686363\n",
      "[2022/02/22 11:46:47 __main__ INFO] - Elapsed 9.26\n",
      "[2022/02/22 11:50:56 __main__ INFO] - Epoch 110 Step 1767/1768 Train Loss 0.46620786\n",
      "[2022/02/22 11:51:05 __main__ INFO] - Epoch 110 Test Loss 0.94892836\n",
      "[2022/02/22 11:51:05 __main__ INFO] - Elapsed 9.29\n",
      "[2022/02/22 11:55:14 __main__ INFO] - Epoch 111 Step 1767/1768 Train Loss 0.46315794\n",
      "[2022/02/22 11:55:23 __main__ INFO] - Epoch 111 Test Loss 0.94679450\n",
      "[2022/02/22 11:55:23 __main__ INFO] - Elapsed 9.35\n",
      "[2022/02/22 11:59:32 __main__ INFO] - Epoch 112 Step 1767/1768 Train Loss 0.46471489\n",
      "[2022/02/22 11:59:41 __main__ INFO] - Epoch 112 Test Loss 0.94776905\n",
      "[2022/02/22 11:59:41 __main__ INFO] - Elapsed 9.28\n",
      "[2022/02/22 12:03:50 __main__ INFO] - Epoch 113 Step 1767/1768 Train Loss 0.46283582\n",
      "[2022/02/22 12:03:59 __main__ INFO] - Epoch 113 Test Loss 0.96056576\n",
      "[2022/02/22 12:03:59 __main__ INFO] - Elapsed 9.32\n",
      "[2022/02/22 12:08:08 __main__ INFO] - Epoch 114 Step 1767/1768 Train Loss 0.46243994\n",
      "[2022/02/22 12:08:17 __main__ INFO] - Epoch 114 Test Loss 0.94365304\n",
      "[2022/02/22 12:08:17 __main__ INFO] - Elapsed 9.32\n",
      "[2022/02/22 12:12:26 __main__ INFO] - Epoch 115 Step 1767/1768 Train Loss 0.45879928\n",
      "[2022/02/22 12:12:35 __main__ INFO] - Epoch 115 Test Loss 0.96521179\n",
      "[2022/02/22 12:12:35 __main__ INFO] - Elapsed 9.33\n",
      "[2022/02/22 12:16:44 __main__ INFO] - Epoch 116 Step 1767/1768 Train Loss 0.45849342\n",
      "[2022/02/22 12:16:54 __main__ INFO] - Epoch 116 Test Loss 0.94257163\n",
      "[2022/02/22 12:16:54 __main__ INFO] - Elapsed 9.28\n",
      "[2022/02/22 12:21:02 __main__ INFO] - Epoch 117 Step 1767/1768 Train Loss 0.45831169\n",
      "[2022/02/22 12:21:12 __main__ INFO] - Epoch 117 Test Loss 0.95916880\n",
      "[2022/02/22 12:21:12 __main__ INFO] - Elapsed 9.32\n",
      "[2022/02/22 12:25:20 __main__ INFO] - Epoch 118 Step 1767/1768 Train Loss 0.45641967\n",
      "[2022/02/22 12:25:30 __main__ INFO] - Epoch 118 Test Loss 0.94811739\n",
      "[2022/02/22 12:25:30 __main__ INFO] - Elapsed 9.32\n",
      "[2022/02/22 12:29:38 __main__ INFO] - Epoch 119 Step 1767/1768 Train Loss 0.45555863\n",
      "[2022/02/22 12:29:48 __main__ INFO] - Epoch 119 Test Loss 0.95385925\n",
      "[2022/02/22 12:29:48 __main__ INFO] - Elapsed 9.34\n",
      "[2022/02/22 12:33:56 __main__ INFO] - Epoch 120 Step 1767/1768 Train Loss 0.45627912\n",
      "[2022/02/22 12:34:06 __main__ INFO] - Epoch 120 Test Loss 0.94103198\n",
      "[2022/02/22 12:34:06 __main__ INFO] - Elapsed 9.28\n",
      "[2022/02/22 12:38:15 __main__ INFO] - Epoch 121 Step 1767/1768 Train Loss 0.45457246\n",
      "[2022/02/22 12:38:24 __main__ INFO] - Epoch 121 Test Loss 0.95769722\n",
      "[2022/02/22 12:38:24 __main__ INFO] - Elapsed 9.27\n",
      "[2022/02/22 12:42:33 __main__ INFO] - Epoch 122 Step 1767/1768 Train Loss 0.45255919\n",
      "[2022/02/22 12:42:42 __main__ INFO] - Epoch 122 Test Loss 0.94354519\n",
      "[2022/02/22 12:42:42 __main__ INFO] - Elapsed 9.29\n",
      "[2022/02/22 12:46:51 __main__ INFO] - Epoch 123 Step 1767/1768 Train Loss 0.45247312\n",
      "[2022/02/22 12:47:00 __main__ INFO] - Epoch 123 Test Loss 0.94906282\n",
      "[2022/02/22 12:47:00 __main__ INFO] - Elapsed 9.31\n",
      "[2022/02/22 12:51:09 __main__ INFO] - Epoch 124 Step 1767/1768 Train Loss 0.45268095\n",
      "[2022/02/22 12:51:18 __main__ INFO] - Epoch 124 Test Loss 0.94718611\n",
      "[2022/02/22 12:51:18 __main__ INFO] - Elapsed 9.29\n",
      "[2022/02/22 12:55:27 __main__ INFO] - Epoch 125 Step 1767/1768 Train Loss 0.45197163\n",
      "[2022/02/22 12:55:36 __main__ INFO] - Epoch 125 Test Loss 0.94947144\n",
      "[2022/02/22 12:55:36 __main__ INFO] - Elapsed 9.32\n",
      "[2022/02/22 12:59:45 __main__ INFO] - Epoch 126 Step 1767/1768 Train Loss 0.45243798\n",
      "[2022/02/22 12:59:54 __main__ INFO] - Epoch 126 Test Loss 0.94568577\n",
      "[2022/02/22 12:59:54 __main__ INFO] - Elapsed 9.30\n",
      "[2022/02/22 13:04:03 __main__ INFO] - Epoch 127 Step 1767/1768 Train Loss 0.45208217\n",
      "[2022/02/22 13:04:12 __main__ INFO] - Epoch 127 Test Loss 0.94433638\n",
      "[2022/02/22 13:04:12 __main__ INFO] - Elapsed 9.31\n",
      "[2022/02/22 13:08:21 __main__ INFO] - Epoch 128 Step 1767/1768 Train Loss 0.45012807\n",
      "[2022/02/22 13:08:30 __main__ INFO] - Epoch 128 Test Loss 0.95476795\n",
      "[2022/02/22 13:08:30 __main__ INFO] - Elapsed 9.34\n",
      "[2022/02/22 13:12:39 __main__ INFO] - Epoch 129 Step 1767/1768 Train Loss 0.45043952\n",
      "[2022/02/22 13:12:48 __main__ INFO] - Epoch 129 Test Loss 0.96304448\n",
      "[2022/02/22 13:12:48 __main__ INFO] - Elapsed 9.34\n",
      "[2022/02/22 13:16:57 __main__ INFO] - Epoch 130 Step 1767/1768 Train Loss 0.44932956\n",
      "[2022/02/22 13:17:07 __main__ INFO] - Epoch 130 Test Loss 0.95171267\n",
      "[2022/02/22 13:17:07 __main__ INFO] - Elapsed 9.30\n",
      "[2022/02/22 13:21:16 __main__ INFO] - Epoch 131 Step 1767/1768 Train Loss 0.44813520\n",
      "[2022/02/22 13:21:25 __main__ INFO] - Epoch 131 Test Loss 0.94539257\n",
      "[2022/02/22 13:21:25 __main__ INFO] - Elapsed 9.33\n",
      "[2022/02/22 13:25:34 __main__ INFO] - Epoch 132 Step 1767/1768 Train Loss 0.44638145\n",
      "[2022/02/22 13:25:43 __main__ INFO] - Epoch 132 Test Loss 0.94980400\n",
      "[2022/02/22 13:25:43 __main__ INFO] - Elapsed 9.34\n",
      "[2022/02/22 13:29:52 __main__ INFO] - Epoch 133 Step 1767/1768 Train Loss 0.44659996\n",
      "[2022/02/22 13:30:01 __main__ INFO] - Epoch 133 Test Loss 0.93961536\n",
      "[2022/02/22 13:30:01 __main__ INFO] - Elapsed 9.32\n",
      "[2022/02/22 13:34:10 __main__ INFO] - Epoch 134 Step 1767/1768 Train Loss 0.44730315\n",
      "[2022/02/22 13:34:19 __main__ INFO] - Epoch 134 Test Loss 0.94414098\n",
      "[2022/02/22 13:34:19 __main__ INFO] - Elapsed 9.31\n",
      "[2022/02/22 13:38:28 __main__ INFO] - Epoch 135 Step 1767/1768 Train Loss 0.44418683\n",
      "[2022/02/22 13:38:37 __main__ INFO] - Epoch 135 Test Loss 0.94957837\n",
      "[2022/02/22 13:38:37 __main__ INFO] - Elapsed 9.33\n",
      "[2022/02/22 13:42:46 __main__ INFO] - Epoch 136 Step 1767/1768 Train Loss 0.44617213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022/02/22 13:42:55 __main__ INFO] - Epoch 136 Test Loss 0.94595744\n",
      "[2022/02/22 13:42:55 __main__ INFO] - Elapsed 9.33\n",
      "[2022/02/22 13:47:04 __main__ INFO] - Epoch 137 Step 1767/1768 Train Loss 0.44290924\n",
      "[2022/02/22 13:47:13 __main__ INFO] - Epoch 137 Test Loss 0.93762109\n",
      "[2022/02/22 13:47:13 __main__ INFO] - Elapsed 9.27\n",
      "[2022/02/22 13:51:22 __main__ INFO] - Epoch 138 Step 1767/1768 Train Loss 0.44292361\n",
      "[2022/02/22 13:51:31 __main__ INFO] - Epoch 138 Test Loss 0.94937245\n",
      "[2022/02/22 13:51:31 __main__ INFO] - Elapsed 9.33\n",
      "[2022/02/22 13:55:41 __main__ INFO] - Epoch 139 Step 1767/1768 Train Loss 0.44160694\n",
      "[2022/02/22 13:55:50 __main__ INFO] - Epoch 139 Test Loss 0.93668211\n",
      "[2022/02/22 13:55:50 __main__ INFO] - Elapsed 9.31\n",
      "[2022/02/22 13:59:59 __main__ INFO] - Epoch 140 Step 1767/1768 Train Loss 0.44172724\n",
      "[2022/02/22 14:00:08 __main__ INFO] - Epoch 140 Test Loss 0.94539962\n",
      "[2022/02/22 14:00:08 __main__ INFO] - Elapsed 9.29\n",
      "[2022/02/22 14:04:18 __main__ INFO] - Epoch 141 Step 1767/1768 Train Loss 0.44127692\n",
      "[2022/02/22 14:04:27 __main__ INFO] - Epoch 141 Test Loss 0.93400085\n",
      "[2022/02/22 14:04:27 __main__ INFO] - Elapsed 9.33\n",
      "[2022/02/22 14:08:36 __main__ INFO] - Epoch 142 Step 1767/1768 Train Loss 0.44070858\n",
      "[2022/02/22 14:08:45 __main__ INFO] - Epoch 142 Test Loss 0.94311842\n",
      "[2022/02/22 14:08:45 __main__ INFO] - Elapsed 9.26\n",
      "[2022/02/22 14:12:54 __main__ INFO] - Epoch 143 Step 1767/1768 Train Loss 0.43967614\n",
      "[2022/02/22 14:13:03 __main__ INFO] - Epoch 143 Test Loss 0.94525425\n",
      "[2022/02/22 14:13:03 __main__ INFO] - Elapsed 9.29\n",
      "[2022/02/22 14:17:12 __main__ INFO] - Epoch 144 Step 1767/1768 Train Loss 0.43861013\n",
      "[2022/02/22 14:17:21 __main__ INFO] - Epoch 144 Test Loss 0.94447407\n",
      "[2022/02/22 14:17:21 __main__ INFO] - Elapsed 9.32\n",
      "[2022/02/22 14:21:30 __main__ INFO] - Epoch 145 Step 1767/1768 Train Loss 0.43769102\n",
      "[2022/02/22 14:21:39 __main__ INFO] - Epoch 145 Test Loss 0.93976624\n",
      "[2022/02/22 14:21:39 __main__ INFO] - Elapsed 9.31\n",
      "[2022/02/22 14:25:49 __main__ INFO] - Epoch 146 Step 1767/1768 Train Loss 0.43271122\n",
      "[2022/02/22 14:25:58 __main__ INFO] - Epoch 146 Test Loss 0.93325393\n",
      "[2022/02/22 14:25:58 __main__ INFO] - Elapsed 9.32\n",
      "[2022/02/22 14:30:07 __main__ INFO] - Epoch 147 Step 1767/1768 Train Loss 0.43283876\n",
      "[2022/02/22 14:30:16 __main__ INFO] - Epoch 147 Test Loss 0.93595869\n",
      "[2022/02/22 14:30:16 __main__ INFO] - Elapsed 9.33\n",
      "[2022/02/22 14:34:25 __main__ INFO] - Epoch 148 Step 1767/1768 Train Loss 0.43261602\n",
      "[2022/02/22 14:34:34 __main__ INFO] - Epoch 148 Test Loss 0.93740407\n",
      "[2022/02/22 14:34:34 __main__ INFO] - Elapsed 9.32\n",
      "[2022/02/22 14:38:48 __main__ INFO] - Epoch 149 Step 1767/1768 Train Loss 0.43177578\n",
      "[2022/02/22 14:38:58 __main__ INFO] - Epoch 149 Test Loss 0.93909227\n",
      "[2022/02/22 14:38:58 __main__ INFO] - Elapsed 9.91\n",
      "[2022/02/22 14:43:15 __main__ INFO] - Epoch 150 Step 1767/1768 Train Loss 0.43065010\n",
      "[2022/02/22 14:43:25 __main__ INFO] - Epoch 150 Test Loss 0.93921859\n",
      "[2022/02/22 14:43:25 __main__ INFO] - Elapsed 9.35\n",
      "[2022/02/22 14:47:36 __main__ INFO] - Epoch 151 Step 1767/1768 Train Loss 0.43088321\n",
      "[2022/02/22 14:47:45 __main__ INFO] - Epoch 151 Test Loss 0.93163792\n",
      "[2022/02/22 14:47:45 __main__ INFO] - Elapsed 9.31\n",
      "[2022/02/22 14:51:54 __main__ INFO] - Epoch 152 Step 1767/1768 Train Loss 0.43110713\n",
      "[2022/02/22 14:52:04 __main__ INFO] - Epoch 152 Test Loss 0.93817064\n",
      "[2022/02/22 14:52:04 __main__ INFO] - Elapsed 9.34\n",
      "[2022/02/22 14:56:13 __main__ INFO] - Epoch 153 Step 1767/1768 Train Loss 0.43049361\n",
      "[2022/02/22 14:56:22 __main__ INFO] - Epoch 153 Test Loss 0.93280841\n",
      "[2022/02/22 14:56:22 __main__ INFO] - Elapsed 9.33\n",
      "[2022/02/22 15:00:31 __main__ INFO] - Epoch 154 Step 1767/1768 Train Loss 0.42826692\n",
      "[2022/02/22 15:00:40 __main__ INFO] - Epoch 154 Test Loss 0.93676437\n",
      "[2022/02/22 15:00:40 __main__ INFO] - Elapsed 9.32\n",
      "[2022/02/22 15:04:49 __main__ INFO] - Epoch 155 Step 1767/1768 Train Loss 0.42964908\n",
      "[2022/02/22 15:04:59 __main__ INFO] - Epoch 155 Test Loss 0.93499777\n",
      "[2022/02/22 15:04:59 __main__ INFO] - Elapsed 9.36\n",
      "[2022/02/22 15:09:08 __main__ INFO] - Epoch 156 Step 1767/1768 Train Loss 0.42893098\n",
      "[2022/02/22 15:09:17 __main__ INFO] - Epoch 156 Test Loss 0.93686427\n",
      "[2022/02/22 15:09:17 __main__ INFO] - Elapsed 9.31\n",
      "[2022/02/22 15:13:26 __main__ INFO] - Epoch 157 Step 1767/1768 Train Loss 0.43000973\n",
      "[2022/02/22 15:13:35 __main__ INFO] - Epoch 157 Test Loss 0.93887409\n",
      "[2022/02/22 15:13:35 __main__ INFO] - Elapsed 9.29\n",
      "[2022/02/22 15:17:44 __main__ INFO] - Epoch 158 Step 1767/1768 Train Loss 0.42936041\n",
      "[2022/02/22 15:17:53 __main__ INFO] - Epoch 158 Test Loss 0.93710346\n",
      "[2022/02/22 15:17:53 __main__ INFO] - Elapsed 9.33\n",
      "[2022/02/22 15:22:02 __main__ INFO] - Epoch 159 Step 1767/1768 Train Loss 0.42990358\n",
      "[2022/02/22 15:22:11 __main__ INFO] - Epoch 159 Test Loss 0.93851880\n",
      "[2022/02/22 15:22:11 __main__ INFO] - Elapsed 9.33\n",
      "[2022/02/22 15:26:20 __main__ INFO] - Epoch 160 Step 1767/1768 Train Loss 0.42922301\n",
      "[2022/02/22 15:26:30 __main__ INFO] - Epoch 160 Test Loss 0.93874309\n",
      "[2022/02/22 15:26:30 __main__ INFO] - Elapsed 9.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 160\n"
     ]
    }
   ],
   "source": [
    "# Test with Adam Optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=optim_config['base_lr'],\n",
    "#                              weight_decay=optim_config['weight_decay'])\n",
    "\n",
    "# run test before start training\n",
    "# test_outputs = test(0, model, criterion, test_loader, run_config, writer, device)\n",
    "\n",
    "ref1 = 0\n",
    "ref2 = 0\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "train_flag = True\n",
    "\n",
    "run_config['scheduler'] = scheduler\n",
    "    \n",
    "for epoch in range(96, 250):\n",
    "\n",
    "    loss_ = train(epoch, model, optimizer, criterion, train_loader, (demo_cs,demo_np), run_config,\n",
    "         writer, device, logger=logger)\n",
    "    train_loss_list.append(loss_)\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    test_loss_ = test(epoch, model, criterion, test_loader, (demo_cs,demo_np), run_config,\n",
    "                    writer, device, logger, return_output=False)\n",
    "    test_loss_list.append(test_loss_)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        if epoch > 105:\n",
    "            if (np.abs(loss_ - ref1)/ref1<ref1*0.01) & (np.abs(loss_ - ref2)/ref2<ref2*0.01):\n",
    "                print(\"Early stopping at epoch\", epoch)\n",
    "                break\n",
    "            if (ref1 < loss_) & (ref1 < ref2):\n",
    "                print(\"Diverging. stop.\")\n",
    "                train_flag = False\n",
    "                break\n",
    "            if loss_ < best:\n",
    "                best = loss_\n",
    "                best_epoch = epoch\n",
    "        else:\n",
    "            best = loss_\n",
    "            best_epoch = epoch\n",
    "\n",
    "        ref2 = ref1\n",
    "        ref1 = loss_\n",
    "\n",
    "        if (config['run_config']['save']) & (best_epoch==epoch):\n",
    "            torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'config': config},\n",
    "                model_dir+\"SAE_\"+zoomlevel+\"_\"+str(model_config['output_dim']**2*2048)+\"_\"+\n",
    "                model_run_date+\"_\"+str(epoch)+\".pt\")\n",
    "\n",
    "            \n",
    "if config['run_config']['save']:\n",
    "    files = glob.glob(model_dir+\"SAE_\"+zoomlevel+\"_\"+str(model_config['output_dim']**2*2048)+\"_\"+\n",
    "                              model_run_date+\"_*.pt\")\n",
    "\n",
    "    for f in files:\n",
    "        e = int(f.split(\"_\")[-1].split(\".\")[0])\n",
    "        if e != best_epoch:\n",
    "            os.remove(f)\n",
    "\n",
    "        \n",
    "if run_config['tensorboard']:\n",
    "    outpath = os.path.join(outdir, 'all_scalars.json')\n",
    "    writer.export_scalars_to_json(outpath)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "ax.plot(train_loss_list, color='cornflowerblue', label='Train')\n",
    "ax.plot(test_loss_list, color='sandybrown', label='Test')\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_ylim([0, 1.1*np.max(train_loss_list+test_loss_list)])\n",
    "ax.legend()\n",
    "plt.show()\n",
    "fig.savefig(out_dir+\"training_plots/SAE_\"+model_run_date+\".png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t10\t20\t30\t40\t50\t60\t70\t80\t90\t100\t110\t120\t130\t140\t150\t160\t170\t180\t190\t0.40756353766276227 0.010623590972356966\n",
      "0\t10\t20\t30\t40\t50\t60\t70\t80\t90\t100\t110\t120\t130\t140\t150\t160\t170\t180\t190\t200\t210\t220\t230\t240\t250\t260\t270\t280\t290\t300\t310\t320\t330\t340\t350\t360\t370\t380\t390\t400\t410\t420\t430\t440\t450\t460\t470\t480\t490\t500\t510\t520\t530\t540\t550\t560\t570\t580\t590\t600\t610\t620\t630\t640\t650\t660\t670\t680\t690\t700\t710\t720\t730\t740\t750\t760\t770\t780\t790\t800\t810\t820\t830\t840\t850\t860\t870\t880\t890\t900\t910\t920\t930\t940\t950\t960\t970\t980\t990\t1000\t1010\t1020\t1030\t1040\t1050\t1060\t1070\t1080\t1090\t1100\t1110\t1120\t1130\t1140\t1150\t1160\t1170\t1180\t1190\t1200\t1210\t1220\t1230\t1240\t1250\t1260\t1270\t1280\t1290\t1300\t1310\t1320\t1330\t1340\t1350\t1360\t1370\t1380\t1390\t1400\t1410\t1420\t1430\t1440\t1450\t1460\t1470\t1480\t1490\t1500\t1510\t"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "loss_meter_1 = AverageMeter()\n",
    "loss_meter_2 = AverageMeter()\n",
    "\n",
    "for step, (image_list, data) in enumerate(test_loader):\n",
    "\n",
    "    census_index = [demo_cs.index(i[i.rfind('/')+1:i.rfind('_')]) for i in image_list]\n",
    "    census_data = demo_np[census_index]\n",
    "\n",
    "    census_data = torch.tensor(census_data).to(device)\n",
    "    data = data.to(device)\n",
    "\n",
    "    out_image, out_demo = model(data)\n",
    "\n",
    "    loss1, loss2 = criterion(out_image, out_demo, data, census_data, return_components=True)\n",
    "\n",
    "    num = data.size(0)\n",
    "\n",
    "    loss_meter_1.update(loss1.item(), num)\n",
    "    loss_meter_2.update(loss2.item(), num)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(step, end='\\t')\n",
    "\n",
    "best_test_1 = loss_meter_1.avg\n",
    "best_test_2 = loss_meter_2.avg\n",
    "print(best_test_1, best_test_2)         \n",
    "\n",
    "loss_meter_1 = AverageMeter()\n",
    "loss_meter_2 = AverageMeter()                                                              \n",
    "for step, (image_list, data) in enumerate(train_loader):\n",
    "\n",
    "    census_index = [demo_cs.index(i[i.rfind('/')+1:i.rfind('_')]) for i in image_list]\n",
    "    census_data = demo_np[census_index]\n",
    "\n",
    "    census_data = torch.tensor(census_data).to(device)\n",
    "    data = data.to(device)\n",
    "\n",
    "    out_image, out_demo = model(data)\n",
    "\n",
    "    loss1, loss2 = criterion(out_image, out_demo, data, census_data, return_components=True)\n",
    "\n",
    "    num = data.size(0)\n",
    "\n",
    "    loss_meter_1.update(loss1.item(), num)\n",
    "    loss_meter_2.update(loss2.item(), num)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(step, end='\\t')\n",
    "\n",
    "best_1 = loss_meter_1.avg\n",
    "best_2 = loss_meter_2.avg\n",
    "print(best_1, best_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_dir+\"SAE_train.csv\", \"a\") as f:\n",
    "    f.write(\"%s,%s,%d,%s,%s,%d,%.4f,%.4f,%.4f,%.4f,%d\\n\" % (model_run_date, zoomlevel, model_config['output_dim']**2*2048, \n",
    "            sampling, normalization, best_epoch, best_1, best_2, best_test_1, best_test_2, train_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse Normalization\n",
    "\n",
    "# CIFAR\n",
    "# inv_normalize = torchvision.transforms.Normalize(\n",
    "#     mean=[-0.4914/0.2470, -0.4822/0.2435, -0.4465/0.2616],\n",
    "#     std=[1/0.2470, 1/0.2435, 1/0.2616]\n",
    "# )\n",
    "\n",
    "\n",
    "# Satellite image\n",
    "inv_normalize = torchvision.transforms.Normalize(\n",
    "    mean=[-0.3733/0.2173, -0.3991/0.2055, -0.3711/0.2143],\n",
    "    std=[1/0.2173, 1/0.2055, 1/0.2143]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for step, (_,data) in enumerate(test_loader):\n",
    "    data = data.to(device)\n",
    "    test_output = model(data)\n",
    "    test_output_orig = inv_normalize(test_output)\n",
    "    data_orig = inv_normalize(data)\n",
    "    if step == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist(data[plot_image, :, :, :].cpu().detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_output[plot_image, :, :, :].cpu().detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(test_output[:,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(test_output[:,1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(test_output[:,2,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(data[:,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(data[:,1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(data[:,1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((test_output - data).detach().cpu().numpy()[:,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((test_output - data).detach().cpu().numpy()[:,1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((test_output - data).detach().cpu().numpy()[:,2,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((test_output - data).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.power((test_output - data).cpu().detach().numpy(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((test_output - data).detach().cpu().numpy()[:,2,:,:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((test_output - data).detach().cpu().numpy()[:,1,:,:].flatten());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((test_output - data).detach().cpu().numpy()[:,0,:,:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_orig[plot_image,:,:,:].cpu().detach().permute(1, 2, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_orig[plot_image,:,:,:].cpu().detach().permute(1, 2, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_output_orig[plot_image,:,:,:].cpu().detach().permute(1, 2, 0))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(data_orig[plot_image,:,:,:].cpu().detach().permute(1, 2, 0));\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data.detach().numpy()[:,0,:,:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
