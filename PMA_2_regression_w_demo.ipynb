{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"models/\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import glob\n",
    "import itertools\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from M1_util_train_test import load_model, test\n",
    "import mnl\n",
    "\n",
    "from dataloader import SurveyDataset, load_aggregate_travel_behavior, load_demo, train_test_split\n",
    "from setup import out_dir, data_dir, image_dir, model_dir, proj_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'SAE'\n",
    "load_model_name = 'Autoencoder'\n",
    "load_model_file = 'sae'\n",
    "model_code = 'M1_A2'\n",
    "zoomlevel = 'zoom13_bilateral'\n",
    "output_dim = 1\n",
    "model_run_date = '220115'\n",
    "\n",
    "variable_names = ['active','auto','mas','pt', 'trpgen']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proj_dir+\"latent_space/\"+model_type+\"_\"+zoomlevel+\"_\"+str(output_dim**2*2048)+\"_\"+\n",
    "                       model_run_date+\".pkl\", \"rb\") as f: \n",
    "    encoder_output = pkl.load(f)\n",
    "    im = pkl.load(f)\n",
    "    ct = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Embeddings\n",
    "unique_ct = list(set(ct))\n",
    "unique_ct.sort()\n",
    "ct = np.array(ct)\n",
    "aggregate_embeddings = []\n",
    "for i in unique_ct:\n",
    "    aggregate_embeddings.append(np.mean(encoder_output[ct == i], axis=0))\n",
    "aggregate_embeddings = np.array(aggregate_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demographic variables\n",
    "\n",
    "demo_variables = ['tot_population','pct25_34yrs','pct35_50yrs','pctover65yrs',\n",
    "         'pctwhite_alone','pct_nonwhite','pctblack_alone',\n",
    "         'pct_col_grad','avg_tt_to_work','inc_per_capita']\n",
    "\n",
    "demo_cs, demo_np = load_demo(data_dir)\n",
    "demo = np.hstack((np.array(demo_cs).reshape(-1,1), demo_np))\n",
    "demo = pd.DataFrame(demo, columns = ['geoid'] + demo_variables)\n",
    "demo_split = train_test_split(demo)\n",
    "\n",
    "demo_train = demo_split[~demo_split['train_test'].astype(bool)][demo_variables].to_numpy(dtype=float)\n",
    "demo_test = demo_split[demo_split['train_test'].astype(bool)][demo_variables].to_numpy(dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Trip Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"origin_trip_behavior.csv\"\n",
    "df_pivot = load_aggregate_travel_behavior(file, unique_ct)\n",
    "\n",
    "train_test_index = df_pivot['train_test'].astype(bool).to_numpy()\n",
    "\n",
    "y = df_pivot[variable_names].to_numpy()\n",
    "y_train = y[~train_test_index,:4]\n",
    "y_test = y[train_test_index,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_train = aggregate_embeddings[~train_test_index, :]\n",
    "embedding_test = aggregate_embeddings[train_test_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate([demo_train, embedding_train], axis=1)\n",
    "x_test = np.concatenate([demo_test, embedding_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNL for Mode Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader and model definition\n",
    "\n",
    "trainset = SurveyDataset(torch.tensor(x_train,  dtype=torch.float), torch.tensor(y_train, dtype=torch.float))\n",
    "trainloader = DataLoader(trainset, batch_size=256, shuffle=True)\n",
    "\n",
    "testset = SurveyDataset(torch.tensor(x_test, dtype=torch.float), torch.tensor(y_test, dtype=torch.float))\n",
    "testloader = DataLoader(testset, batch_size=len(testset), shuffle=True)\n",
    "\n",
    "kldivloss = nn.KLDivLoss(reduction='sum')\n",
    "mseloss = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:  0] Train KL loss: 0.596                 RMSE 0.569                 0.208 0.472 0.199 0.133\n",
      "[epoch:  0] Test KL loss: 0.264                    RMSE 0.336                     0.182 0.256 0.064 0.103\n",
      "\t\t\t\t\t\t\tR2 score: -0.050 -0.053 -2.029 -0.059 \n",
      "[epoch: 10] Train KL loss: 0.114                 RMSE 0.197                 0.117 0.139 0.054 0.054\n",
      "[epoch: 10] Test KL loss: 0.209                    RMSE 0.296                     0.166 0.222 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.126 0.204 -0.074 0.093 \n",
      "[epoch: 20] Train KL loss: 0.100                 RMSE 0.182                 0.107 0.128 0.052 0.051\n",
      "[epoch: 20] Test KL loss: 0.185                    RMSE 0.281                     0.160 0.209 0.037 0.091\n",
      "\t\t\t\t\t\t\tR2 score: 0.185 0.296 -0.021 0.188 \n",
      "[epoch: 30] Train KL loss: 0.094                 RMSE 0.173                 0.100 0.121 0.051 0.050\n",
      "[epoch: 30] Test KL loss: 0.184                    RMSE 0.278                     0.160 0.206 0.038 0.091\n",
      "\t\t\t\t\t\t\tR2 score: 0.190 0.317 -0.035 0.184 \n",
      "[epoch: 40] Train KL loss: 0.089                 RMSE 0.167                 0.096 0.117 0.051 0.049\n",
      "[epoch: 40] Test KL loss: 0.181                    RMSE 0.274                     0.158 0.202 0.038 0.090\n",
      "\t\t\t\t\t\t\tR2 score: 0.209 0.344 -0.042 0.190 \n",
      "[epoch: 50] Train KL loss: 0.085                 RMSE 0.161                 0.092 0.112 0.050 0.048\n",
      "[epoch: 50] Test KL loss: 0.182                    RMSE 0.272                     0.158 0.198 0.040 0.089\n",
      "\t\t\t\t\t\t\tR2 score: 0.206 0.366 -0.141 0.208 \n",
      "[epoch: 60] Train KL loss: 0.082                 RMSE 0.157                 0.090 0.109 0.050 0.047\n",
      "[epoch: 60] Test KL loss: 0.185                    RMSE 0.272                     0.157 0.199 0.039 0.091\n",
      "\t\t\t\t\t\t\tR2 score: 0.216 0.358 -0.106 0.187 \n",
      "[epoch: 70] Train KL loss: 0.079                 RMSE 0.152                 0.087 0.105 0.049 0.046\n",
      "[epoch: 70] Test KL loss: 0.178                    RMSE 0.265                     0.152 0.193 0.039 0.091\n",
      "\t\t\t\t\t\t\tR2 score: 0.262 0.402 -0.119 0.187 \n",
      "[epoch: 80] Train KL loss: 0.077                 RMSE 0.149                 0.085 0.102 0.048 0.046\n",
      "[epoch: 80] Test KL loss: 0.177                    RMSE 0.262                     0.152 0.190 0.040 0.089\n",
      "\t\t\t\t\t\t\tR2 score: 0.261 0.420 -0.196 0.215 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.645                 RMSE 0.592                 0.208 0.494 0.159 0.194\n",
      "[epoch:  0] Test KL loss: 0.262                    RMSE 0.337                     0.175 0.255 0.051 0.124\n",
      "\t\t\t\t\t\t\tR2 score: 0.030 -0.052 -0.929 -0.521 \n",
      "[epoch: 10] Train KL loss: 0.123                 RMSE 0.204                 0.119 0.145 0.055 0.057\n",
      "[epoch: 10] Test KL loss: 0.153                    RMSE 0.248                     0.137 0.184 0.038 0.087\n",
      "\t\t\t\t\t\t\tR2 score: 0.399 0.456 -0.052 0.246 \n",
      "[epoch: 20] Train KL loss: 0.103                 RMSE 0.186                 0.110 0.131 0.052 0.052\n",
      "[epoch: 20] Test KL loss: 0.157                    RMSE 0.256                     0.145 0.189 0.037 0.085\n",
      "\t\t\t\t\t\t\tR2 score: 0.334 0.422 -0.004 0.282 \n",
      "[epoch: 30] Train KL loss: 0.095                 RMSE 0.174                 0.102 0.122 0.052 0.050\n",
      "[epoch: 30] Test KL loss: 0.160                    RMSE 0.256                     0.147 0.188 0.037 0.085\n",
      "\t\t\t\t\t\t\tR2 score: 0.315 0.427 0.007 0.284 \n",
      "[epoch: 40] Train KL loss: 0.089                 RMSE 0.165                 0.096 0.115 0.051 0.049\n",
      "[epoch: 40] Test KL loss: 0.165                    RMSE 0.258                     0.149 0.188 0.037 0.086\n",
      "\t\t\t\t\t\t\tR2 score: 0.293 0.428 -0.005 0.269 \n",
      "[epoch: 50] Train KL loss: 0.085                 RMSE 0.161                 0.093 0.111 0.051 0.048\n",
      "[epoch: 50] Test KL loss: 0.171                    RMSE 0.260                     0.150 0.189 0.038 0.086\n",
      "\t\t\t\t\t\t\tR2 score: 0.279 0.422 -0.067 0.262 \n",
      "[epoch: 60] Train KL loss: 0.083                 RMSE 0.156                 0.090 0.107 0.050 0.047\n",
      "[epoch: 60] Test KL loss: 0.172                    RMSE 0.258                     0.150 0.187 0.039 0.087\n",
      "\t\t\t\t\t\t\tR2 score: 0.288 0.435 -0.087 0.244 \n",
      "[epoch: 70] Train KL loss: 0.080                 RMSE 0.153                 0.088 0.105 0.050 0.047\n",
      "[epoch: 70] Test KL loss: 0.176                    RMSE 0.261                     0.150 0.190 0.039 0.089\n",
      "\t\t\t\t\t\t\tR2 score: 0.282 0.415 -0.082 0.220 \n",
      "[epoch: 80] Train KL loss: 0.079                 RMSE 0.150                 0.085 0.103 0.050 0.046\n",
      "[epoch: 80] Test KL loss: 0.160                    RMSE 0.249                     0.146 0.178 0.038 0.085\n",
      "\t\t\t\t\t\t\tR2 score: 0.319 0.488 -0.080 0.278 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.716                 RMSE 0.627                 0.216 0.525 0.151 0.218\n",
      "[epoch:  0] Test KL loss: 0.282                    RMSE 0.365                     0.181 0.287 0.042 0.130\n",
      "\t\t\t\t\t\t\tR2 score: -0.038 -0.324 -0.284 -0.676 \n",
      "[epoch: 10] Train KL loss: 0.122                 RMSE 0.204                 0.119 0.146 0.056 0.055\n",
      "[epoch: 10] Test KL loss: 0.159                    RMSE 0.252                     0.140 0.185 0.041 0.088\n",
      "\t\t\t\t\t\t\tR2 score: 0.375 0.448 -0.205 0.228 \n",
      "[epoch: 20] Train KL loss: 0.103                 RMSE 0.184                 0.108 0.130 0.052 0.052\n",
      "[epoch: 20] Test KL loss: 0.160                    RMSE 0.257                     0.143 0.191 0.037 0.088\n",
      "\t\t\t\t\t\t\tR2 score: 0.345 0.414 -0.020 0.236 \n",
      "[epoch: 30] Train KL loss: 0.093                 RMSE 0.171                 0.098 0.120 0.052 0.050\n",
      "[epoch: 30] Test KL loss: 0.163                    RMSE 0.258                     0.146 0.190 0.038 0.088\n",
      "\t\t\t\t\t\t\tR2 score: 0.320 0.415 -0.033 0.232 \n",
      "[epoch: 40] Train KL loss: 0.087                 RMSE 0.162                 0.092 0.114 0.051 0.049\n",
      "[epoch: 40] Test KL loss: 0.170                    RMSE 0.262                     0.149 0.193 0.038 0.088\n",
      "\t\t\t\t\t\t\tR2 score: 0.290 0.400 -0.061 0.232 \n",
      "[epoch: 50] Train KL loss: 0.084                 RMSE 0.159                 0.090 0.110 0.050 0.048\n",
      "[epoch: 50] Test KL loss: 0.165                    RMSE 0.256                     0.148 0.186 0.040 0.086\n",
      "\t\t\t\t\t\t\tR2 score: 0.301 0.441 -0.153 0.266 \n",
      "[epoch: 60] Train KL loss: 0.082                 RMSE 0.155                 0.089 0.107 0.050 0.048\n",
      "[epoch: 60] Test KL loss: 0.172                    RMSE 0.261                     0.149 0.190 0.040 0.089\n",
      "\t\t\t\t\t\t\tR2 score: 0.288 0.415 -0.156 0.217 \n",
      "[epoch: 70] Train KL loss: 0.080                 RMSE 0.152                 0.087 0.105 0.050 0.047\n",
      "[epoch: 70] Test KL loss: 0.179                    RMSE 0.265                     0.151 0.194 0.041 0.090\n",
      "\t\t\t\t\t\t\tR2 score: 0.270 0.392 -0.235 0.197 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.561                 RMSE 0.547                 0.192 0.453 0.151 0.187\n",
      "[epoch:  0] Test KL loss: 0.251                    RMSE 0.329                     0.183 0.247 0.039 0.110\n",
      "\t\t\t\t\t\t\tR2 score: -0.070 0.017 -0.101 -0.205 \n",
      "[epoch: 10] Train KL loss: 0.115                 RMSE 0.199                 0.119 0.139 0.055 0.055\n",
      "[epoch: 10] Test KL loss: 0.163                    RMSE 0.255                     0.141 0.189 0.038 0.090\n",
      "\t\t\t\t\t\t\tR2 score: 0.362 0.424 -0.045 0.196 \n",
      "[epoch: 20] Train KL loss: 0.100                 RMSE 0.182                 0.107 0.128 0.052 0.052\n",
      "[epoch: 20] Test KL loss: 0.170                    RMSE 0.267                     0.150 0.198 0.037 0.089\n",
      "\t\t\t\t\t\t\tR2 score: 0.282 0.365 0.008 0.213 \n",
      "[epoch: 30] Train KL loss: 0.093                 RMSE 0.172                 0.099 0.120 0.051 0.051\n",
      "[epoch: 30] Test KL loss: 0.165                    RMSE 0.259                     0.148 0.189 0.038 0.088\n",
      "\t\t\t\t\t\t\tR2 score: 0.305 0.422 -0.062 0.237 \n",
      "[epoch: 40] Train KL loss: 0.088                 RMSE 0.165                 0.095 0.114 0.051 0.050\n",
      "[epoch: 40] Test KL loss: 0.164                    RMSE 0.255                     0.147 0.185 0.038 0.088\n",
      "\t\t\t\t\t\t\tR2 score: 0.310 0.445 -0.053 0.233 \n",
      "[epoch: 50] Train KL loss: 0.084                 RMSE 0.158                 0.090 0.110 0.050 0.048\n",
      "[epoch: 50] Test KL loss: 0.162                    RMSE 0.251                     0.145 0.181 0.040 0.087\n",
      "\t\t\t\t\t\t\tR2 score: 0.330 0.474 -0.152 0.248 \n",
      "[epoch: 60] Train KL loss: 0.082                 RMSE 0.154                 0.088 0.107 0.049 0.047\n",
      "[epoch: 60] Test KL loss: 0.174                    RMSE 0.258                     0.146 0.188 0.039 0.091\n",
      "\t\t\t\t\t\t\tR2 score: 0.322 0.431 -0.122 0.185 \n",
      "[epoch: 70] Train KL loss: 0.078                 RMSE 0.150                 0.086 0.102 0.049 0.046\n",
      "[epoch: 70] Test KL loss: 0.170                    RMSE 0.253                     0.144 0.184 0.040 0.090\n",
      "\t\t\t\t\t\t\tR2 score: 0.337 0.456 -0.173 0.205 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.731                 RMSE 0.636                 0.237 0.534 0.206 0.145\n",
      "[epoch:  0] Test KL loss: 0.305                    RMSE 0.384                     0.194 0.304 0.079 0.104\n",
      "\t\t\t\t\t\t\tR2 score: -0.196 -0.495 -3.551 -0.072 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 10] Train KL loss: 0.125                 RMSE 0.205                 0.118 0.146 0.061 0.055\n",
      "[epoch: 10] Test KL loss: 0.150                    RMSE 0.244                     0.138 0.177 0.042 0.086\n",
      "\t\t\t\t\t\t\tR2 score: 0.394 0.493 -0.285 0.273 \n",
      "[epoch: 20] Train KL loss: 0.102                 RMSE 0.183                 0.108 0.129 0.052 0.051\n",
      "[epoch: 20] Test KL loss: 0.155                    RMSE 0.254                     0.143 0.187 0.038 0.087\n",
      "\t\t\t\t\t\t\tR2 score: 0.345 0.434 -0.026 0.256 \n",
      "[epoch: 30] Train KL loss: 0.091                 RMSE 0.168                 0.096 0.117 0.052 0.049\n",
      "[epoch: 30] Test KL loss: 0.162                    RMSE 0.257                     0.145 0.189 0.038 0.088\n",
      "\t\t\t\t\t\t\tR2 score: 0.328 0.424 -0.032 0.231 \n",
      "[epoch: 40] Train KL loss: 0.085                 RMSE 0.160                 0.091 0.111 0.051 0.047\n",
      "[epoch: 40] Test KL loss: 0.159                    RMSE 0.251                     0.146 0.182 0.039 0.085\n",
      "\t\t\t\t\t\t\tR2 score: 0.324 0.464 -0.120 0.290 \n",
      "[epoch: 50] Train KL loss: 0.082                 RMSE 0.154                 0.088 0.107 0.051 0.045\n",
      "[epoch: 50] Test KL loss: 0.160                    RMSE 0.251                     0.146 0.182 0.039 0.084\n",
      "\t\t\t\t\t\t\tR2 score: 0.324 0.465 -0.122 0.297 \n",
      "[epoch: 60] Train KL loss: 0.081                 RMSE 0.154                 0.088 0.106 0.050 0.045\n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.540                 RMSE 0.539                 0.196 0.448 0.160 0.159\n",
      "[epoch:  0] Test KL loss: 0.230                    RMSE 0.319                     0.173 0.245 0.047 0.098\n",
      "\t\t\t\t\t\t\tR2 score: 0.046 0.030 -0.610 0.046 \n",
      "[epoch: 10] Train KL loss: 0.115                 RMSE 0.200                 0.118 0.143 0.054 0.053\n",
      "[epoch: 10] Test KL loss: 0.170                    RMSE 0.259                     0.146 0.188 0.043 0.091\n",
      "\t\t\t\t\t\t\tR2 score: 0.322 0.427 -0.333 0.181 \n",
      "[epoch: 20] Train KL loss: 0.098                 RMSE 0.178                 0.103 0.126 0.052 0.051\n",
      "[epoch: 20] Test KL loss: 0.163                    RMSE 0.258                     0.148 0.189 0.039 0.087\n",
      "\t\t\t\t\t\t\tR2 score: 0.304 0.425 -0.108 0.253 \n",
      "[epoch: 30] Train KL loss: 0.091                 RMSE 0.168                 0.096 0.118 0.051 0.049\n",
      "[epoch: 30] Test KL loss: 0.192                    RMSE 0.278                     0.160 0.205 0.040 0.090\n",
      "\t\t\t\t\t\t\tR2 score: 0.182 0.325 -0.172 0.189 \n",
      "[epoch: 40] Train KL loss: 0.086                 RMSE 0.162                 0.092 0.113 0.051 0.049\n",
      "[epoch: 40] Test KL loss: 0.193                    RMSE 0.278                     0.161 0.203 0.041 0.090\n",
      "\t\t\t\t\t\t\tR2 score: 0.172 0.332 -0.212 0.193 \n",
      "[epoch: 50] Train KL loss: 0.085                 RMSE 0.160                 0.092 0.111 0.050 0.047\n",
      "[epoch: 50] Test KL loss: 0.194                    RMSE 0.279                     0.162 0.204 0.041 0.090\n",
      "\t\t\t\t\t\t\tR2 score: 0.167 0.329 -0.228 0.192 \n",
      "[epoch: 60] Train KL loss: 0.084                 RMSE 0.159                 0.092 0.111 0.050 0.047\n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.613                 RMSE 0.564                 0.187 0.468 0.176 0.183\n",
      "[epoch:  0] Test KL loss: 0.448                    RMSE 0.406                     0.216 0.322 0.047 0.112\n",
      "\t\t\t\t\t\t\tR2 score: -0.489 -0.668 -0.582 -0.245 \n",
      "[epoch: 10] Train KL loss: 0.106                 RMSE 0.186                 0.110 0.129 0.055 0.053\n",
      "[epoch: 10] Test KL loss: 0.166                    RMSE 0.260                     0.147 0.192 0.040 0.086\n",
      "\t\t\t\t\t\t\tR2 score: 0.307 0.406 -0.155 0.260 \n",
      "[epoch: 20] Train KL loss: 0.094                 RMSE 0.173                 0.100 0.121 0.052 0.050\n",
      "[epoch: 20] Test KL loss: 0.158                    RMSE 0.254                     0.144 0.187 0.037 0.085\n",
      "\t\t\t\t\t\t\tR2 score: 0.342 0.434 -0.021 0.280 \n",
      "[epoch: 30] Train KL loss: 0.088                 RMSE 0.165                 0.095 0.114 0.051 0.049\n",
      "[epoch: 30] Test KL loss: 0.171                    RMSE 0.263                     0.149 0.194 0.037 0.088\n",
      "\t\t\t\t\t\t\tR2 score: 0.290 0.390 -0.025 0.238 \n",
      "[epoch: 40] Train KL loss: 0.083                 RMSE 0.158                 0.090 0.109 0.051 0.048\n",
      "[epoch: 40] Test KL loss: 0.179                    RMSE 0.266                     0.151 0.195 0.039 0.089\n",
      "\t\t\t\t\t\t\tR2 score: 0.271 0.385 -0.094 0.209 \n",
      "[epoch: 50] Train KL loss: 0.084                 RMSE 0.157                 0.090 0.108 0.050 0.048\n",
      "[epoch: 50] Test KL loss: 0.208                    RMSE 0.286                     0.159 0.215 0.038 0.095\n",
      "\t\t\t\t\t\t\tR2 score: 0.193 0.256 -0.059 0.106 \n",
      "[epoch: 60] Train KL loss: 0.081                 RMSE 0.153                 0.088 0.106 0.049 0.047\n",
      "[epoch: 60] Test KL loss: 0.206                    RMSE 0.283                     0.159 0.211 0.039 0.093\n",
      "\t\t\t\t\t\t\tR2 score: 0.193 0.281 -0.127 0.140 \n",
      "[epoch: 70] Train KL loss: 0.076                 RMSE 0.147                 0.084 0.100 0.049 0.046\n",
      "[epoch: 70] Test KL loss: 0.211                    RMSE 0.284                     0.160 0.210 0.042 0.094\n",
      "\t\t\t\t\t\t\tR2 score: 0.181 0.286 -0.278 0.116 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.622                 RMSE 0.573                 0.191 0.474 0.200 0.165\n",
      "[epoch:  0] Test KL loss: 0.406                    RMSE 0.397                     0.212 0.314 0.046 0.111\n",
      "\t\t\t\t\t\t\tR2 score: -0.434 -0.590 -0.565 -0.212 \n",
      "[epoch: 10] Train KL loss: 0.108                 RMSE 0.190                 0.113 0.134 0.053 0.052\n",
      "[epoch: 10] Test KL loss: 0.155                    RMSE 0.250                     0.139 0.184 0.038 0.088\n",
      "\t\t\t\t\t\t\tR2 score: 0.388 0.453 -0.052 0.224 \n",
      "[epoch: 20] Train KL loss: 0.093                 RMSE 0.171                 0.099 0.120 0.052 0.050\n",
      "[epoch: 20] Test KL loss: 0.169                    RMSE 0.265                     0.150 0.196 0.038 0.087\n",
      "\t\t\t\t\t\t\tR2 score: 0.286 0.378 -0.029 0.242 \n",
      "[epoch: 30] Train KL loss: 0.087                 RMSE 0.163                 0.094 0.113 0.050 0.050\n",
      "[epoch: 30] Test KL loss: 0.170                    RMSE 0.262                     0.150 0.193 0.038 0.086\n",
      "\t\t\t\t\t\t\tR2 score: 0.279 0.401 -0.073 0.274 \n",
      "[epoch: 40] Train KL loss: 0.083                 RMSE 0.158                 0.091 0.109 0.050 0.048\n",
      "[epoch: 40] Test KL loss: 0.194                    RMSE 0.277                     0.157 0.206 0.041 0.091\n",
      "\t\t\t\t\t\t\tR2 score: 0.214 0.318 -0.222 0.180 \n",
      "[epoch: 50] Train KL loss: 0.081                 RMSE 0.155                 0.089 0.106 0.049 0.048\n",
      "[epoch: 50] Test KL loss: 0.195                    RMSE 0.276                     0.156 0.204 0.042 0.091\n",
      "\t\t\t\t\t\t\tR2 score: 0.225 0.326 -0.293 0.177 \n",
      "[epoch: 60] Train KL loss: 0.079                 RMSE 0.151                 0.087 0.103 0.049 0.047\n",
      "[epoch: 60] Test KL loss: 0.193                    RMSE 0.275                     0.155 0.203 0.044 0.091\n",
      "\t\t\t\t\t\t\tR2 score: 0.237 0.332 -0.405 0.174 \n",
      "[epoch: 70] Train KL loss: 0.076                 RMSE 0.147                 0.084 0.100 0.048 0.046\n",
      "[epoch: 70] Test KL loss: 0.189                    RMSE 0.271                     0.152 0.200 0.045 0.091\n",
      "\t\t\t\t\t\t\tR2 score: 0.261 0.355 -0.494 0.178 \n",
      "[epoch: 80] Train KL loss: 0.077                 RMSE 0.148                 0.085 0.101 0.047 0.046\n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.546                 RMSE 0.542                 0.225 0.452 0.113 0.164\n",
      "[epoch:  0] Test KL loss: 0.447                    RMSE 0.400                     0.206 0.319 0.049 0.118\n",
      "\t\t\t\t\t\t\tR2 score: -0.345 -0.639 -0.782 -0.370 \n",
      "[epoch: 10] Train KL loss: 0.109                 RMSE 0.191                 0.110 0.136 0.054 0.053\n",
      "[epoch: 10] Test KL loss: 0.159                    RMSE 0.253                     0.146 0.184 0.041 0.084\n",
      "\t\t\t\t\t\t\tR2 score: 0.319 0.454 -0.229 0.299 \n",
      "[epoch: 20] Train KL loss: 0.092                 RMSE 0.170                 0.097 0.120 0.052 0.050\n",
      "[epoch: 20] Test KL loss: 0.173                    RMSE 0.265                     0.152 0.194 0.038 0.090\n",
      "\t\t\t\t\t\t\tR2 score: 0.263 0.395 -0.062 0.202 \n",
      "[epoch: 30] Train KL loss: 0.086                 RMSE 0.161                 0.092 0.112 0.051 0.048\n",
      "[epoch: 30] Test KL loss: 0.180                    RMSE 0.270                     0.157 0.197 0.039 0.089\n",
      "\t\t\t\t\t\t\tR2 score: 0.217 0.374 -0.090 0.215 \n",
      "[epoch: 40] Train KL loss: 0.082                 RMSE 0.156                 0.089 0.108 0.051 0.046\n",
      "[epoch: 40] Test KL loss: 0.188                    RMSE 0.272                     0.157 0.198 0.040 0.092\n",
      "\t\t\t\t\t\t\tR2 score: 0.210 0.368 -0.195 0.167 \n",
      "[epoch: 50] Train KL loss: 0.081                 RMSE 0.154                 0.088 0.107 0.050 0.045\n",
      "[epoch: 50] Test KL loss: 0.229                    RMSE 0.299                     0.170 0.222 0.041 0.097\n",
      "\t\t\t\t\t\t\tR2 score: 0.077 0.203 -0.198 0.058 \n",
      "[epoch: 60] Train KL loss: 0.078                 RMSE 0.152                 0.087 0.105 0.049 0.045\n",
      "[epoch: 60] Test KL loss: 0.175                    RMSE 0.260                     0.152 0.186 0.040 0.091\n",
      "\t\t\t\t\t\t\tR2 score: 0.266 0.441 -0.191 0.185 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 70] Train KL loss: 0.076                 RMSE 0.148                 0.085 0.102 0.049 0.043\n",
      "[epoch: 70] Test KL loss: 0.173                    RMSE 0.256                     0.153 0.180 0.044 0.089\n",
      "\t\t\t\t\t\t\tR2 score: 0.256 0.477 -0.403 0.222 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.501                 RMSE 0.515                 0.202 0.428 0.137 0.151\n",
      "[epoch:  0] Test KL loss: 0.361                    RMSE 0.382                     0.200 0.302 0.048 0.111\n",
      "\t\t\t\t\t\t\tR2 score: -0.273 -0.470 -0.675 -0.215 \n",
      "[epoch: 10] Train KL loss: 0.109                 RMSE 0.194                 0.114 0.138 0.052 0.054\n",
      "[epoch: 10] Test KL loss: 0.151                    RMSE 0.250                     0.144 0.183 0.037 0.082\n",
      "\t\t\t\t\t\t\tR2 score: 0.341 0.458 -0.023 0.328 \n",
      "[epoch: 20] Train KL loss: 0.093                 RMSE 0.172                 0.099 0.122 0.051 0.051\n",
      "[epoch: 20] Test KL loss: 0.169                    RMSE 0.264                     0.155 0.193 0.040 0.084\n",
      "\t\t\t\t\t\t\tR2 score: 0.233 0.401 -0.144 0.295 \n",
      "[epoch: 30] Train KL loss: 0.087                 RMSE 0.163                 0.093 0.114 0.051 0.049\n",
      "[epoch: 30] Test KL loss: 0.165                    RMSE 0.258                     0.153 0.186 0.041 0.082\n",
      "\t\t\t\t\t\t\tR2 score: 0.251 0.440 -0.198 0.329 \n",
      "[epoch: 40] Train KL loss: 0.083                 RMSE 0.157                 0.090 0.109 0.050 0.047\n",
      "[epoch: 40] Test KL loss: 0.162                    RMSE 0.255                     0.152 0.184 0.039 0.082\n",
      "\t\t\t\t\t\t\tR2 score: 0.268 0.456 -0.137 0.333 \n",
      "[epoch: 50] Train KL loss: 0.082                 RMSE 0.156                 0.090 0.108 0.050 0.045\n",
      "[epoch: 50] Test KL loss: 0.156                    RMSE 0.249                     0.150 0.179 0.040 0.078\n",
      "\t\t\t\t\t\t\tR2 score: 0.280 0.484 -0.147 0.395 \n",
      "[epoch: 60] Train KL loss: 0.079                 RMSE 0.152                 0.087 0.104 0.049 0.045\n",
      "[epoch: 60] Test KL loss: 0.165                    RMSE 0.256                     0.151 0.184 0.042 0.082\n",
      "\t\t\t\t\t\t\tR2 score: 0.270 0.452 -0.286 0.329 \n",
      "[epoch: 70] Train KL loss: 0.078                 RMSE 0.150                 0.087 0.103 0.049 0.043\n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.493                 RMSE 0.492                 0.193 0.403 0.143 0.148\n",
      "[epoch:  0] Test KL loss: 0.469                    RMSE 0.403                     0.207 0.321 0.049 0.119\n",
      "\t\t\t\t\t\t\tR2 score: -0.360 -0.664 -0.781 -0.397 \n",
      "[epoch: 10] Train KL loss: 0.107                 RMSE 0.189                 0.109 0.135 0.054 0.052\n",
      "[epoch: 10] Test KL loss: 0.171                    RMSE 0.263                     0.150 0.193 0.041 0.087\n",
      "\t\t\t\t\t\t\tR2 score: 0.282 0.398 -0.237 0.244 \n",
      "[epoch: 20] Train KL loss: 0.093                 RMSE 0.172                 0.099 0.121 0.051 0.050\n",
      "[epoch: 20] Test KL loss: 0.174                    RMSE 0.268                     0.154 0.198 0.038 0.088\n",
      "\t\t\t\t\t\t\tR2 score: 0.245 0.367 -0.041 0.238 \n",
      "[epoch: 30] Train KL loss: 0.085                 RMSE 0.160                 0.091 0.111 0.050 0.048\n",
      "[epoch: 30] Test KL loss: 0.189                    RMSE 0.278                     0.160 0.205 0.039 0.089\n",
      "\t\t\t\t\t\t\tR2 score: 0.185 0.322 -0.101 0.211 \n",
      "[epoch: 40] Train KL loss: 0.089                 RMSE 0.166                 0.095 0.116 0.050 0.049\n",
      "[epoch: 40] Test KL loss: 0.176                    RMSE 0.268                     0.158 0.195 0.040 0.085\n",
      "\t\t\t\t\t\t\tR2 score: 0.206 0.389 -0.183 0.289 \n",
      "[epoch: 50] Train KL loss: 0.079                 RMSE 0.151                 0.086 0.104 0.049 0.046\n",
      "[epoch: 50] Test KL loss: 0.170                    RMSE 0.259                     0.150 0.189 0.040 0.085\n",
      "\t\t\t\t\t\t\tR2 score: 0.279 0.424 -0.192 0.291 \n",
      "[epoch: 60] Train KL loss: 0.076                 RMSE 0.145                 0.082 0.100 0.048 0.046\n",
      "[epoch: 60] Test KL loss: 0.182                    RMSE 0.268                     0.152 0.199 0.039 0.087\n",
      "\t\t\t\t\t\t\tR2 score: 0.260 0.363 -0.110 0.249 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.643                 RMSE 0.587                 0.191 0.483 0.220 0.162\n",
      "[epoch:  0] Test KL loss: 0.356                    RMSE 0.361                     0.193 0.278 0.041 0.118\n",
      "\t\t\t\t\t\t\tR2 score: -0.184 -0.251 -0.226 -0.371 \n",
      "[epoch: 10] Train KL loss: 0.110                 RMSE 0.194                 0.115 0.138 0.053 0.053\n",
      "[epoch: 10] Test KL loss: 0.162                    RMSE 0.258                     0.145 0.191 0.039 0.086\n",
      "\t\t\t\t\t\t\tR2 score: 0.328 0.410 -0.105 0.267 \n",
      "[epoch: 20] Train KL loss: 0.093                 RMSE 0.172                 0.099 0.121 0.051 0.050\n",
      "[epoch: 20] Test KL loss: 0.187                    RMSE 0.277                     0.157 0.205 0.039 0.092\n",
      "\t\t\t\t\t\t\tR2 score: 0.211 0.322 -0.117 0.160 \n",
      "[epoch: 30] Train KL loss: 0.087                 RMSE 0.163                 0.094 0.113 0.051 0.049\n",
      "[epoch: 30] Test KL loss: 0.210                    RMSE 0.290                     0.165 0.216 0.041 0.094\n",
      "\t\t\t\t\t\t\tR2 score: 0.137 0.249 -0.245 0.129 \n",
      "[epoch: 40] Train KL loss: 0.088                 RMSE 0.164                 0.092 0.115 0.051 0.050\n",
      "[epoch: 40] Test KL loss: 0.199                    RMSE 0.283                     0.158 0.212 0.040 0.094\n",
      "\t\t\t\t\t\t\tR2 score: 0.207 0.277 -0.141 0.116 \n",
      "[epoch: 50] Train KL loss: 0.083                 RMSE 0.157                 0.091 0.108 0.050 0.046\n",
      "[epoch: 50] Test KL loss: 0.180                    RMSE 0.266                     0.151 0.196 0.040 0.091\n",
      "\t\t\t\t\t\t\tR2 score: 0.275 0.383 -0.154 0.186 \n",
      "[epoch: 60] Train KL loss: 0.080                 RMSE 0.152                 0.086 0.105 0.049 0.046\n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.603                 RMSE 0.517                 0.189 0.423 0.145 0.178\n",
      "[epoch:  0] Test KL loss: 0.528                    RMSE 0.418                     0.219 0.333 0.051 0.115\n",
      "\t\t\t\t\t\t\tR2 score: -0.523 -0.784 -0.903 -0.317 \n",
      "[epoch: 10] Train KL loss: 0.106                 RMSE 0.187                 0.110 0.130 0.055 0.054\n",
      "[epoch: 10] Test KL loss: 0.176                    RMSE 0.270                     0.154 0.200 0.038 0.087\n",
      "\t\t\t\t\t\t\tR2 score: 0.246 0.352 -0.079 0.255 \n",
      "[epoch: 20] Train KL loss: 0.090                 RMSE 0.168                 0.097 0.117 0.051 0.049\n",
      "[epoch: 20] Test KL loss: 0.173                    RMSE 0.262                     0.149 0.192 0.039 0.090\n",
      "\t\t\t\t\t\t\tR2 score: 0.297 0.405 -0.124 0.204 \n",
      "[epoch: 30] Train KL loss: 0.083                 RMSE 0.159                 0.091 0.110 0.049 0.048\n",
      "[epoch: 30] Test KL loss: 0.184                    RMSE 0.269                     0.152 0.197 0.044 0.090\n",
      "\t\t\t\t\t\t\tR2 score: 0.260 0.373 -0.391 0.191 \n",
      "[epoch: 40] Train KL loss: 0.082                 RMSE 0.157                 0.090 0.109 0.049 0.047\n",
      "[epoch: 40] Test KL loss: 0.179                    RMSE 0.265                     0.150 0.193 0.046 0.090\n",
      "\t\t\t\t\t\t\tR2 score: 0.281 0.397 -0.552 0.193 \n",
      "[epoch: 50] Train KL loss: 0.078                 RMSE 0.149                 0.087 0.102 0.048 0.045\n",
      "[epoch: 50] Test KL loss: 0.177                    RMSE 0.263                     0.147 0.193 0.044 0.091\n",
      "\t\t\t\t\t\t\tR2 score: 0.308 0.397 -0.433 0.180 \n",
      "[epoch: 60] Train KL loss: 0.076                 RMSE 0.145                 0.082 0.099 0.049 0.045\n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.564                 RMSE 0.520                 0.202 0.430 0.170 0.124\n",
      "[epoch:  0] Test KL loss: 0.431                    RMSE 0.317                     0.187 0.220 0.049 0.121\n",
      "\t\t\t\t\t\t\tR2 score: -0.116 0.217 -0.753 -0.459 \n",
      "[epoch: 10] Train KL loss: 0.103                 RMSE 0.184                 0.110 0.128 0.053 0.053\n",
      "[epoch: 10] Test KL loss: 0.191                    RMSE 0.283                     0.161 0.210 0.037 0.093\n",
      "\t\t\t\t\t\t\tR2 score: 0.179 0.285 -0.022 0.145 \n",
      "[epoch: 20] Train KL loss: 0.092                 RMSE 0.170                 0.098 0.119 0.051 0.049\n",
      "[epoch: 20] Test KL loss: 0.254                    RMSE 0.319                     0.179 0.242 0.038 0.100\n",
      "\t\t\t\t\t\t\tR2 score: -0.021 0.056 -0.042 0.010 \n",
      "[epoch: 30] Train KL loss: 0.084                 RMSE 0.160                 0.092 0.112 0.050 0.049\n",
      "[epoch: 30] Test KL loss: 0.197                    RMSE 0.284                     0.164 0.209 0.040 0.092\n",
      "\t\t\t\t\t\t\tR2 score: 0.140 0.297 -0.159 0.165 \n",
      "[epoch: 40] Train KL loss: 0.078                 RMSE 0.149                 0.086 0.102 0.049 0.047\n",
      "[epoch: 40] Test KL loss: 0.200                    RMSE 0.287                     0.163 0.213 0.039 0.094\n",
      "\t\t\t\t\t\t\tR2 score: 0.152 0.270 -0.089 0.119 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 50] Train KL loss: 0.074                 RMSE 0.143                 0.082 0.097 0.048 0.045\n",
      "[epoch: 50] Test KL loss: 0.204                    RMSE 0.288                     0.166 0.212 0.041 0.094\n",
      "\t\t\t\t\t\t\tR2 score: 0.124 0.278 -0.221 0.133 \n",
      "[epoch: 60] Train KL loss: 0.078                 RMSE 0.151                 0.085 0.104 0.049 0.048\n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.436                 RMSE 0.461                 0.190 0.373 0.141 0.132\n",
      "[epoch:  0] Test KL loss: 0.233                    RMSE 0.319                     0.171 0.245 0.053 0.098\n",
      "\t\t\t\t\t\t\tR2 score: 0.066 0.034 -1.029 0.040 \n",
      "[epoch: 10] Train KL loss: 0.102                 RMSE 0.183                 0.107 0.128 0.052 0.051\n",
      "[epoch: 10] Test KL loss: 0.199                    RMSE 0.288                     0.165 0.213 0.040 0.092\n",
      "\t\t\t\t\t\t\tR2 score: 0.131 0.270 -0.196 0.155 \n",
      "[epoch: 20] Train KL loss: 0.088                 RMSE 0.163                 0.094 0.113 0.050 0.049\n",
      "[epoch: 20] Test KL loss: 0.179                    RMSE 0.269                     0.155 0.197 0.042 0.088\n",
      "\t\t\t\t\t\t\tR2 score: 0.231 0.376 -0.294 0.226 \n",
      "[epoch: 30] Train KL loss: 0.088                 RMSE 0.159                 0.092 0.109 0.051 0.048\n",
      "[epoch: 30] Test KL loss: 0.187                    RMSE 0.275                     0.159 0.201 0.044 0.089\n",
      "\t\t\t\t\t\t\tR2 score: 0.196 0.347 -0.408 0.221 \n",
      "[epoch: 40] Train KL loss: 0.079                 RMSE 0.149                 0.086 0.102 0.050 0.045\n",
      "[epoch: 40] Test KL loss: 0.187                    RMSE 0.276                     0.156 0.204 0.042 0.092\n",
      "\t\t\t\t\t\t\tR2 score: 0.222 0.331 -0.284 0.169 \n",
      "[epoch: 50] Train KL loss: 0.076                 RMSE 0.145                 0.085 0.098 0.049 0.043\n",
      "[epoch: 50] Test KL loss: 0.172                    RMSE 0.261                     0.150 0.189 0.045 0.088\n",
      "\t\t\t\t\t\t\tR2 score: 0.285 0.421 -0.492 0.230 \n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.533                 RMSE 0.518                 0.198 0.426 0.142 0.167\n",
      "[epoch:  0] Test KL loss: 0.223                    RMSE 0.309                     0.168 0.236 0.037 0.103\n",
      "\t\t\t\t\t\t\tR2 score: 0.103 0.103 0.006 -0.057 \n",
      "[epoch: 10] Train KL loss: 0.105                 RMSE 0.185                 0.109 0.130 0.053 0.053\n",
      "[epoch: 10] Test KL loss: 0.183                    RMSE 0.272                     0.153 0.201 0.042 0.091\n",
      "\t\t\t\t\t\t\tR2 score: 0.252 0.345 -0.260 0.181 \n",
      "[epoch: 20] Train KL loss: 0.090                 RMSE 0.167                 0.095 0.116 0.051 0.050\n",
      "[epoch: 20] Test KL loss: 0.176                    RMSE 0.262                     0.152 0.191 0.041 0.088\n",
      "\t\t\t\t\t\t\tR2 score: 0.262 0.414 -0.213 0.238 \n",
      "[epoch: 30] Train KL loss: 0.087                 RMSE 0.163                 0.093 0.114 0.050 0.048\n",
      "[epoch: 30] Test KL loss: 0.196                    RMSE 0.272                     0.156 0.198 0.049 0.092\n",
      "\t\t\t\t\t\t\tR2 score: 0.221 0.370 -0.721 0.166 \n",
      "[epoch: 40] Train KL loss: 0.087                 RMSE 0.163                 0.093 0.114 0.050 0.049\n",
      "[epoch: 40] Test KL loss: 0.164                    RMSE 0.255                     0.151 0.181 0.040 0.088\n",
      "\t\t\t\t\t\t\tR2 score: 0.273 0.472 -0.172 0.241 \n",
      "[epoch: 50] Train KL loss: 0.088                 RMSE 0.160                 0.093 0.108 0.053 0.051\n",
      "[epoch: 50] Test KL loss: 0.178                    RMSE 0.264                     0.156 0.187 0.038 0.096\n",
      "\t\t\t\t\t\t\tR2 score: 0.230 0.437 -0.062 0.092 \n",
      "[epoch: 60] Train KL loss: 0.080                 RMSE 0.150                 0.087 0.103 0.050 0.045\n",
      "[epoch: 60] Test KL loss: 0.170                    RMSE 0.261                     0.152 0.189 0.042 0.087\n",
      "\t\t\t\t\t\t\tR2 score: 0.268 0.422 -0.299 0.250 \n",
      "[epoch: 70] Train KL loss: 0.073                 RMSE 0.141                 0.081 0.097 0.048 0.042\n",
      "[epoch: 70] Test KL loss: 0.188                    RMSE 0.273                     0.157 0.197 0.048 0.093\n",
      "\t\t\t\t\t\t\tR2 score: 0.219 0.375 -0.686 0.136 \n",
      "[epoch: 80] Train KL loss: 0.072                 RMSE 0.139                 0.080 0.094 0.049 0.041\n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.550                 RMSE 0.512                 0.197 0.420 0.145 0.161\n",
      "[epoch:  0] Test KL loss: 0.253                    RMSE 0.331                     0.169 0.255 0.085 0.095\n",
      "\t\t\t\t\t\t\tR2 score: 0.089 -0.046 -4.287 0.099 \n",
      "[epoch: 10] Train KL loss: 0.104                 RMSE 0.184                 0.105 0.130 0.054 0.054\n",
      "[epoch: 10] Test KL loss: 0.180                    RMSE 0.269                     0.151 0.198 0.039 0.092\n",
      "\t\t\t\t\t\t\tR2 score: 0.271 0.366 -0.112 0.157 \n",
      "[epoch: 20] Train KL loss: 0.091                 RMSE 0.167                 0.096 0.117 0.052 0.049\n",
      "[epoch: 20] Test KL loss: 0.179                    RMSE 0.271                     0.156 0.199 0.037 0.089\n",
      "\t\t\t\t\t\t\tR2 score: 0.223 0.361 -0.019 0.213 \n",
      "[epoch: 30] Train KL loss: 0.088                 RMSE 0.163                 0.094 0.113 0.051 0.048\n",
      "[epoch: 30] Test KL loss: 0.196                    RMSE 0.282                     0.164 0.207 0.039 0.089\n",
      "\t\t\t\t\t\t\tR2 score: 0.138 0.306 -0.087 0.210 \n",
      "[epoch: 40] Train KL loss: 0.095                 RMSE 0.172                 0.098 0.122 0.050 0.049\n",
      "[epoch: 40] Test KL loss: 0.207                    RMSE 0.289                     0.164 0.216 0.038 0.094\n",
      "\t\t\t\t\t\t\tR2 score: 0.143 0.250 -0.067 0.117 \n",
      "[epoch: 50] Train KL loss: 0.080                 RMSE 0.155                 0.090 0.106 0.050 0.046\n",
      "[epoch: 50] Test KL loss: 0.169                    RMSE 0.261                     0.158 0.185 0.040 0.086\n",
      "\t\t\t\t\t\t\tR2 score: 0.209 0.449 -0.158 0.275 \n",
      "[epoch: 60] Train KL loss: 0.077                 RMSE 0.148                 0.084 0.102 0.050 0.045\n",
      "[epoch: 60] Test KL loss: 0.173                    RMSE 0.260                     0.152 0.188 0.040 0.089\n",
      "\t\t\t\t\t\t\tR2 score: 0.264 0.432 -0.159 0.217 \n",
      "[epoch: 70] Train KL loss: 0.081                 RMSE 0.153                 0.088 0.105 0.049 0.046\n",
      "Diverging. stop.\n",
      "[epoch:  0] Train KL loss: 0.534                 RMSE 0.517                 0.218 0.425 0.116 0.161\n",
      "[epoch:  0] Test KL loss: 0.216                    RMSE 0.311                     0.172 0.237 0.037 0.098\n",
      "\t\t\t\t\t\t\tR2 score: 0.058 0.097 0.027 0.051 \n",
      "[epoch: 10] Train KL loss: 0.097                 RMSE 0.176                 0.103 0.123 0.052 0.049\n",
      "[epoch: 10] Test KL loss: 0.181                    RMSE 0.275                     0.158 0.204 0.037 0.088\n",
      "\t\t\t\t\t\t\tR2 score: 0.206 0.327 0.005 0.233 \n",
      "[epoch: 20] Train KL loss: 0.089                 RMSE 0.163                 0.093 0.114 0.053 0.048\n",
      "[epoch: 20] Test KL loss: 0.177                    RMSE 0.267                     0.153 0.195 0.038 0.089\n",
      "\t\t\t\t\t\t\tR2 score: 0.250 0.384 -0.079 0.217 \n",
      "[epoch: 30] Train KL loss: 0.081                 RMSE 0.154                 0.089 0.105 0.050 0.046\n",
      "[epoch: 30] Test KL loss: 0.178                    RMSE 0.268                     0.154 0.197 0.040 0.088\n",
      "\t\t\t\t\t\t\tR2 score: 0.240 0.373 -0.193 0.238 \n",
      "[epoch: 40] Train KL loss: 0.088                 RMSE 0.160                 0.092 0.112 0.050 0.047\n",
      "[epoch: 40] Test KL loss: 0.152                    RMSE 0.247                     0.143 0.178 0.043 0.082\n",
      "\t\t\t\t\t\t\tR2 score: 0.349 0.488 -0.343 0.333 \n",
      "[epoch: 50] Train KL loss: 0.078                 RMSE 0.147                 0.085 0.100 0.050 0.043\n",
      "[epoch: 50] Test KL loss: 0.171                    RMSE 0.263                     0.146 0.195 0.040 0.091\n",
      "\t\t\t\t\t\t\tR2 score: 0.322 0.388 -0.164 0.175 \n",
      "[epoch: 60] Train KL loss: 0.074                 RMSE 0.143                 0.083 0.097 0.049 0.043\n",
      "[epoch: 60] Test KL loss: 0.161                    RMSE 0.254                     0.146 0.185 0.042 0.086\n",
      "\t\t\t\t\t\t\tR2 score: 0.326 0.447 -0.266 0.259 \n",
      "Diverging. stop.\n"
     ]
    }
   ],
   "source": [
    "wd_list = [0.00005,0.0001,0.0005,0.001,0.005,0.01]\n",
    "lr_list = [0.005, 0.01, 0.02]\n",
    "\n",
    "for (lr, wd) in itertools.product(lr_list, wd_list):\n",
    "    \n",
    "    # model setup\n",
    "    model = mnl.MNL2(n_alts=4, dim_embed=2048*output_dim*output_dim, dim_demo=len(demo_variables))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    \n",
    "    # model training\n",
    "    ref1 = 0\n",
    "    ref2 = 0\n",
    "\n",
    "    for epoch in range(500):\n",
    "\n",
    "        kl_ = 0\n",
    "        mse_ = 0\n",
    "        mse1_ = 0\n",
    "        mse2_ = 0\n",
    "        mse3_ = 0\n",
    "        mse4_ = 0\n",
    "\n",
    "        for batch, (x_batch, y_batch) in enumerate(trainloader):\n",
    "            # Compute prediction and loss\n",
    "            util = model(x_batch)\n",
    "            probs = torch.log(nn.functional.softmax(util, dim=1))\n",
    "            kl = kldivloss(probs, y_batch)\n",
    "    #         kl = kldivloss(torch.log(util), y_batch)\n",
    "            kl_ += kl.item()\n",
    "\n",
    "            mse = mseloss(torch.exp(probs), y_batch)\n",
    "    #         mse = mseloss(util, y_batch)\n",
    "            mse_ += mse.sum().item()\n",
    "            mse1_ += mse[:,0].sum().item()\n",
    "            mse2_ += mse[:,1].sum().item()\n",
    "            mse3_ += mse[:,2].sum().item()\n",
    "            mse4_ += mse[:,3].sum().item()\n",
    "            mse = mse.sum()\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            kl.backward()\n",
    "            optimizer.step()\n",
    "           \n",
    "\n",
    "        train_kl = kl_/len(trainset)\n",
    "        train_mse = np.sqrt(mse_/len(trainset))\n",
    "        train_mse1 = np.sqrt(mse1_/len(trainset))\n",
    "        train_mse2 = np.sqrt(mse2_/len(trainset))\n",
    "        train_mse3 = np.sqrt(mse3_/len(trainset))\n",
    "        train_mse4 = np.sqrt(mse4_/len(trainset))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[epoch: {epoch:>2d}] Train KL loss: {train_kl:.3f} \\\n",
    "                RMSE {train_mse:.3f} \\\n",
    "                {train_mse1:.3f} {train_mse2:.3f} {train_mse3:.3f} {train_mse4:.3f}\")\n",
    "        loss_ = train_kl\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            if epoch > 50:\n",
    "                if (np.abs(loss_ - ref1)/ref1<ref1*0.01) & (np.abs(loss_ - ref2)/ref2<ref2*0.01):\n",
    "                    print(\"Early stopping at epoch\", epoch)\n",
    "                    break\n",
    "                if (ref1 < loss_) & (ref1 < ref2):\n",
    "                    print(\"Diverging. stop.\")\n",
    "                    break\n",
    "                if loss_ < best:\n",
    "                    best = loss_\n",
    "                    best_epoch = epoch\n",
    "            else:\n",
    "                best = loss_\n",
    "                best_epoch = epoch\n",
    "\n",
    "            ref2 = ref1\n",
    "            ref1 = loss_\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "\n",
    "            kl_ = 0\n",
    "            mse_ = 0 \n",
    "            mse1_ = 0\n",
    "            mse2_ = 0\n",
    "            mse3_ = 0\n",
    "            mse4_ = 0\n",
    "\n",
    "            for batch, (x_batch, y_batch) in enumerate(testloader):\n",
    "                util = model(x_batch)\n",
    "                probs = torch.log(nn.functional.softmax(util,dim=1))\n",
    "                kl = kldivloss(probs, y_batch)\n",
    "        #         kl = kldivloss(torch.log(util), y_batch)\n",
    "                kl_ += kl.item()\n",
    "\n",
    "                mse = mseloss(torch.exp(probs), y_batch)\n",
    "        #         mse = mseloss(util, y_batch)\n",
    "                mse_ += mse.sum().item()\n",
    "                mse1_ += mse[:,0].sum().item()\n",
    "                mse2_ += mse[:,1].sum().item()\n",
    "                mse3_ += mse[:,2].sum().item()\n",
    "                mse4_ += mse[:,3].sum().item()\n",
    "\n",
    "            test_kl = kl_/len(testset)\n",
    "            test_mse = np.sqrt(mse_/len(testset))\n",
    "            test_mse1 = np.sqrt(mse1_/len(testset))\n",
    "            test_mse2 = np.sqrt(mse2_/len(testset))\n",
    "            test_mse3 = np.sqrt(mse3_/len(testset))\n",
    "            test_mse4 = np.sqrt(mse4_/len(testset))\n",
    "\n",
    "            r1 = r2_score(y_batch.numpy()[:,0],torch.exp(probs).detach().numpy()[:,0])\n",
    "            r2 = r2_score(y_batch.numpy()[:,1],torch.exp(probs).detach().numpy()[:,1])\n",
    "            r3 = r2_score(y_batch.numpy()[:,2],torch.exp(probs).detach().numpy()[:,2])\n",
    "            r4 = r2_score(y_batch.numpy()[:,3],torch.exp(probs).detach().numpy()[:,3])\n",
    "\n",
    "            print(f\"[epoch: {epoch:>2d}] Test KL loss: {kl_/len(testset):.3f}\\\n",
    "                    RMSE {np.sqrt(mse_/len(testset)):.3f} \\\n",
    "                    {np.sqrt(mse1_/len(testset)):.3f} {np.sqrt(mse2_/len(testset)):.3f} {np.sqrt(mse3_/len(testset)):.3f} {np.sqrt(mse4_/len(testset)):.3f}\")\n",
    "            print(f\"\\t\\t\\t\\t\\t\\t\\tR2 score: {r1:.3f} {r2:.3f} {r3:.3f} {r4:.3f} \")\n",
    "\n",
    "\n",
    "    with open(out_dir+model_code+\"_mode_choice.csv\", \"a\") as f:\n",
    "        f.write(\"%s,%s,%s,%s,%.4f,%.5f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f\\n\" % \\\n",
    "            (model_run_date, model_type, zoomlevel, \"MNL\", lr, wd, \n",
    "              train_kl, train_mse, train_mse1, train_mse2, train_mse3, train_mse4,\n",
    "              test_kl, test_mse, test_mse1, test_mse2, test_mse3, test_mse4,\n",
    "              r1, r2, r3, r4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
    "for i in range(4):\n",
    "    ax.scatter(y_batch.detach().numpy()[:,i], torch.exp(probs).detach().numpy()[:,i], s=10)\n",
    "\n",
    "ax.set_xlabel(\"True Probs\")\n",
    "ax.set_ylabel(\"Predicted Probs\")\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_ylim([0,1]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR for trip generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "trpgen_train =  y[~train_test_index,1]\n",
    "trpgen_test =  y[train_test_index,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader and model definition\n",
    "\n",
    "trainset2 = SurveyDataset(torch.tensor(x_train,  dtype=torch.float), torch.tensor(trpgen_train, dtype=torch.float))\n",
    "trainloader2 = DataLoader(trainset2, batch_size=256, shuffle=True)\n",
    "\n",
    "testset2 = SurveyDataset(torch.tensor(x_test, dtype=torch.float), torch.tensor(trpgen_test, dtype=torch.float))\n",
    "testloader2 = DataLoader(testset2, batch_size=len(testset), shuffle=True)\n",
    "\n",
    "mseloss = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:  0] Train RMSE : 0.029 R2 score: -1.882 \n",
      "[epoch:  0] Test RMSE 0.036 R2 score: -0.665 \n",
      "[epoch: 10] Train RMSE : 0.010 R2 score: 0.614 \n",
      "[epoch: 10] Test RMSE 0.021 R2 score: 0.444 \n",
      "[epoch: 20] Train RMSE : 0.009 R2 score: 0.716 \n",
      "[epoch: 20] Test RMSE 0.022 R2 score: 0.409 \n",
      "[epoch: 30] Train RMSE : 0.008 R2 score: 0.788 \n",
      "[epoch: 30] Test RMSE 0.022 R2 score: 0.404 \n",
      "[epoch: 40] Train RMSE : 0.008 R2 score: 0.730 \n",
      "[epoch: 40] Test RMSE 0.022 R2 score: 0.369 \n",
      "[epoch: 50] Train RMSE : 0.007 R2 score: 0.807 \n",
      "[epoch: 50] Test RMSE 0.022 R2 score: 0.401 \n",
      "Early stopping at epoch 55\n",
      "[epoch:  0] Train RMSE : 0.048 R2 score: -4.500 \n",
      "[epoch:  0] Test RMSE 0.028 R2 score: -0.013 \n",
      "[epoch: 10] Train RMSE : 0.010 R2 score: 0.629 \n",
      "[epoch: 10] Test RMSE 0.021 R2 score: 0.432 \n",
      "[epoch: 20] Train RMSE : 0.008 R2 score: 0.814 \n",
      "[epoch: 20] Test RMSE 0.021 R2 score: 0.450 \n",
      "[epoch: 30] Train RMSE : 0.008 R2 score: 0.768 \n",
      "[epoch: 30] Test RMSE 0.021 R2 score: 0.453 \n",
      "[epoch: 40] Train RMSE : 0.008 R2 score: 0.784 \n",
      "[epoch: 40] Test RMSE 0.021 R2 score: 0.422 \n",
      "[epoch: 50] Train RMSE : 0.008 R2 score: 0.816 \n",
      "[epoch: 50] Test RMSE 0.021 R2 score: 0.424 \n",
      "Early stopping at epoch 55\n",
      "[epoch:  0] Train RMSE : 0.035 R2 score: -2.345 \n",
      "[epoch:  0] Test RMSE 0.038 R2 score: -0.834 \n",
      "[epoch: 10] Train RMSE : 0.010 R2 score: 0.675 \n",
      "[epoch: 10] Test RMSE 0.020 R2 score: 0.492 \n",
      "[epoch: 20] Train RMSE : 0.009 R2 score: 0.702 \n",
      "[epoch: 20] Test RMSE 0.021 R2 score: 0.462 \n",
      "[epoch: 30] Train RMSE : 0.008 R2 score: 0.745 \n",
      "[epoch: 30] Test RMSE 0.021 R2 score: 0.436 \n",
      "[epoch: 40] Train RMSE : 0.008 R2 score: 0.751 \n",
      "[epoch: 40] Test RMSE 0.022 R2 score: 0.368 \n",
      "[epoch: 50] Train RMSE : 0.008 R2 score: 0.795 \n",
      "[epoch: 50] Test RMSE 0.021 R2 score: 0.430 \n",
      "Early stopping at epoch 55\n",
      "[epoch:  0] Train RMSE : 0.032 R2 score: -0.202 \n",
      "[epoch:  0] Test RMSE 0.047 R2 score: -1.854 \n",
      "[epoch: 10] Train RMSE : 0.009 R2 score: 0.658 \n",
      "[epoch: 10] Test RMSE 0.021 R2 score: 0.464 \n",
      "[epoch: 20] Train RMSE : 0.009 R2 score: 0.708 \n",
      "[epoch: 20] Test RMSE 0.022 R2 score: 0.411 \n",
      "[epoch: 30] Train RMSE : 0.008 R2 score: 0.755 \n",
      "[epoch: 30] Test RMSE 0.020 R2 score: 0.471 \n",
      "[epoch: 40] Train RMSE : 0.008 R2 score: 0.694 \n",
      "[epoch: 40] Test RMSE 0.020 R2 score: 0.474 \n",
      "[epoch: 50] Train RMSE : 0.008 R2 score: 0.638 \n",
      "[epoch: 50] Test RMSE 0.021 R2 score: 0.455 \n",
      "Early stopping at epoch 55\n",
      "[epoch:  0] Train RMSE : 0.029 R2 score: 0.058 \n",
      "[epoch:  0] Test RMSE 0.043 R2 score: -1.316 \n",
      "[epoch: 10] Train RMSE : 0.010 R2 score: 0.551 \n",
      "[epoch: 10] Test RMSE 0.020 R2 score: 0.493 \n",
      "[epoch: 20] Train RMSE : 0.010 R2 score: 0.658 \n",
      "[epoch: 20] Test RMSE 0.020 R2 score: 0.510 \n",
      "[epoch: 30] Train RMSE : 0.010 R2 score: 0.613 \n",
      "[epoch: 30] Test RMSE 0.020 R2 score: 0.497 \n",
      "[epoch: 40] Train RMSE : 0.010 R2 score: 0.621 \n",
      "[epoch: 40] Test RMSE 0.020 R2 score: 0.468 \n",
      "[epoch: 50] Train RMSE : 0.010 R2 score: 0.662 \n",
      "[epoch: 50] Test RMSE 0.021 R2 score: 0.456 \n",
      "Early stopping at epoch 55\n",
      "[epoch:  0] Train RMSE : 0.035 R2 score: -0.063 \n",
      "[epoch:  0] Test RMSE 0.048 R2 score: -1.916 \n",
      "[epoch: 10] Train RMSE : 0.010 R2 score: 0.613 \n",
      "[epoch: 10] Test RMSE 0.021 R2 score: 0.435 \n",
      "[epoch: 20] Train RMSE : 0.010 R2 score: 0.585 \n",
      "[epoch: 20] Test RMSE 0.024 R2 score: 0.293 \n",
      "[epoch: 30] Train RMSE : 0.009 R2 score: 0.772 \n",
      "[epoch: 30] Test RMSE 0.022 R2 score: 0.406 \n",
      "[epoch: 40] Train RMSE : 0.009 R2 score: 0.649 \n",
      "[epoch: 40] Test RMSE 0.022 R2 score: 0.392 \n",
      "[epoch: 50] Train RMSE : 0.010 R2 score: 0.606 \n",
      "[epoch: 50] Test RMSE 0.022 R2 score: 0.370 \n",
      "Early stopping at epoch 55\n",
      "[epoch:  0] Train RMSE : 0.054 R2 score: -8.949 \n",
      "[epoch:  0] Test RMSE 0.087 R2 score: -8.719 \n",
      "[epoch: 10] Train RMSE : 0.014 R2 score: -0.049 \n",
      "[epoch: 10] Test RMSE 0.026 R2 score: 0.155 \n",
      "[epoch: 20] Train RMSE : 0.009 R2 score: 0.619 \n",
      "[epoch: 20] Test RMSE 0.023 R2 score: 0.348 \n",
      "[epoch: 30] Train RMSE : 0.008 R2 score: 0.721 \n",
      "[epoch: 30] Test RMSE 0.022 R2 score: 0.374 \n",
      "[epoch: 40] Train RMSE : 0.008 R2 score: 0.663 \n",
      "[epoch: 40] Test RMSE 0.024 R2 score: 0.266 \n",
      "[epoch: 50] Train RMSE : 0.007 R2 score: 0.833 \n",
      "[epoch: 50] Test RMSE 0.021 R2 score: 0.417 \n",
      "Early stopping at epoch 55\n",
      "[epoch:  0] Train RMSE : 0.032 R2 score: -1.243 \n",
      "[epoch:  0] Test RMSE 0.022 R2 score: 0.358 \n",
      "[epoch: 10] Train RMSE : 0.009 R2 score: 0.666 \n",
      "[epoch: 10] Test RMSE 0.020 R2 score: 0.490 \n",
      "[epoch: 20] Train RMSE : 0.008 R2 score: 0.738 \n",
      "[epoch: 20] Test RMSE 0.021 R2 score: 0.434 \n",
      "[epoch: 30] Train RMSE : 0.008 R2 score: 0.760 \n",
      "[epoch: 30] Test RMSE 0.021 R2 score: 0.428 \n",
      "[epoch: 40] Train RMSE : 0.009 R2 score: 0.772 \n",
      "[epoch: 40] Test RMSE 0.023 R2 score: 0.345 \n",
      "[epoch: 50] Train RMSE : 0.008 R2 score: 0.758 \n",
      "[epoch: 50] Test RMSE 0.022 R2 score: 0.399 \n",
      "Early stopping at epoch 55\n",
      "[epoch:  0] Train RMSE : 0.034 R2 score: -0.172 \n",
      "[epoch:  0] Test RMSE 0.030 R2 score: -0.144 \n",
      "[epoch: 10] Train RMSE : 0.010 R2 score: 0.693 \n",
      "[epoch: 10] Test RMSE 0.022 R2 score: 0.364 \n",
      "[epoch: 20] Train RMSE : 0.009 R2 score: 0.747 \n",
      "[epoch: 20] Test RMSE 0.022 R2 score: 0.403 \n",
      "[epoch: 30] Train RMSE : 0.008 R2 score: 0.771 \n",
      "[epoch: 30] Test RMSE 0.023 R2 score: 0.315 \n",
      "[epoch: 40] Train RMSE : 0.009 R2 score: 0.659 \n",
      "[epoch: 40] Test RMSE 0.021 R2 score: 0.421 \n",
      "[epoch: 50] Train RMSE : 0.008 R2 score: 0.787 \n",
      "[epoch: 50] Test RMSE 0.023 R2 score: 0.347 \n",
      "Early stopping at epoch 55\n",
      "[epoch:  0] Train RMSE : 0.048 R2 score: -11.269 \n",
      "[epoch:  0] Test RMSE 0.038 R2 score: -0.841 \n",
      "[epoch: 10] Train RMSE : 0.009 R2 score: 0.685 \n",
      "[epoch: 10] Test RMSE 0.021 R2 score: 0.412 \n",
      "[epoch: 20] Train RMSE : 0.010 R2 score: 0.486 \n",
      "[epoch: 20] Test RMSE 0.021 R2 score: 0.430 \n",
      "[epoch: 30] Train RMSE : 0.008 R2 score: 0.749 \n",
      "[epoch: 30] Test RMSE 0.021 R2 score: 0.418 \n",
      "[epoch: 40] Train RMSE : 0.009 R2 score: 0.557 \n",
      "[epoch: 40] Test RMSE 0.022 R2 score: 0.358 \n",
      "[epoch: 50] Train RMSE : 0.009 R2 score: 0.678 \n",
      "[epoch: 50] Test RMSE 0.021 R2 score: 0.436 \n",
      "Early stopping at epoch 55\n",
      "[epoch:  0] Train RMSE : 0.041 R2 score: -3.365 \n",
      "[epoch:  0] Test RMSE 0.034 R2 score: -0.446 \n",
      "[epoch: 10] Train RMSE : 0.011 R2 score: 0.656 \n",
      "[epoch: 10] Test RMSE 0.024 R2 score: 0.248 \n",
      "[epoch: 20] Train RMSE : 0.009 R2 score: 0.699 \n",
      "[epoch: 20] Test RMSE 0.020 R2 score: 0.486 \n",
      "[epoch: 30] Train RMSE : 0.009 R2 score: 0.643 \n",
      "[epoch: 30] Test RMSE 0.020 R2 score: 0.471 \n",
      "[epoch: 40] Train RMSE : 0.009 R2 score: 0.697 \n",
      "[epoch: 40] Test RMSE 0.020 R2 score: 0.469 \n",
      "[epoch: 50] Train RMSE : 0.010 R2 score: 0.540 \n",
      "[epoch: 50] Test RMSE 0.021 R2 score: 0.457 \n",
      "Early stopping at epoch 55\n",
      "[epoch:  0] Train RMSE : 0.041 R2 score: -1.152 \n",
      "[epoch:  0] Test RMSE 0.039 R2 score: -0.942 \n",
      "[epoch: 10] Train RMSE : 0.010 R2 score: 0.674 \n",
      "[epoch: 10] Test RMSE 0.022 R2 score: 0.359 \n",
      "[epoch: 20] Train RMSE : 0.010 R2 score: 0.610 \n",
      "[epoch: 20] Test RMSE 0.021 R2 score: 0.439 \n",
      "[epoch: 30] Train RMSE : 0.010 R2 score: 0.626 \n",
      "[epoch: 30] Test RMSE 0.021 R2 score: 0.446 \n",
      "[epoch: 40] Train RMSE : 0.011 R2 score: 0.561 \n",
      "[epoch: 40] Test RMSE 0.022 R2 score: 0.399 \n",
      "[epoch: 50] Train RMSE : 0.010 R2 score: 0.634 \n",
      "[epoch: 50] Test RMSE 0.023 R2 score: 0.347 \n",
      "Early stopping at epoch 55\n",
      "[epoch:  0] Train RMSE : 0.057 R2 score: -9.890 \n",
      "[epoch:  0] Test RMSE 0.087 R2 score: -8.719 \n",
      "[epoch: 10] Train RMSE : 0.011 R2 score: 0.685 \n",
      "[epoch: 10] Test RMSE 0.025 R2 score: 0.197 \n",
      "[epoch: 20] Train RMSE : 0.009 R2 score: 0.614 \n",
      "[epoch: 20] Test RMSE 0.022 R2 score: 0.358 \n",
      "[epoch: 30] Train RMSE : 0.008 R2 score: 0.680 \n",
      "[epoch: 30] Test RMSE 0.022 R2 score: 0.397 \n",
      "[epoch: 40] Train RMSE : 0.008 R2 score: 0.768 \n",
      "[epoch: 40] Test RMSE 0.021 R2 score: 0.427 \n",
      "[epoch: 50] Train RMSE : 0.008 R2 score: 0.802 \n",
      "[epoch: 50] Test RMSE 0.020 R2 score: 0.471 \n",
      "Early stopping at epoch 55\n",
      "[epoch:  0] Train RMSE : 0.067 R2 score: -7.198 \n",
      "[epoch:  0] Test RMSE 0.087 R2 score: -8.683 \n",
      "[epoch: 10] Train RMSE : 0.011 R2 score: 0.520 \n",
      "[epoch: 10] Test RMSE 0.020 R2 score: 0.468 \n",
      "[epoch: 20] Train RMSE : 0.008 R2 score: 0.768 \n",
      "[epoch: 20] Test RMSE 0.020 R2 score: 0.482 \n",
      "[epoch: 30] Train RMSE : 0.008 R2 score: 0.783 \n",
      "[epoch: 30] Test RMSE 0.020 R2 score: 0.497 \n",
      "[epoch: 40] Train RMSE : 0.008 R2 score: 0.784 \n",
      "[epoch: 40] Test RMSE 0.019 R2 score: 0.542 \n",
      "[epoch: 50] Train RMSE : 0.008 R2 score: 0.720 \n",
      "[epoch: 50] Test RMSE 0.020 R2 score: 0.505 \n",
      "Early stopping at epoch 55\n",
      "[epoch:  0] Train RMSE : 0.059 R2 score: -3.410 \n",
      "[epoch:  0] Test RMSE 0.063 R2 score: -4.137 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 10] Train RMSE : 0.009 R2 score: 0.690 \n",
      "[epoch: 10] Test RMSE 0.020 R2 score: 0.468 \n",
      "[epoch: 20] Train RMSE : 0.008 R2 score: 0.719 \n",
      "[epoch: 20] Test RMSE 0.020 R2 score: 0.475 \n",
      "[epoch: 30] Train RMSE : 0.010 R2 score: 0.650 \n",
      "[epoch: 30] Test RMSE 0.021 R2 score: 0.460 \n",
      "[epoch: 40] Train RMSE : 0.008 R2 score: 0.782 \n",
      "[epoch: 40] Test RMSE 0.018 R2 score: 0.572 \n",
      "[epoch: 50] Train RMSE : 0.008 R2 score: 0.847 \n",
      "[epoch: 50] Test RMSE 0.021 R2 score: 0.445 \n",
      "Early stopping at epoch 55\n",
      "[epoch:  0] Train RMSE : 0.057 R2 score: -8.721 \n",
      "[epoch:  0] Test RMSE 0.086 R2 score: -8.409 \n",
      "[epoch: 10] Train RMSE : 0.010 R2 score: 0.552 \n",
      "[epoch: 10] Test RMSE 0.023 R2 score: 0.348 \n",
      "[epoch: 20] Train RMSE : 0.010 R2 score: 0.681 \n",
      "[epoch: 20] Test RMSE 0.024 R2 score: 0.259 \n",
      "[epoch: 30] Train RMSE : 0.009 R2 score: 0.779 \n",
      "[epoch: 30] Test RMSE 0.020 R2 score: 0.490 \n",
      "[epoch: 40] Train RMSE : 0.009 R2 score: 0.747 \n",
      "[epoch: 40] Test RMSE 0.020 R2 score: 0.498 \n",
      "[epoch: 50] Train RMSE : 0.008 R2 score: 0.780 \n",
      "[epoch: 50] Test RMSE 0.019 R2 score: 0.556 \n",
      "Early stopping at epoch 55\n",
      "[epoch:  0] Train RMSE : 0.058 R2 score: -5.628 \n",
      "[epoch:  0] Test RMSE 0.071 R2 score: -5.465 \n",
      "[epoch: 10] Train RMSE : 0.010 R2 score: 0.616 \n",
      "[epoch: 10] Test RMSE 0.020 R2 score: 0.504 \n",
      "[epoch: 20] Train RMSE : 0.011 R2 score: 0.657 \n",
      "[epoch: 20] Test RMSE 0.020 R2 score: 0.492 \n",
      "[epoch: 30] Train RMSE : 0.010 R2 score: 0.610 \n",
      "[epoch: 30] Test RMSE 0.020 R2 score: 0.486 \n",
      "[epoch: 40] Train RMSE : 0.010 R2 score: 0.578 \n",
      "[epoch: 40] Test RMSE 0.019 R2 score: 0.536 \n",
      "[epoch: 50] Train RMSE : 0.009 R2 score: 0.659 \n",
      "[epoch: 50] Test RMSE 0.019 R2 score: 0.520 \n",
      "Early stopping at epoch 55\n",
      "[epoch:  0] Train RMSE : 0.059 R2 score: -0.964 \n",
      "[epoch:  0] Test RMSE 0.061 R2 score: -3.715 \n",
      "[epoch: 10] Train RMSE : 0.011 R2 score: 0.439 \n",
      "[epoch: 10] Test RMSE 0.021 R2 score: 0.421 \n",
      "[epoch: 20] Train RMSE : 0.010 R2 score: 0.623 \n",
      "[epoch: 20] Test RMSE 0.019 R2 score: 0.521 \n",
      "[epoch: 30] Train RMSE : 0.010 R2 score: 0.602 \n",
      "[epoch: 30] Test RMSE 0.019 R2 score: 0.530 \n",
      "[epoch: 40] Train RMSE : 0.012 R2 score: 0.440 \n",
      "[epoch: 40] Test RMSE 0.021 R2 score: 0.432 \n",
      "[epoch: 50] Train RMSE : 0.010 R2 score: 0.596 \n",
      "[epoch: 50] Test RMSE 0.019 R2 score: 0.537 \n",
      "Early stopping at epoch 55\n"
     ]
    }
   ],
   "source": [
    "wd_list = [0.00005,0.0001,0.0005,0.001,0.005,0.01]\n",
    "lr_list = [0.005, 0.01, 0.02]\n",
    "\n",
    "for (lr, wd) in itertools.product(lr_list, wd_list):\n",
    "    \n",
    "    # model setup\n",
    "    model = mnl.MNL2(n_alts=1, dim_embed=2048*output_dim*output_dim, dim_demo=len(demo_variables))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    \n",
    "    # model training\n",
    "    ref1 = 0\n",
    "    ref2 = 0\n",
    "\n",
    "    for epoch in range(500):\n",
    "\n",
    "        mse_ = 0\n",
    "\n",
    "        for batch, (x_batch, y_batch) in enumerate(trainloader2):\n",
    "            # Compute prediction and loss\n",
    "            pred = model(x_batch)\n",
    "            pred = F.relu(pred).squeeze()\n",
    "\n",
    "            mse = mseloss(pred, y_batch)\n",
    "            mse_ += mse.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            mse.backward()\n",
    "            optimizer.step()\n",
    "           \n",
    "\n",
    "        train_r = r2_score(y_batch.numpy(), pred.detach().numpy())\n",
    "        train_rmse = np.sqrt(mse_/len(trainset2))\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[epoch: {epoch:>2d}] Train RMSE : {train_rmse:.3f} R2 score: {train_r:.3f} \")\n",
    "        loss_ = train_mse\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            if epoch > 50:\n",
    "                if (np.abs(loss_ - ref1)/ref1<ref1*0.01) & (np.abs(loss_ - ref2)/ref2<ref2*0.01):\n",
    "                    print(\"Early stopping at epoch\", epoch)\n",
    "                    break\n",
    "                if (ref1 < loss_) & (ref1 < ref2):\n",
    "                    print(\"Diverging. stop.\")\n",
    "                    break\n",
    "                if loss_ < best:\n",
    "                    best = loss_\n",
    "                    best_epoch = epoch\n",
    "            else:\n",
    "                best = loss_\n",
    "                best_epoch = epoch\n",
    "\n",
    "            ref2 = ref1\n",
    "            ref1 = loss_\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "\n",
    "            mse_ = 0 \n",
    "\n",
    "            for batch, (x_batch, y_batch) in enumerate(testloader2):\n",
    "                pred = model(x_batch)\n",
    "                pred = F.relu(pred).squeeze()\n",
    "\n",
    "                mse = mseloss(pred, y_batch)\n",
    "                mse_ += mse.item()\n",
    "            \n",
    "            test_rmse = np.sqrt(mse_/len(testset2))\n",
    "            test_r = r2_score(y_batch.numpy(),pred.detach().numpy())\n",
    "            \n",
    "            print(f\"[epoch: {epoch:>2d}] Test RMSE {test_rmse:.3f} R2 score: {test_r:.3f} \")\n",
    "\n",
    "\n",
    "    with open(out_dir+model_code+\"_regression_trpgen.csv\", \"a\") as f:\n",
    "        f.write(\"%s,%s,%s,%s,%.4f,%.5f,%.4f,%.4f,%.4f,%.4f\\n\" % \\\n",
    "            (model_run_date, model_type, zoomlevel, \"MNL\", lr, wd, \n",
    "              train_rmse, train_r, test_rmse, test_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
