{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85568914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"models/stylegan2/\")\n",
    "\n",
    "from sgan2_enc_trainer import Trainer\n",
    "\n",
    "import os\n",
    "import fire\n",
    "import random\n",
    "from retry.api import retry_call\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "from util import NanException\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from setup import proj_dir, image_dir, out_dir, model_dir\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87101533",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = image_dir+\"zoom15/\"\n",
    "results_dir = out_dir\n",
    "models_dir = model_dir\n",
    "stylegan_name = '2303-2'\n",
    "encoder_name = '2001'\n",
    "\n",
    "# GAN\n",
    "gan_load_from = 99\n",
    "# encoder\n",
    "new = True\n",
    "enc_load_from = -1\n",
    "\n",
    "image_size = 64\n",
    "network_capacity = 16\n",
    "fmap_max = 512\n",
    "transparent = False\n",
    "batch_size = 2\n",
    "gradient_accumulate_every = 6\n",
    "num_train_steps = 150000\n",
    "learning_rate = 1e-5\n",
    "lr_mlp = 0.1\n",
    "ttur_mult = 1.5\n",
    "rel_disc_loss = False\n",
    "num_workers =  None\n",
    "save_every = 1000\n",
    "evaluate_every = 1000\n",
    "generate = False\n",
    "num_generate = 1\n",
    "generate_interpolation = False\n",
    "interpolation_num_steps = 100\n",
    "save_frames = False\n",
    "num_image_tiles = 8\n",
    "trunc_psi = 0.75\n",
    "mixed_prob = 0.9\n",
    "fp16 = False\n",
    "no_pl_reg = False\n",
    "cl_reg = False\n",
    "fq_layers = []\n",
    "fq_dict_size = 256\n",
    "attn_layers = []\n",
    "no_const = False\n",
    "aug_prob = 0.\n",
    "aug_types = ['translation', 'cutout']\n",
    "top_k_training = False\n",
    "generator_top_k_gamma = 0.99\n",
    "generator_top_k_frac = 0.5\n",
    "dual_contrast_loss = False\n",
    "dataset_aug_prob = 0.\n",
    "multi_gpus = True\n",
    "calculate_fid_every = None\n",
    "calculate_fid_num_images = 12800\n",
    "clear_fid_cache = False\n",
    "seed = 42\n",
    "log = False\n",
    "\n",
    "# A global scale to the custom losses\n",
    "kl_scaling=1\n",
    "rec_scaling=1\n",
    "\n",
    "# If unspecified, use the Discriminator as an encoder (like the authors did).\n",
    "# This is the way to go if we want to be close to the original paper.\n",
    "# Check out debug_encoders.py for the names of classes if you still want\n",
    "# to use a different encoder.\n",
    "encoder_class='GHFeat'\n",
    "\n",
    "kl_rec_during_disc=False\n",
    "\n",
    "# This is for making the image results be results of the\n",
    "# image -> encoder -> generator pipeline\n",
    "# Set False if training a standard GAN or if you want to see\n",
    "# examples from a noise vector.\n",
    "sample_from_encoder=True\n",
    "\n",
    "# Alternatively trains the model with the StylEx loss\n",
    "# and the regular StyleGAN loss. If False just trains\n",
    "# using the encoder.\n",
    "alternating_training=False\n",
    "\n",
    "tensorboard_dir=None  # Put to None for not logging\n",
    "\n",
    "rank = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd3379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(\n",
    "        stylegan_name=stylegan_name,\n",
    "        encoder_name=encoder_name,\n",
    "        results_dir=results_dir,\n",
    "        models_dir=models_dir,\n",
    "        batch_size=batch_size,\n",
    "        gradient_accumulate_every=gradient_accumulate_every,\n",
    "        image_size=image_size,\n",
    "        network_capacity=network_capacity,\n",
    "        fmap_max=fmap_max,\n",
    "        transparent=transparent,\n",
    "        lr=learning_rate,\n",
    "        lr_mlp=lr_mlp,\n",
    "        ttur_mult=ttur_mult,\n",
    "        rel_disc_loss=rel_disc_loss,\n",
    "        num_workers=num_workers,\n",
    "        save_every=save_every,\n",
    "        evaluate_every=evaluate_every,\n",
    "        num_image_tiles=num_image_tiles,\n",
    "        trunc_psi=trunc_psi,\n",
    "        fp16=fp16,\n",
    "        no_pl_reg=no_pl_reg,\n",
    "        cl_reg=cl_reg,\n",
    "        fq_layers=fq_layers,\n",
    "        fq_dict_size=fq_dict_size,\n",
    "        attn_layers=attn_layers,\n",
    "        no_const=no_const,\n",
    "        aug_prob=aug_prob,\n",
    "        aug_types=aug_types,\n",
    "        top_k_training=top_k_training,\n",
    "        generator_top_k_gamma=generator_top_k_gamma,\n",
    "        generator_top_k_frac=generator_top_k_frac,\n",
    "        dual_contrast_loss=dual_contrast_loss,\n",
    "        dataset_aug_prob=dataset_aug_prob,\n",
    "        calculate_fid_every=calculate_fid_every,\n",
    "        calculate_fid_num_images=calculate_fid_num_images,\n",
    "        clear_fid_cache=clear_fid_cache,\n",
    "        mixed_prob=mixed_prob,\n",
    "        log=log,\n",
    "        kl_scaling=kl_scaling,\n",
    "        rec_scaling=rec_scaling,\n",
    "#         classifier_path=classifier_path,\n",
    "#         num_classes=num_classes,\n",
    "        encoder_class=encoder_class,\n",
    "        sample_from_encoder=sample_from_encoder,\n",
    "        alternating_training=alternating_training,\n",
    "        kl_rec_during_disc=kl_rec_during_disc,\n",
    "        tensorboard_dir=tensorboard_dir,\n",
    "#         classifier_name=classifier_name,\n",
    "        rank=rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe155351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_training(rank, world_size, model_args, data, gan_load_from, enc_load_from, new, num_train_steps, seed):\n",
    "#     is_main = rank == 0\n",
    "    is_main = True\n",
    "    is_ddp = world_size > 1\n",
    "\n",
    "    if is_ddp:\n",
    "        set_seed(seed)\n",
    "        os.environ['MASTER_ADDR'] = 'localhost'\n",
    "        os.environ['MASTER_PORT'] = '12355'\n",
    "        dist.init_process_group('nccl', rank=rank, world_size=world_size)\n",
    "\n",
    "        print(f\"{rank + 1}/{world_size} process initialized.\")\n",
    "\n",
    "    model_args.update(\n",
    "        is_ddp = is_ddp,\n",
    "        rank = rank,\n",
    "        world_size = world_size\n",
    "    )\n",
    "\n",
    "    model = Trainer(**model_args)\n",
    "    \n",
    "    model.load('model', gan_load_from)\n",
    "    if not new:\n",
    "        model.load('enc', enc_load_from)\n",
    "    else:\n",
    "        model.clear()\n",
    "\n",
    "    model.set_data_src(data)\n",
    "    model.set_test_data_src(data, 8)\n",
    "    \n",
    "    progress_bar = tqdm(initial = model.steps, total = num_train_steps, mininterval=10., desc=f'<{data}>', position=0, leave=True)\n",
    "    while model.steps < num_train_steps:\n",
    "        retry_call(model.train_encoder_only, tries=3, exceptions=NanException)\n",
    "        progress_bar.n = model.steps\n",
    "        progress_bar.refresh()\n",
    "        if is_main and model.steps % 500 == 0:\n",
    "            model.print_log()\n",
    "\n",
    "    model.save(model.checkpoint_num)\n",
    "\n",
    "    if is_ddp:\n",
    "        dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e1d210",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results directory: /home/jtl/Dropbox (MIT)/project_image_demand/results/sGAN2/2303-2/2001\n",
      "Model directory: /dreambig/qingyi/image_chicago/models/sGAN2/2303-2\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Loading /dreambig/qingyi/image_chicago/models/sGAN2/2303-2/model_99.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "</dreambig/qingyi/image_chicago/data/images/satellite/zoom15/>:   0%|          | 500/150000 [18:06<90:13:23,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.00 | D: 0.11 | GP: 0.29 | Rec: 2.23 | Rec_w: 0.39 | Rec_i: 0.19 | Rec_pips: 0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "</dreambig/qingyi/image_chicago/data/images/satellite/zoom15/>:   1%|          | 1000/150000 [36:11<89:52:55,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.00 | D: 0.22 | GP: 0.06 | Rec: 2.09 | Rec_w: 0.36 | Rec_i: 0.20 | Rec_pips: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "</dreambig/qingyi/image_chicago/data/images/satellite/zoom15/>:   1%|          | 1500/150000 [54:17<89:34:57,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.00 | D: 0.40 | GP: 0.38 | Rec: 1.93 | Rec_w: 0.33 | Rec_i: 0.18 | Rec_pips: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "</dreambig/qingyi/image_chicago/data/images/satellite/zoom15/>:   1%|▏         | 2000/150000 [1:12:22<89:16:21,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.00 | D: 0.15 | GP: 0.22 | Rec: 2.14 | Rec_w: 0.37 | Rec_i: 0.19 | Rec_pips: 0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "</dreambig/qingyi/image_chicago/data/images/satellite/zoom15/>:   2%|▏         | 2500/150000 [1:30:29<88:59:22,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.00 | D: 0.13 | GP: 0.20 | Rec: 1.87 | Rec_w: 0.29 | Rec_i: 0.19 | Rec_pips: 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "</dreambig/qingyi/image_chicago/data/images/satellite/zoom15/>:   2%|▏         | 3000/150000 [1:48:30<88:37:13,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.00 | D: 0.22 | GP: 0.13 | Rec: 1.90 | Rec_w: 0.33 | Rec_i: 0.16 | Rec_pips: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "</dreambig/qingyi/image_chicago/data/images/satellite/zoom15/>:   2%|▏         | 3500/150000 [2:06:33<88:17:01,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.00 | D: 0.26 | GP: 0.33 | Rec: 1.50 | Rec_w: 0.22 | Rec_i: 0.17 | Rec_pips: 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "</dreambig/qingyi/image_chicago/data/images/satellite/zoom15/>:   3%|▎         | 4000/150000 [2:24:34<87:56:55,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.00 | D: 0.34 | GP: 0.16 | Rec: 1.51 | Rec_w: 0.21 | Rec_i: 0.17 | Rec_pips: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "</dreambig/qingyi/image_chicago/data/images/satellite/zoom15/>:   3%|▎         | 4500/150000 [2:42:36<87:37:53,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.00 | D: 0.25 | GP: 0.15 | Rec: 1.65 | Rec_w: 0.21 | Rec_i: 0.21 | Rec_pips: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "</dreambig/qingyi/image_chicago/data/images/satellite/zoom15/>:   3%|▎         | 5000/150000 [3:00:39<87:18:52,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.00 | D: 0.65 | GP: 0.36 | Rec: 1.26 | Rec_w: 0.14 | Rec_i: 0.19 | Rec_pips: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "</dreambig/qingyi/image_chicago/data/images/satellite/zoom15/>:   4%|▎         | 5500/150000 [3:18:41<87:00:17,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.00 | D: 0.56 | GP: 0.22 | Rec: 1.28 | Rec_w: 0.16 | Rec_i: 0.17 | Rec_pips: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "</dreambig/qingyi/image_chicago/data/images/satellite/zoom15/>:   4%|▍         | 6000/150000 [3:36:43<86:41:28,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.00 | D: 0.59 | GP: 0.12 | Rec: 1.30 | Rec_w: 0.16 | Rec_i: 0.18 | Rec_pips: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "</dreambig/qingyi/image_chicago/data/images/satellite/zoom15/>:   4%|▍         | 6500/150000 [3:54:45<86:22:52,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.00 | D: 0.54 | GP: 0.28 | Rec: 1.33 | Rec_w: 0.14 | Rec_i: 0.18 | Rec_pips: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "</dreambig/qingyi/image_chicago/data/images/satellite/zoom15/>:   5%|▍         | 7000/150000 [4:12:47<86:04:00,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 0.00 | D: 0.42 | GP: 0.17 | Rec: 1.28 | Rec_w: 0.15 | Rec_i: 0.17 | Rec_pips: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "</dreambig/qingyi/image_chicago/data/images/satellite/zoom15/>:   5%|▍         | 7356/150000 [4:25:37<85:50:44,  2.17s/it]"
     ]
    }
   ],
   "source": [
    "rank = 0\n",
    "world_size = 1\n",
    "\n",
    "encoder_training(rank, world_size, model_args, data, gan_load_from, enc_load_from, new, num_train_steps, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d296de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1da3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qingyi",
   "language": "python",
   "name": "qingyi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
