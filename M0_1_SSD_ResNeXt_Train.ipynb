{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"models/\")\n",
    "from setup import out_dir, data_dir, image_dir, model_dir\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn\n",
    "import torchvision.utils\n",
    "import torchvision.transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    from tensorboardX import SummaryWriter\n",
    "    is_tensorboard_available = True\n",
    "except Exception:\n",
    "    is_tensorboard_available = False\n",
    "\n",
    "from dataloader import image_loader, load_demo\n",
    "from autoencoder import Autoencoder\n",
    "from M0_1_util_train_test import load_model, train, test, AverageMeter\n",
    "from util_model import my_loss\n",
    "from setup import *\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='[%(asctime)s %(name)s %(levelname)s] - %(message)s',\n",
    "    datefmt='%Y/%m/%d %H:%M:%S',\n",
    "    level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoomlevel = 'zoom15'\n",
    "output_dim = 3\n",
    "model_run_date = \"2208\"\n",
    "sampling='stratified'\n",
    "normalization = 'minmax'\n",
    "data_version = '1571'\n",
    "variable_names = ['tot_population','pct25_34yrs','pct35_50yrs','pctover65yrs',\n",
    "         'pctwhite_alone','pct_nonwhite','pctblack_alone',\n",
    "         'pct_col_grad','avg_tt_to_work','inc_per_capita']\n",
    "model_save_variable_names = ['totpop','pct25-34','pct35-50','pctsenior',\n",
    "         'pctwhite_alone','pct_nonwhite','pctblack_alone',\n",
    "         'pctcolgrad','avg_tt_to_work','inc']\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_cs, demo_np = load_demo(data_dir, norm=normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = 1 \n",
    "_, lr, wd = get_hp_from_version_code(None,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "        'image_size': 224, \n",
    "        'depth': -1,\n",
    "       'base_channels':64,\n",
    "       'output_dim':output_dim,\n",
    "       'num_demo_vars':len(variable_names),\n",
    "       'demo_norm': normalization,\n",
    "       'cardinality':1,\n",
    "       'epochs':300,\n",
    "       'batch_size':16,\n",
    "       'base_lr':lr,\n",
    "       'weight_decay':wd,\n",
    "       'momentum': 0.9,\n",
    "       'nesterov': True,\n",
    "       'milestones': '[50,100]',\n",
    "       'lr_decay':0.1,\n",
    "       'seed': 1234,\n",
    "       'outdir':out_dir,\n",
    "       'num_workers':8,\n",
    "       'tensorboard':False,\n",
    "       'save':True}\n",
    "\n",
    "model_config = OrderedDict([\n",
    "    ('arch', 'resnext'),\n",
    "    ('depth', args['depth']),\n",
    "    ('base_channels', args['base_channels']),\n",
    "    ('cardinality', args['cardinality']),\n",
    "    ('input_shape', (1, 3, 32, 32)),\n",
    "    ('output_dim', args['output_dim']),\n",
    "    ('num_demo_vars', args['num_demo_vars'])\n",
    "])\n",
    "\n",
    "optim_config = OrderedDict([\n",
    "    ('epochs', args['epochs']),\n",
    "    ('batch_size', args['batch_size']),\n",
    "    ('base_lr', args['base_lr']),\n",
    "    ('weight_decay', args['weight_decay']),\n",
    "    ('momentum', args['momentum']),\n",
    "    ('nesterov', args['nesterov']),\n",
    "    ('milestones', json.loads(args['milestones'])),\n",
    "    ('lr_decay', args['lr_decay']),\n",
    "])\n",
    "\n",
    "data_config = OrderedDict([\n",
    "    ('dataset', 'CIFAR10'),\n",
    "    ('image_size', args['image_size']),\n",
    "    ('demo_norm', args['demo_norm'])\n",
    "])\n",
    "\n",
    "run_config = OrderedDict([\n",
    "#     ('weight', args['weight']),\n",
    "    ('seed', args['seed']),\n",
    "    ('outdir', args['outdir']),\n",
    "    ('save', args['save']),\n",
    "    ('num_workers', args['num_workers']),\n",
    "    ('tensorboard', args['tensorboard']),\n",
    "])\n",
    "\n",
    "config = OrderedDict([\n",
    "    ('model_config', model_config),\n",
    "    ('optim_config', optim_config),\n",
    "    ('data_config', data_config),\n",
    "    ('run_config', run_config),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse command line arguments\n",
    "#config = parse_args()\n",
    "#logger.info(json.dumps(config, indent=2))\n",
    "\n",
    "model_name = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "\n",
    "run_config = config['run_config']\n",
    "optim_config = config['optim_config']\n",
    "\n",
    "# TensorBoard SummaryWriter\n",
    "# writer = SummaryWriter(proj_dir+\"tensorboard/runs/SSD_\"+zoomlevel+\"_\"+str(model_config['output_dim']**2*2048)+\"_\"+\n",
    "#                               model_run_date) if run_config['tensorboard'] else None\n",
    "\n",
    "# set random seed\n",
    "seed = run_config['seed']\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# create output directory\n",
    "outdir = run_config['outdir']\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# save config as json file in output directory\n",
    "# outpath = os.path.join(outdir, 'config.json')\n",
    "# with open(outpath, 'w') as fout:\n",
    "#     json.dump(config, fout, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28278 images in dataset\n",
      "3142 images in dataset\n"
     ]
    }
   ],
   "source": [
    "# data loaders\n",
    "# train_loader, test_loader = get_loader(optim_config['batch_size'], run_config['num_workers'])\n",
    "train_loader, test_loader = image_loader(image_dir+zoomlevel+\"/\", data_dir, optim_config['batch_size'], \n",
    "                                         run_config['num_workers'], \n",
    "                                         data_config['image_size'], \n",
    "                                         data_version=data_version, sampling=sampling, \n",
    "                                         recalculate_normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='mean')\n",
    "# criterion = my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "config['model_config']['input_shape'] = (1,3,data_config['image_size'],data_config['image_size'])\n",
    "\n",
    "encoder = load_model(config['model_config']['arch'], 'Encoder', config['model_config'])\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "config['encoder'] = encoder\n",
    "\n",
    "model = load_model('direct_regression','Supervised_Demo', config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022/08/16 01:16:31 __main__ INFO] - n_params: 23508032\n"
     ]
    }
   ],
   "source": [
    "n_params = sum([param.view(-1).size()[0] for param in encoder.parameters()])\n",
    "logger.info('n_params: {}'.format(n_params))\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=optim_config['base_lr'],\n",
    "    momentum=optim_config['momentum'],\n",
    "    weight_decay=optim_config['weight_decay'],\n",
    "    nesterov=optim_config['nesterov'])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones=optim_config['milestones'],\n",
    "    gamma=optim_config['lr_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022/08/16 01:19:45 __main__ INFO] - Epoch 0 Step 1767/1768 Train Loss 0.03359925\n",
      "[2022/08/16 01:19:49 __main__ INFO] - Epoch 0 Test Loss 0.03239897\n",
      "[2022/08/16 01:19:49 __main__ INFO] - Elapsed 4.45\n",
      "[2022/08/16 01:21:57 __main__ INFO] - Epoch 1 Step 1767/1768 Train Loss 0.03192915\n",
      "[2022/08/16 01:22:01 __main__ INFO] - Epoch 1 Test Loss 0.03094709\n",
      "[2022/08/16 01:22:01 __main__ INFO] - Elapsed 4.41\n",
      "[2022/08/16 01:24:08 __main__ INFO] - Epoch 2 Step 1767/1768 Train Loss 0.03105995\n",
      "[2022/08/16 01:24:13 __main__ INFO] - Epoch 2 Test Loss 0.03056156\n",
      "[2022/08/16 01:24:13 __main__ INFO] - Elapsed 4.42\n",
      "[2022/08/16 01:26:20 __main__ INFO] - Epoch 3 Step 1767/1768 Train Loss 0.03047597\n",
      "[2022/08/16 01:26:24 __main__ INFO] - Epoch 3 Test Loss 0.03033651\n",
      "[2022/08/16 01:26:24 __main__ INFO] - Elapsed 4.43\n",
      "[2022/08/16 01:28:31 __main__ INFO] - Epoch 4 Step 1767/1768 Train Loss 0.02990481\n",
      "[2022/08/16 01:28:36 __main__ INFO] - Epoch 4 Test Loss 0.02988489\n",
      "[2022/08/16 01:28:36 __main__ INFO] - Elapsed 4.45\n",
      "[2022/08/16 01:30:43 __main__ INFO] - Epoch 5 Step 1767/1768 Train Loss 0.02978008\n",
      "[2022/08/16 01:30:47 __main__ INFO] - Epoch 5 Test Loss 0.02932854\n",
      "[2022/08/16 01:30:47 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 01:32:55 __main__ INFO] - Epoch 6 Step 1767/1768 Train Loss 0.02943909\n",
      "[2022/08/16 01:32:59 __main__ INFO] - Epoch 6 Test Loss 0.02857894\n",
      "[2022/08/16 01:32:59 __main__ INFO] - Elapsed 4.43\n",
      "[2022/08/16 01:35:06 __main__ INFO] - Epoch 7 Step 1767/1768 Train Loss 0.02907284\n",
      "[2022/08/16 01:35:10 __main__ INFO] - Epoch 7 Test Loss 0.02849927\n",
      "[2022/08/16 01:35:10 __main__ INFO] - Elapsed 4.45\n",
      "[2022/08/16 01:37:17 __main__ INFO] - Epoch 8 Step 1767/1768 Train Loss 0.02869708\n",
      "[2022/08/16 01:37:22 __main__ INFO] - Epoch 8 Test Loss 0.02778766\n",
      "[2022/08/16 01:37:22 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 01:39:29 __main__ INFO] - Epoch 9 Step 1767/1768 Train Loss 0.02803951\n",
      "[2022/08/16 01:39:33 __main__ INFO] - Epoch 9 Test Loss 0.02778023\n",
      "[2022/08/16 01:39:33 __main__ INFO] - Elapsed 4.40\n",
      "[2022/08/16 01:41:40 __main__ INFO] - Epoch 10 Step 1767/1768 Train Loss 0.02770362\n",
      "[2022/08/16 01:41:45 __main__ INFO] - Epoch 10 Test Loss 0.02667368\n",
      "[2022/08/16 01:41:45 __main__ INFO] - Elapsed 4.45\n",
      "[2022/08/16 01:43:52 __main__ INFO] - Epoch 11 Step 1767/1768 Train Loss 0.02739638\n",
      "[2022/08/16 01:43:56 __main__ INFO] - Epoch 11 Test Loss 0.02628859\n",
      "[2022/08/16 01:43:56 __main__ INFO] - Elapsed 4.42\n",
      "[2022/08/16 01:46:04 __main__ INFO] - Epoch 12 Step 1767/1768 Train Loss 0.02723140\n",
      "[2022/08/16 01:46:08 __main__ INFO] - Epoch 12 Test Loss 0.02595547\n",
      "[2022/08/16 01:46:08 __main__ INFO] - Elapsed 4.40\n",
      "[2022/08/16 01:48:15 __main__ INFO] - Epoch 13 Step 1767/1768 Train Loss 0.02688437\n",
      "[2022/08/16 01:48:19 __main__ INFO] - Epoch 13 Test Loss 0.02557131\n",
      "[2022/08/16 01:48:19 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 01:50:27 __main__ INFO] - Epoch 14 Step 1767/1768 Train Loss 0.02652043\n",
      "[2022/08/16 01:50:31 __main__ INFO] - Epoch 14 Test Loss 0.02589927\n",
      "[2022/08/16 01:50:31 __main__ INFO] - Elapsed 4.42\n",
      "[2022/08/16 01:52:38 __main__ INFO] - Epoch 15 Step 1767/1768 Train Loss 0.02630224\n",
      "[2022/08/16 01:52:43 __main__ INFO] - Epoch 15 Test Loss 0.02513593\n",
      "[2022/08/16 01:52:43 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 01:54:50 __main__ INFO] - Epoch 16 Step 1767/1768 Train Loss 0.02610641\n",
      "[2022/08/16 01:54:55 __main__ INFO] - Epoch 16 Test Loss 0.02467616\n",
      "[2022/08/16 01:54:55 __main__ INFO] - Elapsed 4.45\n",
      "[2022/08/16 01:57:02 __main__ INFO] - Epoch 17 Step 1767/1768 Train Loss 0.02549813\n",
      "[2022/08/16 01:57:06 __main__ INFO] - Epoch 17 Test Loss 0.02447557\n",
      "[2022/08/16 01:57:06 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 01:59:13 __main__ INFO] - Epoch 18 Step 1767/1768 Train Loss 0.02498860\n",
      "[2022/08/16 01:59:18 __main__ INFO] - Epoch 18 Test Loss 0.02340888\n",
      "[2022/08/16 01:59:18 __main__ INFO] - Elapsed 4.42\n",
      "[2022/08/16 02:01:25 __main__ INFO] - Epoch 19 Step 1767/1768 Train Loss 0.02443172\n",
      "[2022/08/16 02:01:29 __main__ INFO] - Epoch 19 Test Loss 0.02230408\n",
      "[2022/08/16 02:01:29 __main__ INFO] - Elapsed 4.47\n",
      "[2022/08/16 02:03:36 __main__ INFO] - Epoch 20 Step 1767/1768 Train Loss 0.02386640\n",
      "[2022/08/16 02:03:41 __main__ INFO] - Epoch 20 Test Loss 0.02234601\n",
      "[2022/08/16 02:03:41 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 02:05:48 __main__ INFO] - Epoch 21 Step 1767/1768 Train Loss 0.02350362\n",
      "[2022/08/16 02:05:53 __main__ INFO] - Epoch 21 Test Loss 0.02143691\n",
      "[2022/08/16 02:05:53 __main__ INFO] - Elapsed 4.40\n",
      "[2022/08/16 02:08:00 __main__ INFO] - Epoch 22 Step 1767/1768 Train Loss 0.02317090\n",
      "[2022/08/16 02:08:04 __main__ INFO] - Epoch 22 Test Loss 0.02189747\n",
      "[2022/08/16 02:08:04 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 02:10:12 __main__ INFO] - Epoch 23 Step 1767/1768 Train Loss 0.02283345\n",
      "[2022/08/16 02:10:16 __main__ INFO] - Epoch 23 Test Loss 0.02111864\n",
      "[2022/08/16 02:10:16 __main__ INFO] - Elapsed 4.42\n",
      "[2022/08/16 02:12:23 __main__ INFO] - Epoch 24 Step 1767/1768 Train Loss 0.02233034\n",
      "[2022/08/16 02:12:27 __main__ INFO] - Epoch 24 Test Loss 0.02129613\n",
      "[2022/08/16 02:12:27 __main__ INFO] - Elapsed 4.45\n",
      "[2022/08/16 02:14:35 __main__ INFO] - Epoch 25 Step 1767/1768 Train Loss 0.02183295\n",
      "[2022/08/16 02:14:39 __main__ INFO] - Epoch 25 Test Loss 0.01994584\n",
      "[2022/08/16 02:14:39 __main__ INFO] - Elapsed 4.43\n",
      "[2022/08/16 02:16:47 __main__ INFO] - Epoch 26 Step 1767/1768 Train Loss 0.02163395\n",
      "[2022/08/16 02:16:51 __main__ INFO] - Epoch 26 Test Loss 0.01986611\n",
      "[2022/08/16 02:16:51 __main__ INFO] - Elapsed 4.45\n",
      "[2022/08/16 02:18:58 __main__ INFO] - Epoch 27 Step 1767/1768 Train Loss 0.02122407\n",
      "[2022/08/16 02:19:03 __main__ INFO] - Epoch 27 Test Loss 0.02012971\n",
      "[2022/08/16 02:19:03 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 02:21:10 __main__ INFO] - Epoch 28 Step 1767/1768 Train Loss 0.02074687\n",
      "[2022/08/16 02:21:14 __main__ INFO] - Epoch 28 Test Loss 0.01948169\n",
      "[2022/08/16 02:21:14 __main__ INFO] - Elapsed 4.40\n",
      "[2022/08/16 02:23:21 __main__ INFO] - Epoch 29 Step 1767/1768 Train Loss 0.02041673\n",
      "[2022/08/16 02:23:26 __main__ INFO] - Epoch 29 Test Loss 0.01854290\n",
      "[2022/08/16 02:23:26 __main__ INFO] - Elapsed 4.41\n",
      "[2022/08/16 02:25:33 __main__ INFO] - Epoch 30 Step 1767/1768 Train Loss 0.02001343\n",
      "[2022/08/16 02:25:37 __main__ INFO] - Epoch 30 Test Loss 0.01837475\n",
      "[2022/08/16 02:25:37 __main__ INFO] - Elapsed 4.48\n",
      "[2022/08/16 02:27:45 __main__ INFO] - Epoch 31 Step 1767/1768 Train Loss 0.01975258\n",
      "[2022/08/16 02:27:49 __main__ INFO] - Epoch 31 Test Loss 0.01839729\n",
      "[2022/08/16 02:27:49 __main__ INFO] - Elapsed 4.41\n",
      "[2022/08/16 02:29:56 __main__ INFO] - Epoch 32 Step 1767/1768 Train Loss 0.01941726\n",
      "[2022/08/16 02:30:01 __main__ INFO] - Epoch 32 Test Loss 0.01839175\n",
      "[2022/08/16 02:30:01 __main__ INFO] - Elapsed 4.45\n",
      "[2022/08/16 02:32:08 __main__ INFO] - Epoch 33 Step 1767/1768 Train Loss 0.01937147\n",
      "[2022/08/16 02:32:12 __main__ INFO] - Epoch 33 Test Loss 0.01961146\n",
      "[2022/08/16 02:32:12 __main__ INFO] - Elapsed 4.48\n",
      "[2022/08/16 02:34:20 __main__ INFO] - Epoch 34 Step 1767/1768 Train Loss 0.01919660\n",
      "[2022/08/16 02:34:24 __main__ INFO] - Epoch 34 Test Loss 0.01864578\n",
      "[2022/08/16 02:34:24 __main__ INFO] - Elapsed 4.45\n",
      "[2022/08/16 02:36:31 __main__ INFO] - Epoch 35 Step 1767/1768 Train Loss 0.01895104\n",
      "[2022/08/16 02:36:36 __main__ INFO] - Epoch 35 Test Loss 0.01737960\n",
      "[2022/08/16 02:36:36 __main__ INFO] - Elapsed 4.45\n",
      "[2022/08/16 02:38:43 __main__ INFO] - Epoch 36 Step 1767/1768 Train Loss 0.01879932\n",
      "[2022/08/16 02:38:47 __main__ INFO] - Epoch 36 Test Loss 0.02048050\n",
      "[2022/08/16 02:38:47 __main__ INFO] - Elapsed 4.45\n",
      "[2022/08/16 02:40:54 __main__ INFO] - Epoch 37 Step 1767/1768 Train Loss 0.01855440\n",
      "[2022/08/16 02:40:59 __main__ INFO] - Epoch 37 Test Loss 0.01750438\n",
      "[2022/08/16 02:40:59 __main__ INFO] - Elapsed 4.41\n",
      "[2022/08/16 02:43:06 __main__ INFO] - Epoch 38 Step 1767/1768 Train Loss 0.01842582\n",
      "[2022/08/16 02:43:11 __main__ INFO] - Epoch 38 Test Loss 0.01798485\n",
      "[2022/08/16 02:43:11 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 02:45:18 __main__ INFO] - Epoch 39 Step 1767/1768 Train Loss 0.01847132\n",
      "[2022/08/16 02:45:22 __main__ INFO] - Epoch 39 Test Loss 0.01683903\n",
      "[2022/08/16 02:45:22 __main__ INFO] - Elapsed 4.41\n",
      "[2022/08/16 02:47:29 __main__ INFO] - Epoch 40 Step 1767/1768 Train Loss 0.01821120\n",
      "[2022/08/16 02:47:34 __main__ INFO] - Epoch 40 Test Loss 0.01715866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022/08/16 02:47:34 __main__ INFO] - Elapsed 4.40\n",
      "[2022/08/16 02:49:41 __main__ INFO] - Epoch 41 Step 1767/1768 Train Loss 0.01802113\n",
      "[2022/08/16 02:49:45 __main__ INFO] - Epoch 41 Test Loss 0.01689073\n",
      "[2022/08/16 02:49:45 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 02:51:52 __main__ INFO] - Epoch 42 Step 1767/1768 Train Loss 0.01788475\n",
      "[2022/08/16 02:51:57 __main__ INFO] - Epoch 42 Test Loss 0.01682600\n",
      "[2022/08/16 02:51:57 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 02:54:04 __main__ INFO] - Epoch 43 Step 1767/1768 Train Loss 0.01781463\n",
      "[2022/08/16 02:54:08 __main__ INFO] - Epoch 43 Test Loss 0.01686424\n",
      "[2022/08/16 02:54:08 __main__ INFO] - Elapsed 4.43\n",
      "[2022/08/16 02:56:15 __main__ INFO] - Epoch 44 Step 1767/1768 Train Loss 0.01767424\n",
      "[2022/08/16 02:56:20 __main__ INFO] - Epoch 44 Test Loss 0.01771826\n",
      "[2022/08/16 02:56:20 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 02:58:27 __main__ INFO] - Epoch 45 Step 1767/1768 Train Loss 0.01763776\n",
      "[2022/08/16 02:58:31 __main__ INFO] - Epoch 45 Test Loss 0.01862427\n",
      "[2022/08/16 02:58:31 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 03:00:39 __main__ INFO] - Epoch 46 Step 1767/1768 Train Loss 0.01748261\n",
      "[2022/08/16 03:00:43 __main__ INFO] - Epoch 46 Test Loss 0.01668190\n",
      "[2022/08/16 03:00:43 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 03:02:50 __main__ INFO] - Epoch 47 Step 1767/1768 Train Loss 0.01725498\n",
      "[2022/08/16 03:02:55 __main__ INFO] - Epoch 47 Test Loss 0.01592016\n",
      "[2022/08/16 03:02:55 __main__ INFO] - Elapsed 4.49\n",
      "[2022/08/16 03:05:02 __main__ INFO] - Epoch 48 Step 1767/1768 Train Loss 0.01720814\n",
      "[2022/08/16 03:05:07 __main__ INFO] - Epoch 48 Test Loss 0.01607467\n",
      "[2022/08/16 03:05:07 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 03:07:14 __main__ INFO] - Epoch 49 Step 1767/1768 Train Loss 0.01711793\n",
      "[2022/08/16 03:07:18 __main__ INFO] - Epoch 49 Test Loss 0.01645958\n",
      "[2022/08/16 03:07:18 __main__ INFO] - Elapsed 4.40\n",
      "[2022/08/16 03:09:25 __main__ INFO] - Epoch 50 Step 1767/1768 Train Loss 0.01673902\n",
      "[2022/08/16 03:09:30 __main__ INFO] - Epoch 50 Test Loss 0.01612648\n",
      "[2022/08/16 03:09:30 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 03:11:37 __main__ INFO] - Epoch 51 Step 1767/1768 Train Loss 0.01659298\n",
      "[2022/08/16 03:11:42 __main__ INFO] - Epoch 51 Test Loss 0.01594773\n",
      "[2022/08/16 03:11:42 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 03:13:49 __main__ INFO] - Epoch 52 Step 1767/1768 Train Loss 0.01642086\n",
      "[2022/08/16 03:13:53 __main__ INFO] - Epoch 52 Test Loss 0.01587763\n",
      "[2022/08/16 03:13:53 __main__ INFO] - Elapsed 4.47\n",
      "[2022/08/16 03:16:00 __main__ INFO] - Epoch 53 Step 1767/1768 Train Loss 0.01646595\n",
      "[2022/08/16 03:16:05 __main__ INFO] - Epoch 53 Test Loss 0.01570346\n",
      "[2022/08/16 03:16:05 __main__ INFO] - Elapsed 4.43\n",
      "[2022/08/16 03:18:12 __main__ INFO] - Epoch 54 Step 1767/1768 Train Loss 0.01643166\n",
      "[2022/08/16 03:18:17 __main__ INFO] - Epoch 54 Test Loss 0.01592711\n",
      "[2022/08/16 03:18:17 __main__ INFO] - Elapsed 4.42\n",
      "[2022/08/16 03:20:24 __main__ INFO] - Epoch 55 Step 1767/1768 Train Loss 0.01635365\n",
      "[2022/08/16 03:20:28 __main__ INFO] - Epoch 55 Test Loss 0.01583151\n",
      "[2022/08/16 03:20:28 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 03:22:35 __main__ INFO] - Epoch 56 Step 1767/1768 Train Loss 0.01638191\n",
      "[2022/08/16 03:22:40 __main__ INFO] - Epoch 56 Test Loss 0.01568551\n",
      "[2022/08/16 03:22:40 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 03:24:47 __main__ INFO] - Epoch 57 Step 1767/1768 Train Loss 0.01626704\n",
      "[2022/08/16 03:24:52 __main__ INFO] - Epoch 57 Test Loss 0.01578474\n",
      "[2022/08/16 03:24:52 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 03:26:59 __main__ INFO] - Epoch 58 Step 1767/1768 Train Loss 0.01618662\n",
      "[2022/08/16 03:27:03 __main__ INFO] - Epoch 58 Test Loss 0.01573127\n",
      "[2022/08/16 03:27:03 __main__ INFO] - Elapsed 4.49\n",
      "[2022/08/16 03:29:10 __main__ INFO] - Epoch 59 Step 1767/1768 Train Loss 0.01619332\n",
      "[2022/08/16 03:29:15 __main__ INFO] - Epoch 59 Test Loss 0.01582571\n",
      "[2022/08/16 03:29:15 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 03:31:22 __main__ INFO] - Epoch 60 Step 1767/1768 Train Loss 0.01627835\n",
      "[2022/08/16 03:31:26 __main__ INFO] - Epoch 60 Test Loss 0.01581967\n",
      "[2022/08/16 03:31:26 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 03:33:34 __main__ INFO] - Epoch 61 Step 1767/1768 Train Loss 0.01620915\n",
      "[2022/08/16 03:33:38 __main__ INFO] - Epoch 61 Test Loss 0.01560698\n",
      "[2022/08/16 03:33:38 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 03:35:46 __main__ INFO] - Epoch 62 Step 1767/1768 Train Loss 0.01614979\n",
      "[2022/08/16 03:35:50 __main__ INFO] - Epoch 62 Test Loss 0.01579413\n",
      "[2022/08/16 03:35:50 __main__ INFO] - Elapsed 4.45\n",
      "[2022/08/16 03:37:57 __main__ INFO] - Epoch 63 Step 1767/1768 Train Loss 0.01622714\n",
      "[2022/08/16 03:38:02 __main__ INFO] - Epoch 63 Test Loss 0.01582785\n",
      "[2022/08/16 03:38:02 __main__ INFO] - Elapsed 4.47\n",
      "[2022/08/16 03:40:09 __main__ INFO] - Epoch 64 Step 1767/1768 Train Loss 0.01620077\n",
      "[2022/08/16 03:40:13 __main__ INFO] - Epoch 64 Test Loss 0.01556804\n",
      "[2022/08/16 03:40:13 __main__ INFO] - Elapsed 4.43\n",
      "[2022/08/16 03:42:20 __main__ INFO] - Epoch 65 Step 1767/1768 Train Loss 0.01612743\n",
      "[2022/08/16 03:42:25 __main__ INFO] - Epoch 65 Test Loss 0.01565805\n",
      "[2022/08/16 03:42:25 __main__ INFO] - Elapsed 4.41\n",
      "[2022/08/16 03:44:32 __main__ INFO] - Epoch 66 Step 1767/1768 Train Loss 0.01601491\n",
      "[2022/08/16 03:44:37 __main__ INFO] - Epoch 66 Test Loss 0.01544727\n",
      "[2022/08/16 03:44:37 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 03:46:44 __main__ INFO] - Epoch 67 Step 1767/1768 Train Loss 0.01602951\n",
      "[2022/08/16 03:46:49 __main__ INFO] - Epoch 67 Test Loss 0.01544404\n",
      "[2022/08/16 03:46:49 __main__ INFO] - Elapsed 4.43\n",
      "[2022/08/16 03:48:56 __main__ INFO] - Epoch 68 Step 1767/1768 Train Loss 0.01597744\n",
      "[2022/08/16 03:49:00 __main__ INFO] - Epoch 68 Test Loss 0.01561983\n",
      "[2022/08/16 03:49:00 __main__ INFO] - Elapsed 4.47\n",
      "[2022/08/16 03:51:07 __main__ INFO] - Epoch 69 Step 1767/1768 Train Loss 0.01605264\n",
      "[2022/08/16 03:51:12 __main__ INFO] - Epoch 69 Test Loss 0.01546603\n",
      "[2022/08/16 03:51:12 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 03:53:19 __main__ INFO] - Epoch 70 Step 1767/1768 Train Loss 0.01609965\n",
      "[2022/08/16 03:53:24 __main__ INFO] - Epoch 70 Test Loss 0.01533042\n",
      "[2022/08/16 03:53:24 __main__ INFO] - Elapsed 4.47\n",
      "[2022/08/16 03:55:31 __main__ INFO] - Epoch 71 Step 1767/1768 Train Loss 0.01609162\n",
      "[2022/08/16 03:55:36 __main__ INFO] - Epoch 71 Test Loss 0.01601112\n",
      "[2022/08/16 03:55:36 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 03:57:43 __main__ INFO] - Epoch 72 Step 1767/1768 Train Loss 0.01576349\n",
      "[2022/08/16 03:57:47 __main__ INFO] - Epoch 72 Test Loss 0.01542833\n",
      "[2022/08/16 03:57:47 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 03:59:54 __main__ INFO] - Epoch 73 Step 1767/1768 Train Loss 0.01602978\n",
      "[2022/08/16 03:59:59 __main__ INFO] - Epoch 73 Test Loss 0.01545000\n",
      "[2022/08/16 03:59:59 __main__ INFO] - Elapsed 4.39\n",
      "[2022/08/16 04:02:06 __main__ INFO] - Epoch 74 Step 1767/1768 Train Loss 0.01592161\n",
      "[2022/08/16 04:02:10 __main__ INFO] - Epoch 74 Test Loss 0.01534888\n",
      "[2022/08/16 04:02:10 __main__ INFO] - Elapsed 4.45\n",
      "[2022/08/16 04:04:18 __main__ INFO] - Epoch 75 Step 1767/1768 Train Loss 0.01587376\n",
      "[2022/08/16 04:04:22 __main__ INFO] - Epoch 75 Test Loss 0.01539582\n",
      "[2022/08/16 04:04:22 __main__ INFO] - Elapsed 4.40\n",
      "[2022/08/16 04:06:29 __main__ INFO] - Epoch 76 Step 1767/1768 Train Loss 0.01576333\n",
      "[2022/08/16 04:06:34 __main__ INFO] - Epoch 76 Test Loss 0.01533782\n",
      "[2022/08/16 04:06:34 __main__ INFO] - Elapsed 4.45\n",
      "[2022/08/16 04:08:41 __main__ INFO] - Epoch 77 Step 1767/1768 Train Loss 0.01590946\n",
      "[2022/08/16 04:08:46 __main__ INFO] - Epoch 77 Test Loss 0.01554934\n",
      "[2022/08/16 04:08:46 __main__ INFO] - Elapsed 4.43\n",
      "[2022/08/16 04:10:53 __main__ INFO] - Epoch 78 Step 1767/1768 Train Loss 0.01584163\n",
      "[2022/08/16 04:10:57 __main__ INFO] - Epoch 78 Test Loss 0.01557958\n",
      "[2022/08/16 04:10:57 __main__ INFO] - Elapsed 4.43\n",
      "[2022/08/16 04:13:04 __main__ INFO] - Epoch 79 Step 1767/1768 Train Loss 0.01585596\n",
      "[2022/08/16 04:13:09 __main__ INFO] - Epoch 79 Test Loss 0.01535568\n",
      "[2022/08/16 04:13:09 __main__ INFO] - Elapsed 4.43\n",
      "[2022/08/16 04:15:16 __main__ INFO] - Epoch 80 Step 1767/1768 Train Loss 0.01577727\n",
      "[2022/08/16 04:15:20 __main__ INFO] - Epoch 80 Test Loss 0.01536316\n",
      "[2022/08/16 04:15:20 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 04:17:28 __main__ INFO] - Epoch 81 Step 1767/1768 Train Loss 0.01571788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022/08/16 04:17:32 __main__ INFO] - Epoch 81 Test Loss 0.01529741\n",
      "[2022/08/16 04:17:32 __main__ INFO] - Elapsed 4.42\n",
      "[2022/08/16 04:19:39 __main__ INFO] - Epoch 82 Step 1767/1768 Train Loss 0.01579614\n",
      "[2022/08/16 04:19:44 __main__ INFO] - Epoch 82 Test Loss 0.01517255\n",
      "[2022/08/16 04:19:44 __main__ INFO] - Elapsed 4.47\n",
      "[2022/08/16 04:21:51 __main__ INFO] - Epoch 83 Step 1767/1768 Train Loss 0.01576851\n",
      "[2022/08/16 04:21:56 __main__ INFO] - Epoch 83 Test Loss 0.01530276\n",
      "[2022/08/16 04:21:56 __main__ INFO] - Elapsed 4.43\n",
      "[2022/08/16 04:24:03 __main__ INFO] - Epoch 84 Step 1767/1768 Train Loss 0.01564085\n",
      "[2022/08/16 04:24:07 __main__ INFO] - Epoch 84 Test Loss 0.01547974\n",
      "[2022/08/16 04:24:07 __main__ INFO] - Elapsed 4.47\n",
      "[2022/08/16 04:26:14 __main__ INFO] - Epoch 85 Step 1767/1768 Train Loss 0.01566275\n",
      "[2022/08/16 04:26:19 __main__ INFO] - Epoch 85 Test Loss 0.01521524\n",
      "[2022/08/16 04:26:19 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 04:28:26 __main__ INFO] - Epoch 86 Step 1767/1768 Train Loss 0.01577144\n",
      "[2022/08/16 04:28:31 __main__ INFO] - Epoch 86 Test Loss 0.01540287\n",
      "[2022/08/16 04:28:31 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 04:30:38 __main__ INFO] - Epoch 87 Step 1767/1768 Train Loss 0.01576323\n",
      "[2022/08/16 04:30:42 __main__ INFO] - Epoch 87 Test Loss 0.01520806\n",
      "[2022/08/16 04:30:42 __main__ INFO] - Elapsed 4.42\n",
      "[2022/08/16 04:32:50 __main__ INFO] - Epoch 88 Step 1767/1768 Train Loss 0.01569132\n",
      "[2022/08/16 04:32:54 __main__ INFO] - Epoch 88 Test Loss 0.01520759\n",
      "[2022/08/16 04:32:54 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 04:35:01 __main__ INFO] - Epoch 89 Step 1767/1768 Train Loss 0.01570833\n",
      "[2022/08/16 04:35:06 __main__ INFO] - Epoch 89 Test Loss 0.01529593\n",
      "[2022/08/16 04:35:06 __main__ INFO] - Elapsed 4.45\n",
      "[2022/08/16 04:37:13 __main__ INFO] - Epoch 90 Step 1767/1768 Train Loss 0.01561276\n",
      "[2022/08/16 04:37:17 __main__ INFO] - Epoch 90 Test Loss 0.01511549\n",
      "[2022/08/16 04:37:17 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 04:39:25 __main__ INFO] - Epoch 91 Step 1767/1768 Train Loss 0.01558531\n",
      "[2022/08/16 04:39:29 __main__ INFO] - Epoch 91 Test Loss 0.01545292\n",
      "[2022/08/16 04:39:29 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 04:41:36 __main__ INFO] - Epoch 92 Step 1767/1768 Train Loss 0.01556699\n",
      "[2022/08/16 04:41:41 __main__ INFO] - Epoch 92 Test Loss 0.01526294\n",
      "[2022/08/16 04:41:41 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 04:43:48 __main__ INFO] - Epoch 93 Step 1767/1768 Train Loss 0.01561837\n",
      "[2022/08/16 04:43:53 __main__ INFO] - Epoch 93 Test Loss 0.01531812\n",
      "[2022/08/16 04:43:53 __main__ INFO] - Elapsed 4.47\n",
      "[2022/08/16 04:46:00 __main__ INFO] - Epoch 94 Step 1767/1768 Train Loss 0.01571991\n",
      "[2022/08/16 04:46:04 __main__ INFO] - Epoch 94 Test Loss 0.01526169\n",
      "[2022/08/16 04:46:04 __main__ INFO] - Elapsed 4.43\n",
      "[2022/08/16 04:48:11 __main__ INFO] - Epoch 95 Step 1767/1768 Train Loss 0.01556069\n",
      "[2022/08/16 04:48:16 __main__ INFO] - Epoch 95 Test Loss 0.01563370\n",
      "[2022/08/16 04:48:16 __main__ INFO] - Elapsed 4.40\n",
      "[2022/08/16 04:50:23 __main__ INFO] - Epoch 96 Step 1767/1768 Train Loss 0.01554764\n",
      "[2022/08/16 04:50:28 __main__ INFO] - Epoch 96 Test Loss 0.01525407\n",
      "[2022/08/16 04:50:28 __main__ INFO] - Elapsed 4.43\n",
      "[2022/08/16 04:52:35 __main__ INFO] - Epoch 97 Step 1767/1768 Train Loss 0.01563962\n",
      "[2022/08/16 04:52:39 __main__ INFO] - Epoch 97 Test Loss 0.01498904\n",
      "[2022/08/16 04:52:39 __main__ INFO] - Elapsed 4.47\n",
      "[2022/08/16 04:54:47 __main__ INFO] - Epoch 98 Step 1767/1768 Train Loss 0.01547009\n",
      "[2022/08/16 04:54:51 __main__ INFO] - Epoch 98 Test Loss 0.01528718\n",
      "[2022/08/16 04:54:51 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 04:56:58 __main__ INFO] - Epoch 99 Step 1767/1768 Train Loss 0.01549326\n",
      "[2022/08/16 04:57:03 __main__ INFO] - Epoch 99 Test Loss 0.01508431\n",
      "[2022/08/16 04:57:03 __main__ INFO] - Elapsed 4.42\n",
      "[2022/08/16 04:59:10 __main__ INFO] - Epoch 100 Step 1767/1768 Train Loss 0.01536477\n",
      "[2022/08/16 04:59:14 __main__ INFO] - Epoch 100 Test Loss 0.01504276\n",
      "[2022/08/16 04:59:14 __main__ INFO] - Elapsed 4.42\n",
      "[2022/08/16 05:01:22 __main__ INFO] - Epoch 101 Step 1767/1768 Train Loss 0.01533298\n",
      "[2022/08/16 05:01:26 __main__ INFO] - Epoch 101 Test Loss 0.01532784\n",
      "[2022/08/16 05:01:26 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 05:03:33 __main__ INFO] - Epoch 102 Step 1767/1768 Train Loss 0.01545910\n",
      "[2022/08/16 05:03:38 __main__ INFO] - Epoch 102 Test Loss 0.01524245\n",
      "[2022/08/16 05:03:38 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 05:05:45 __main__ INFO] - Epoch 103 Step 1767/1768 Train Loss 0.01538522\n",
      "[2022/08/16 05:05:49 __main__ INFO] - Epoch 103 Test Loss 0.01545593\n",
      "[2022/08/16 05:05:49 __main__ INFO] - Elapsed 4.48\n",
      "[2022/08/16 05:07:57 __main__ INFO] - Epoch 104 Step 1767/1768 Train Loss 0.01545364\n",
      "[2022/08/16 05:08:01 __main__ INFO] - Epoch 104 Test Loss 0.01522378\n",
      "[2022/08/16 05:08:01 __main__ INFO] - Elapsed 4.47\n",
      "[2022/08/16 05:10:08 __main__ INFO] - Epoch 105 Step 1767/1768 Train Loss 0.01543825\n",
      "[2022/08/16 05:10:13 __main__ INFO] - Epoch 105 Test Loss 0.01510012\n",
      "[2022/08/16 05:10:13 __main__ INFO] - Elapsed 4.49\n",
      "[2022/08/16 05:12:20 __main__ INFO] - Epoch 106 Step 1767/1768 Train Loss 0.01549772\n",
      "[2022/08/16 05:12:25 __main__ INFO] - Epoch 106 Test Loss 0.01501719\n",
      "[2022/08/16 05:12:25 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 05:14:32 __main__ INFO] - Epoch 107 Step 1767/1768 Train Loss 0.01553344\n",
      "[2022/08/16 05:14:36 __main__ INFO] - Epoch 107 Test Loss 0.01530675\n",
      "[2022/08/16 05:14:36 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 05:16:43 __main__ INFO] - Epoch 108 Step 1767/1768 Train Loss 0.01545825\n",
      "[2022/08/16 05:16:48 __main__ INFO] - Epoch 108 Test Loss 0.01504268\n",
      "[2022/08/16 05:16:48 __main__ INFO] - Elapsed 4.43\n",
      "[2022/08/16 05:18:55 __main__ INFO] - Epoch 109 Step 1767/1768 Train Loss 0.01546653\n",
      "[2022/08/16 05:18:59 __main__ INFO] - Epoch 109 Test Loss 0.01506118\n",
      "[2022/08/16 05:18:59 __main__ INFO] - Elapsed 4.46\n",
      "[2022/08/16 05:21:07 __main__ INFO] - Epoch 110 Step 1767/1768 Train Loss 0.01546057\n",
      "[2022/08/16 05:21:11 __main__ INFO] - Epoch 110 Test Loss 0.01518731\n",
      "[2022/08/16 05:21:11 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 05:23:18 __main__ INFO] - Epoch 111 Step 1767/1768 Train Loss 0.01551072\n",
      "[2022/08/16 05:23:23 __main__ INFO] - Epoch 111 Test Loss 0.01516481\n",
      "[2022/08/16 05:23:23 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 05:25:30 __main__ INFO] - Epoch 112 Step 1767/1768 Train Loss 0.01550367\n",
      "[2022/08/16 05:25:34 __main__ INFO] - Epoch 112 Test Loss 0.01506714\n",
      "[2022/08/16 05:25:34 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 05:27:41 __main__ INFO] - Epoch 113 Step 1767/1768 Train Loss 0.01544206\n",
      "[2022/08/16 05:27:46 __main__ INFO] - Epoch 113 Test Loss 0.01500562\n",
      "[2022/08/16 05:27:46 __main__ INFO] - Elapsed 4.44\n",
      "[2022/08/16 05:29:53 __main__ INFO] - Epoch 114 Step 1767/1768 Train Loss 0.01531535\n",
      "[2022/08/16 05:29:57 __main__ INFO] - Epoch 114 Test Loss 0.01509195\n",
      "[2022/08/16 05:29:57 __main__ INFO] - Elapsed 4.47\n",
      "[2022/08/16 05:32:04 __main__ INFO] - Epoch 115 Step 1767/1768 Train Loss 0.01544383\n",
      "[2022/08/16 05:32:09 __main__ INFO] - Epoch 115 Test Loss 0.01514280\n",
      "[2022/08/16 05:32:09 __main__ INFO] - Elapsed 4.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 115\n"
     ]
    }
   ],
   "source": [
    "# Test with Adam Optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=optim_config['base_lr'],\n",
    "#                              weight_decay=optim_config['weight_decay'])\n",
    "\n",
    "# run test before start training\n",
    "# test_outputs = test(0, model, criterion, test_loader, run_config, writer, device)\n",
    "\n",
    "ref1 = 0\n",
    "ref2 = 0\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "train_flag = True\n",
    "\n",
    "run_config['scheduler'] = scheduler\n",
    "    \n",
    "for epoch in range(optim_config['epochs']):\n",
    "\n",
    "    loss_ = train(epoch, model, optimizer, criterion, train_loader, (demo_cs,demo_np), run_config,\n",
    "         writer, device, logger=logger)\n",
    "    train_loss_list.append(loss_)\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    test_loss_ = test(epoch, model, criterion, test_loader, (demo_cs,demo_np), run_config,\n",
    "                    writer, device, logger, return_output=False)\n",
    "    test_loss_list.append(test_loss_)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        if epoch > 105:\n",
    "            if (np.abs(loss_ - ref1)<ref1*0.005) & (np.abs(loss_ - ref2)<ref2*0.005):\n",
    "                print(\"Early stopping at epoch\", epoch)\n",
    "                break\n",
    "            if (ref1 < loss_) & (ref1 < ref2):\n",
    "                print(\"Diverging. stop.\")\n",
    "                train_flag = False\n",
    "                break\n",
    "            if loss_ < best:\n",
    "                best = loss_\n",
    "                best_epoch = epoch\n",
    "        else:\n",
    "            best = loss_\n",
    "            best_epoch = epoch\n",
    "\n",
    "        ref2 = ref1\n",
    "        ref1 = loss_\n",
    "\n",
    "        if (config['run_config']['save']) & (best_epoch==epoch):\n",
    "            torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'config': config},\n",
    "                model_dir+\"SSD_\"+zoomlevel+\"_\"+str(model_config['output_dim']**2*2048)+\"_\"+str(v2)+\"_\"+\n",
    "                       str(model_run_date)+\"_\"+str(epoch)+\".pt\")\n",
    "\n",
    "            \n",
    "if config['run_config']['save']:\n",
    "    files = glob.glob(model_dir+\"SSD_\"+zoomlevel+\"_\"+str(model_config['output_dim']**2*2048)+\"_\"+str(v2)+\"_\"+\n",
    "                       str(model_run_date)+\"_\"+str(epoch)+\".pt\")\n",
    "\n",
    "    for f in files:\n",
    "        e = int(f.split(\"_\")[-1].split(\".\")[0])\n",
    "        if e != best_epoch:\n",
    "            os.remove(f)\n",
    "\n",
    "        \n",
    "# if run_config['tensorboard']:\n",
    "#     outpath = os.path.join(outdir, 'all_scalars.json')\n",
    "#     writer.export_scalars_to_json(outpath)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "ax.plot(train_loss_list, color='cornflowerblue', label='Train')\n",
    "ax.plot(test_loss_list, color='sandybrown', label='Test')\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_ylim([0, 1.1*np.max(train_loss_list+test_loss_list)])\n",
    "ax.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(out_dir+\"training_plots/SSD_\"+model_run_date+\".png\")#, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t10\t20\t30\t40\t50\t60\t70\t80\t90\t100\t110\t120\t130\t140\t150\t160\t170\t180\t190\t200\t210\t220\t230\t240\t250\t260\t270\t280\t290\t300\t310\t320\t330\t340\t350\t360\t370\t380\t390\t400\t410\t420\t430\t440\t450\t460\t470\t480\t490\t500\t510\t520\t530\t540\t550\t560\t570\t580\t590\t600\t610\t620\t630\t640\t650\t660\t670\t680\t690\t700\t710\t720\t730\t740\t750\t760\t770\t780\t790\t800\t810\t820\t830\t840\t850\t860\t870\t880\t890\t900\t910\t920\t930\t940\t950\t960\t970\t980\t990\t1000\t1010\t1020\t1030\t1040\t1050\t1060\t1070\t1080\t1090\t1100\t1110\t1120\t1130\t1140\t1150\t1160\t1170\t1180\t1190\t1200\t1210\t1220\t1230\t1240\t1250\t1260\t1270\t1280\t1290\t1300\t1310\t1320\t1330\t1340\t1350\t1360\t1370\t1380\t1390\t1400\t1410\t1420\t1430\t1440\t1450\t1460\t1470\t1480\t1490\t1500\t1510\t1520\t1530\t1540\t1550\t1560\t1570\t1580\t1590\t1600\t1610\t1620\t1630\t1640\t1650\t1660\t1670\t1680\t1690\t1700\t1710\t1720\t1730\t1740\t1750\t1760\t0\t10\t20\t30\t40\t50\t60\t70\t80\t90\t100\t110\t120\t130\t140\t150\t160\t170\t180\t190\t(31420, 10)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "ct = []\n",
    "encoder_output = []\n",
    "im = []\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    data1 = data[1].to(device)\n",
    "    ct += [s[s.rindex(\"/\")+1: s.rindex(\"_\")]for s in data[0]]\n",
    "    encoder_output += [model(data1).cpu().detach().numpy()]\n",
    "    im += data[0]\n",
    "    if step % 10 == 0:\n",
    "        print(step, end='\\t')\n",
    "\n",
    "for step, data in enumerate(test_loader):\n",
    "    data1 = data[1].to(device)\n",
    "    ct += [s[s.rindex(\"/\")+1: s.rindex(\"_\")]for s in data[0]]\n",
    "    encoder_output += [model(data1).cpu().detach().numpy()]\n",
    "    im += data[0]\n",
    "    if step % 10 == 0:\n",
    "        print(step, end='\\t')\n",
    "\n",
    "encoder_output = np.vstack(encoder_output)    \n",
    "print(encoder_output.shape)\n",
    "encoder_output = encoder_output.reshape(len(encoder_output), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proj_dir+\"latent_space/SSD_\"+zoomlevel+\"_\"+str(model_config['output_dim']**2*2048)+\"_\"+str(v2)+\"_\"+\n",
    "                       str(model_run_date)+\".pkl\", \"wb\") as f:\n",
    "        pkl.dump(encoder_output, f)\n",
    "        pkl.dump(im, f)\n",
    "        pkl.dump(ct, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_dir+\"SAE_train.csv\", \"a\") as f:\n",
    "    f.write(\"%s,%s,%d,%s,%s,%d,%.4f,%.4f,%.4f,%.4f,%d\\n\" % (model_run_date, zoomlevel, model_config['output_dim']**2*2048, \n",
    "            sampling, normalization, best_epoch, best_1, best_2, best_test_1, best_test_2, train_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse Normalization\n",
    "\n",
    "# CIFAR\n",
    "# inv_normalize = torchvision.transforms.Normalize(\n",
    "#     mean=[-0.4914/0.2470, -0.4822/0.2435, -0.4465/0.2616],\n",
    "#     std=[1/0.2470, 1/0.2435, 1/0.2616]\n",
    "# )\n",
    "\n",
    "\n",
    "# Satellite image\n",
    "inv_normalize = torchvision.transforms.Normalize(\n",
    "    mean=[-0.3733/0.2173, -0.3991/0.2055, -0.3711/0.2143],\n",
    "    std=[1/0.2173, 1/0.2055, 1/0.2143]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for step, (_,data) in enumerate(test_loader):\n",
    "    data = data.to(device)\n",
    "    test_output = model(data)\n",
    "    test_output_orig = inv_normalize(test_output)\n",
    "    data_orig = inv_normalize(data)\n",
    "    if step == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist(data[plot_image, :, :, :].cpu().detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_output[plot_image, :, :, :].cpu().detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(test_output[:,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(test_output[:,1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(test_output[:,2,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(data[:,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(data[:,1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(data[:,1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((test_output - data).detach().cpu().numpy()[:,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((test_output - data).detach().cpu().numpy()[:,1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((test_output - data).detach().cpu().numpy()[:,2,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((test_output - data).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.power((test_output - data).cpu().detach().numpy(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((test_output - data).detach().cpu().numpy()[:,2,:,:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((test_output - data).detach().cpu().numpy()[:,1,:,:].flatten());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((test_output - data).detach().cpu().numpy()[:,0,:,:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_orig[plot_image,:,:,:].cpu().detach().permute(1, 2, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_orig[plot_image,:,:,:].cpu().detach().permute(1, 2, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_output_orig[plot_image,:,:,:].cpu().detach().permute(1, 2, 0))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(data_orig[plot_image,:,:,:].cpu().detach().permute(1, 2, 0));\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data.detach().numpy()[:,0,:,:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qingyi",
   "language": "python",
   "name": "qingyi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
