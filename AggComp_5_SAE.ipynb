{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook documents part 2 of the complementarity of image and demographic information: the ability of latent space extracted from Autoencoders to predict mode choice and trip generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"models/\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "from dataloader import SurveyDataset, load_aggregate_travel_behavior, load_demo\n",
    "from M1_util_train_test import load_model, test\n",
    "import linear_reg\n",
    "import mnl\n",
    "from setup import out_dir, data_dir, image_dir, model_dir, proj_dir\n",
    "\n",
    "plt.rcParams.update({\"font.size\":12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_version = '1571'\n",
    "\n",
    "model_type = 'SAE'\n",
    "model_code = 'M1_A1'\n",
    "sampling = 's'\n",
    "\n",
    "zoomlevel = 'zoom13'\n",
    "output_dim = 1\n",
    "model_run_date = '22021402'\n",
    "\n",
    "variable_names = ['active','auto','mas','pt', 'trpgen']\n",
    "\n",
    "demo_variables = ['tot_population','pct25_34yrs','pct35_50yrs','pctover65yrs',\n",
    "         'pctwhite_alone','pct_nonwhite','pctblack_alone',\n",
    "         'pct_col_grad','avg_tt_to_work','inc_per_capita']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Load Model Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proj_dir+\"latent_space/\"+model_type+\"_\"+zoomlevel+\"_\"+str(output_dim**2*2048)+\"_\"+\n",
    "                       model_run_date+\".pkl\", \"rb\") as f: \n",
    "    encoder_output = pkl.load(f)\n",
    "    im = pkl.load(f)\n",
    "    ct = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Embeddings\n",
    "unique_ct = list(set(ct))\n",
    "unique_ct.sort()\n",
    "ct = np.array(ct)\n",
    "aggregate_embeddings = []\n",
    "for i in unique_ct:\n",
    "    aggregate_embeddings.append(np.mean(encoder_output[ct == i], axis=0))\n",
    "aggregate_embeddings = np.array(aggregate_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Trip Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"origin_trip_behavior.csv\"\n",
    "df_pivot = load_aggregate_travel_behavior(file, data_version)\n",
    "\n",
    "train_test_index = df_pivot['train_test'].astype(bool).to_numpy()\n",
    "# train_test_index = np.random.rand(len(df_pivot)) < 0.2\n",
    "\n",
    "y = df_pivot[variable_names].to_numpy()\n",
    "y_train = y[~train_test_index,:4]\n",
    "y_test = y[train_test_index,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = aggregate_embeddings[~train_test_index, :]\n",
    "x_test = aggregate_embeddings[train_test_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_train = y[~train_test_index,1]\n",
    "auto_test = y[train_test_index,1]\n",
    "\n",
    "pt_train = y[~train_test_index,3]\n",
    "pt_test = y[train_test_index,3]\n",
    "\n",
    "active_train = y[~train_test_index,0]\n",
    "active_test = y[train_test_index,0]\n",
    "\n",
    "trpgen_train = y[~train_test_index,-1]\n",
    "trpgen_test = y[train_test_index,-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression\n",
    "\n",
    "Highlight:\n",
    "\n",
    "Auto Lasso\n",
    "Parameter: 3.00e-04 Train R2: 0.7074 \t Test R: 0.6966 \t Nonzero coef: 132\n",
    "\n",
    "Auto Ridge\n",
    "Parameter: 1.00e+01 Train R2: 0.7656 \t Test R: 0.7267\n",
    "\n",
    "PT Lasso\n",
    "Parameter: 1.00e-04 Train R2: 0.6486 \t Test R: 0.5349 \t Nonzero coef: 209\n",
    "\n",
    "PT Ridge\n",
    "Parameter: 1.00e+01 Train R2: 0.6856 \t Test R: 0.5561\n",
    "\n",
    "Active Lasso\n",
    "Parameter: 2.00e-04 Train R2: 0.6585 \t Test R: 0.6040 \t Nonzero coef: 158\n",
    "\n",
    "Active Ridge\n",
    "Parameter: 1.00e+01 Train R2: 0.7123 \t Test R: 0.6091\n",
    "\n",
    "Trip Gen Lasso\n",
    "Parameter: 1.00e-01 Train R2: 0.2968 \t Test R: 0.2061 \t Nonzero coef: 34\n",
    "\n",
    "Trip Gen Ridge\n",
    "Parameter: 8.00e+01 Train R2: 0.3982 \t Test R: 0.2069"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Auto Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 1.0000 \t Test R2: -0.5727\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression without Regularization\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(x_train, auto_train)\n",
    "# with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#     f.write(\"%s,%s,%s,%.4f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], -1, \n",
    "#         lr.score(x_train, auto_train), lr.score(x_test, auto_test), 'lr', zoomlevel,\n",
    "#         np.sum(lr.coef_ != 0), len(lr.coef_)))\n",
    "print(\"Train R2: %.4f \\t Test R2: %.4f\" % (lr.score(x_train, auto_train), lr.score(x_test, auto_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  after removing the cwd from sys.path.\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.322e+00, tolerance: 7.704e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.9657 \t Test R: -0.2055 \t Nonzero coef: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.346e+00, tolerance: 7.704e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 1.00e-05 Train R2: 0.9556 \t Test R: 0.2199 \t Nonzero coef: 1798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.479e+00, tolerance: 7.704e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 1.00e-04 Train R2: 0.8245 \t Test R: 0.6305 \t Nonzero coef: 564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.166e+00, tolerance: 7.704e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 2.00e-04 Train R2: 0.7398 \t Test R: 0.6423 \t Nonzero coef: 287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.279e-01, tolerance: 7.704e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 3.00e-04 Train R2: 0.6942 \t Test R: 0.6366 \t Nonzero coef: 176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.245e-01, tolerance: 7.704e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 4.00e-04 Train R2: 0.6681 \t Test R: 0.6228 \t Nonzero coef: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.437e-01, tolerance: 7.704e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 5.00e-04 Train R2: 0.6484 \t Test R: 0.6168 \t Nonzero coef: 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.847e-01, tolerance: 7.704e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 6.00e-04 Train R2: 0.6353 \t Test R: 0.6160 \t Nonzero coef: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.469e-01, tolerance: 7.704e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 7.00e-04 Train R2: 0.6261 \t Test R: 0.6132 \t Nonzero coef: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.340e-01, tolerance: 7.704e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 8.00e-04 Train R2: 0.6185 \t Test R: 0.6077 \t Nonzero coef: 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.525e-01, tolerance: 7.704e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 1.00e-03 Train R2: 0.6038 \t Test R: 0.5919 \t Nonzero coef: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.284e-02, tolerance: 7.704e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 2.00e-03 Train R2: 0.5610 \t Test R: 0.5445 \t Nonzero coef: 30\n",
      "Parameter: 5.00e-03 Train R2: 0.4755 \t Test R: 0.4378 \t Nonzero coef: 13\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "for a in (1e-4)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, auto_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d\" % (a, lasso.score(x_train, auto_train), \n",
    "                                                                                  lasso.score(x_test, auto_test), \n",
    "                                                                                  np.sum(lasso.coef_ != 0)))\n",
    "\n",
    "#     with open(out_dir+\"BA_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%.6f,%.4f,%.4f,%s,%d,%d\\n\" % (a, \n",
    "#             lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test), 'lasso', \n",
    "#             np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:194: LinAlgWarning: Ill-conditioned matrix (rcond=4.46374e-11): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, sym_pos=True, overwrite_a=False)\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:194: LinAlgWarning: Ill-conditioned matrix (rcond=4.04756e-08): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, sym_pos=True, overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.9401 \t Test R: -0.5934\n",
      "Parameter: 5.00e-01 Train R2: 0.9313 \t Test R: 0.5258\n",
      "Parameter: 5.00e+00 Train R2: 0.8045 \t Test R: 0.6644\n",
      "Parameter: 1.00e+01 Train R2: 0.7604 \t Test R: 0.6688\n",
      "Parameter: 1.50e+01 Train R2: 0.7356 \t Test R: 0.6671\n",
      "Parameter: 2.00e+01 Train R2: 0.7189 \t Test R: 0.6643\n",
      "Parameter: 2.50e+01 Train R2: 0.7064 \t Test R: 0.6613\n",
      "Parameter: 3.00e+01 Train R2: 0.6966 \t Test R: 0.6585\n",
      "Parameter: 3.50e+01 Train R2: 0.6887 \t Test R: 0.6558\n",
      "Parameter: 4.00e+01 Train R2: 0.6820 \t Test R: 0.6532\n",
      "Parameter: 5.00e+01 Train R2: 0.6712 \t Test R: 0.6484\n",
      "Parameter: 1.00e+02 Train R2: 0.6410 \t Test R: 0.6305\n",
      "Parameter: 2.50e+02 Train R2: 0.6050 \t Test R: 0.5991\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "\n",
    "for a in (5e+0)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, auto_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, auto_train), \n",
    "                                                              ridge.score(x_test, auto_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  after removing the cwd from sys.path.\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.424e-01, tolerance: 1.373e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.9647 \t Test R: -0.4727 \t Nonzero coef: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.707e-01, tolerance: 1.373e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 1.00e-05 Train R2: 0.9315 \t Test R: 0.1211 \t Nonzero coef: 1447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.621e-01, tolerance: 1.373e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 1.00e-04 Train R2: 0.6686 \t Test R: 0.4007 \t Nonzero coef: 275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.188e-01, tolerance: 1.373e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 2.00e-04 Train R2: 0.5573 \t Test R: 0.4486 \t Nonzero coef: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.465e-01, tolerance: 1.373e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 3.00e-04 Train R2: 0.5104 \t Test R: 0.4515 \t Nonzero coef: 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.078e-01, tolerance: 1.373e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 4.00e-04 Train R2: 0.4891 \t Test R: 0.4436 \t Nonzero coef: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.693e-02, tolerance: 1.373e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 5.00e-04 Train R2: 0.4752 \t Test R: 0.4345 \t Nonzero coef: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.042e-03, tolerance: 1.373e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 6.00e-04 Train R2: 0.4643 \t Test R: 0.4234 \t Nonzero coef: 24\n",
      "Parameter: 7.00e-04 Train R2: 0.4555 \t Test R: 0.4129 \t Nonzero coef: 20\n",
      "Parameter: 8.00e-04 Train R2: 0.4465 \t Test R: 0.4020 \t Nonzero coef: 18\n",
      "Parameter: 9.00e-04 Train R2: 0.4392 \t Test R: 0.3923 \t Nonzero coef: 16\n",
      "Parameter: 1.00e-03 Train R2: 0.4313 \t Test R: 0.3811 \t Nonzero coef: 16\n",
      "Parameter: 2.00e-03 Train R2: 0.3848 \t Test R: 0.3388 \t Nonzero coef: 8\n",
      "Parameter: 5.00e-03 Train R2: 0.3334 \t Test R: 0.2915 \t Nonzero coef: 5\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "for a in (1e-4)*np.array([0,0.1,1,2,3,4,5,6,7,8,9,10,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, pt_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d\" % (a, lasso.score(x_train, pt_train), \n",
    "                                                                                  lasso.score(x_test, pt_test), \n",
    "                                                                                  np.sum(lasso.coef_ != 0)))\n",
    "\n",
    "#     with open(out_dir+\"BA_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%.6f,%.4f,%.4f,%s,%d,%d\\n\" % (a, \n",
    "#             lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test), 'lasso', \n",
    "#             np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:194: LinAlgWarning: Ill-conditioned matrix (rcond=4.46374e-11): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, sym_pos=True, overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.0373 \t Test R: -1.8961\n",
      "Parameter: 5.00e+00 Train R2: 0.7567 \t Test R: 0.4224\n",
      "Parameter: 6.00e+00 Train R2: 0.7404 \t Test R: 0.4298\n",
      "Parameter: 7.00e+00 Train R2: 0.7266 \t Test R: 0.4353\n",
      "Parameter: 8.00e+00 Train R2: 0.7147 \t Test R: 0.4398\n",
      "Parameter: 9.00e+00 Train R2: 0.7043 \t Test R: 0.4435\n",
      "Parameter: 1.00e+01 Train R2: 0.6950 \t Test R: 0.4465\n",
      "Parameter: 2.00e+01 Train R2: 0.6365 \t Test R: 0.4597\n",
      "Parameter: 3.00e+01 Train R2: 0.6051 \t Test R: 0.4625\n",
      "Parameter: 4.00e+01 Train R2: 0.5844 \t Test R: 0.4625\n",
      "Parameter: 5.00e+01 Train R2: 0.5693 \t Test R: 0.4616\n",
      "Parameter: 1.00e+02 Train R2: 0.5280 \t Test R: 0.4545\n",
      "Parameter: 2.00e+02 Train R2: 0.4943 \t Test R: 0.4428\n",
      "Parameter: 5.00e+02 Train R2: 0.4574 \t Test R: 0.4204\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "\n",
    "for a in (1e+1)*np.array([0,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,10,20,50]):\n",
    "# for a in (5e0) * np.array([0, 4.5,4.6,4.7,4.8,4.9, 5, 5.1,5.2,5.3,5.4,5.5]):\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, pt_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, pt_train), \n",
    "                                                              ridge.score(x_test, pt_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.983e-01, tolerance: 3.791e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.9579 \t Test R: -0.2282 \t Nonzero coef: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.184e+00, tolerance: 3.791e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 5.00e-05 Train R2: 0.8410 \t Test R: 0.4854 \t Nonzero coef: 832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.418e+00, tolerance: 3.791e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 6.00e-05 Train R2: 0.8178 \t Test R: 0.5048 \t Nonzero coef: 721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.303e+00, tolerance: 3.791e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 7.00e-05 Train R2: 0.7955 \t Test R: 0.5153 \t Nonzero coef: 643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.038e+00, tolerance: 3.791e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 8.00e-05 Train R2: 0.7766 \t Test R: 0.5180 \t Nonzero coef: 576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.583e+00, tolerance: 3.791e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 9.00e-05 Train R2: 0.7600 \t Test R: 0.5238 \t Nonzero coef: 524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.285e+00, tolerance: 3.791e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 1.00e-04 Train R2: 0.7445 \t Test R: 0.5216 \t Nonzero coef: 466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.119e-01, tolerance: 3.791e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 2.00e-04 Train R2: 0.6444 \t Test R: 0.5047 \t Nonzero coef: 223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.788e-01, tolerance: 3.791e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 3.00e-04 Train R2: 0.5911 \t Test R: 0.5036 \t Nonzero coef: 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.621e-01, tolerance: 3.791e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 4.00e-04 Train R2: 0.5579 \t Test R: 0.4916 \t Nonzero coef: 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.783e-01, tolerance: 3.791e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 5.00e-04 Train R2: 0.5372 \t Test R: 0.4820 \t Nonzero coef: 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.946e-02, tolerance: 3.791e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 1.00e-03 Train R2: 0.4902 \t Test R: 0.4651 \t Nonzero coef: 35\n",
      "Parameter: 2.00e-03 Train R2: 0.4436 \t Test R: 0.4222 \t Nonzero coef: 24\n",
      "Parameter: 5.00e-03 Train R2: 0.3562 \t Test R: 0.3287 \t Nonzero coef: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.006e-02, tolerance: 3.791e-03\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    }
   ],
   "source": [
    "for a in (1e-4)*np.array([0,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,10,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, active_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d\" % (a, lasso.score(x_train, active_train), \n",
    "                                                                                  lasso.score(x_test, active_test), \n",
    "                                                                                  np.sum(lasso.coef_ != 0)))\n",
    "\n",
    "#     with open(out_dir+\"BA_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%.6f,%.4f,%.4f,%s,%d,%d\\n\" % (a, \n",
    "#             lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test), 'lasso', \n",
    "#             np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 1.0000 \t Test R: -0.3472\n",
      "Parameter: 1.00e+01 Train R2: 0.7555 \t Test R: 0.5881\n",
      "Parameter: 1.50e+01 Train R2: 0.7269 \t Test R: 0.6001\n",
      "Parameter: 2.00e+01 Train R2: 0.7074 \t Test R: 0.6060\n",
      "Parameter: 1.50e+01 Train R2: 0.7269 \t Test R: 0.6001\n",
      "Parameter: 3.00e+01 Train R2: 0.6811 \t Test R: 0.6114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:194: LinAlgWarning: Ill-conditioned matrix (rcond=1.20557e-10): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, sym_pos=True, overwrite_a=False)\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "\n",
    "# for a in (1e+1)*np.array([0,0.1,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,10,20,50]):\n",
    "for a in (1e+1)*np.array([0,1, 1.5, 2, 1.5, 3]):\n",
    "    \n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, active_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, active_train), \n",
    "                                                              ridge.score(x_test, active_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Trip Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.257e+03, tolerance: 3.630e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 0.9876 \t Test R: -2.5644 \t Nonzero coef: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.003e+04, tolerance: 3.630e+01\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 1.00e-03 Train R2: 0.9552 \t Test R: -1.1065 \t Nonzero coef: 1481\n",
      "Parameter: 6.00e-02 Train R2: 0.3600 \t Test R: 0.1904 \t Nonzero coef: 64\n",
      "Parameter: 7.00e-02 Train R2: 0.3392 \t Test R: 0.2006 \t Nonzero coef: 56\n",
      "Parameter: 8.00e-02 Train R2: 0.3210 \t Test R: 0.2057 \t Nonzero coef: 49\n",
      "Parameter: 1.00e-01 Train R2: 0.2968 \t Test R: 0.2061 \t Nonzero coef: 34\n",
      "Parameter: 1.10e-01 Train R2: 0.2901 \t Test R: 0.2051 \t Nonzero coef: 32\n",
      "Parameter: 1.20e-01 Train R2: 0.2845 \t Test R: 0.2024 \t Nonzero coef: 29\n",
      "Parameter: 1.30e-01 Train R2: 0.2799 \t Test R: 0.2015 \t Nonzero coef: 26\n",
      "Parameter: 1.40e-01 Train R2: 0.2752 \t Test R: 0.2003 \t Nonzero coef: 22\n",
      "Parameter: 1.50e-01 Train R2: 0.2707 \t Test R: 0.1989 \t Nonzero coef: 23\n",
      "Parameter: 2.00e-01 Train R2: 0.2482 \t Test R: 0.1862 \t Nonzero coef: 17\n",
      "Parameter: 5.00e-01 Train R2: 0.1208 \t Test R: 0.0963 \t Nonzero coef: 9\n"
     ]
    }
   ],
   "source": [
    "for a in (1e-2)*np.array([0,0.1,6,7,8,10,11,12,13,14,15,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    lasso.fit(x_train, trpgen_train)\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f \\t Nonzero coef: %d\" % (a, lasso.score(x_train, trpgen_train), \n",
    "                                                                                  lasso.score(x_test, trpgen_test), \n",
    "                                                                                  np.sum(lasso.coef_ != 0)))\n",
    "#     with open(out_dir+\"BA_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%.6f,%.4f,%.4f,%s,%d,%d\\n\" % (a, \n",
    "#             lasso.score(x_train, trpgen_train), lasso.score(x_test, trpgen_test), 'lasso', \n",
    "#             np.sum(lasso.coef_ != 0), len(lasso.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:194: LinAlgWarning: Ill-conditioned matrix (rcond=4.47584e-11): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, sym_pos=True, overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 Train R2: 1.0000 \t Test R: -2.4222\n",
      "Parameter: 1.00e+00 Train R2: 0.8127 \t Test R: -0.3083\n",
      "Parameter: 1.00e+01 Train R2: 0.5850 \t Test R: 0.1181\n",
      "Parameter: 2.00e+01 Train R2: 0.5189 \t Test R: 0.1722\n",
      "Parameter: 3.00e+01 Train R2: 0.4820 \t Test R: 0.1911\n",
      "Parameter: 4.00e+01 Train R2: 0.4566 \t Test R: 0.1997\n",
      "Parameter: 5.00e+01 Train R2: 0.4373 \t Test R: 0.2039\n",
      "Parameter: 6.00e+01 Train R2: 0.4219 \t Test R: 0.2059\n",
      "Parameter: 7.00e+01 Train R2: 0.4091 \t Test R: 0.2068\n",
      "Parameter: 8.00e+01 Train R2: 0.3982 \t Test R: 0.2069\n",
      "Parameter: 1.00e+02 Train R2: 0.3804 \t Test R: 0.2063\n",
      "Parameter: 2.00e+02 Train R2: 0.3295 \t Test R: 0.1990\n",
      "Parameter: 5.00e+02 Train R2: 0.2723 \t Test R: 0.1859\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "\n",
    "for a in (1e+1)*np.array([0,0.1,1,2,3,4,5,6,7,8,10,20,50]):\n",
    "\n",
    "    ridge = linear_model.Ridge(alpha=a)\n",
    "    ridge.fit(x_train, trpgen_train)\n",
    "#     with open(out_dir+sampling+\"_\"+model_code+\"_regression_\"+variable_names[-1]+\".csv\", \"a\") as f:\n",
    "#         f.write(\"%s,%s,%s,%.5f,%.4f,%.4f,%s,%s,%d,%d\\n\" % (model_run_date, model_type, variable_names[-1], a, \n",
    "#             ridge.score(x_train, trpgen_train), ridge.score(x_test, trpgen_test), 'ridge', zoomlevel,\n",
    "#             np.sum(ridge.coef_ != 0), len(ridge.coef_)))\n",
    "    print(\"Parameter: %.2e Train R2: %.4f \\t Test R: %.4f\" % (a, ridge.score(x_train, trpgen_train), \n",
    "                                                              ridge.score(x_test, trpgen_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Regression (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pytorch_lr(w1_list, lr_list, x1, x2, y1, y2):\n",
    "    \n",
    "    mseloss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    trainset = SurveyDataset(torch.tensor(x1,  dtype=torch.float), torch.tensor(y1, dtype=torch.float))\n",
    "    trainloader = DataLoader(trainset, batch_size=len(trainset), shuffle=True)\n",
    "\n",
    "    testset = SurveyDataset(torch.tensor(x2, dtype=torch.float), torch.tensor(y2, dtype=torch.float))\n",
    "    testloader = DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "\n",
    "    # decay rates for embedding\n",
    "#     w1_list = [0]\n",
    "    # decay rates for demo (There is no demo in this case)\n",
    "    w2_list = [0]\n",
    "    # lr_list = [0.005,0.01, 0.02]\n",
    "#     lr_list = [0.002]\n",
    "\n",
    "    dim_demo = 0\n",
    "    dim_embed = x1.shape[1]\n",
    "\n",
    "    for lr in lr_list:\n",
    "\n",
    "        for w1, w2 in itertools.product(w1_list, w2_list):\n",
    "\n",
    "            # model setup\n",
    "            model = linear_reg.LR(dim_embed=dim_embed, dim_demo=dim_demo)\n",
    "\n",
    "#             print(model)\n",
    "            embed_params = []\n",
    "            demo_params = []\n",
    "            other_params = []\n",
    "            for name, m in model.named_parameters():\n",
    "        #             print(name)\n",
    "                if 'embed' in name:\n",
    "                    embed_params.append(m)\n",
    "                elif 'demo' in name:\n",
    "                    demo_params.append(m)\n",
    "                else:\n",
    "                    other_params.append(m)\n",
    "\n",
    "#             optimizer = torch.optim.Adam([{'params':demo_params,'lr':lr}])\n",
    "            optimizer = torch.optim.Adam([{'params':embed_params,'weight_decay':w1,'lr':lr},\n",
    "                                          {'params':demo_params,'weight_decay':w2, 'lr':lr},\n",
    "                                          {'params':other_params,'weight_decay':0, 'lr':lr}])\n",
    "#             optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#             print(optimizer)\n",
    "#             print(demo_params)\n",
    "            \n",
    "            # model training\n",
    "            ref1 = 0\n",
    "            ref2 = 0\n",
    "\n",
    "            for epoch in range(1000):\n",
    "\n",
    "                mse_ = 0\n",
    "\n",
    "                for batch, (x_batch, y_batch) in enumerate(trainloader):\n",
    "                    # Compute prediction and loss\n",
    "                    pred = model(x_batch, None)\n",
    "                    pred = F.relu(pred).squeeze()\n",
    "\n",
    "                    mse = mseloss(pred, y_batch)\n",
    "                    mse_ += mse.item()\n",
    "\n",
    "                    # Backpropagation\n",
    "                    optimizer.zero_grad()\n",
    "                    mse.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                train_r = r2_score(y_batch.numpy(), pred.detach().numpy())\n",
    "                train_mse = mse_/len(trainset)\n",
    "\n",
    "                if epoch % 20 == 0:\n",
    "                    print(f\"[epoch: {epoch:>3d}] Train MSE : {train_mse:.4f} R2 score: {train_r:.3f} \")\n",
    "                loss_ = train_mse\n",
    "\n",
    "                if epoch % 5 == 0:\n",
    "                    if epoch >=40:\n",
    "                        if (np.abs(loss_ - ref1)/ref1<0.0005) & (np.abs(loss_ - ref2)/ref2<0.0005):\n",
    "                            print(\"Early stopping at epoch\", epoch)\n",
    "                            print(ref2, ref1, loss_)\n",
    "                            break\n",
    "                        if (ref1 < loss_) & (ref1 < ref2):\n",
    "                            print(\"Diverging. stop.\")\n",
    "                            break\n",
    "                        if loss_ < best:\n",
    "                            best = loss_\n",
    "                            best_epoch = epoch\n",
    "                    else:\n",
    "                        best = loss_\n",
    "                        best_epoch = epoch\n",
    "\n",
    "                    ref2 = ref1\n",
    "                    ref1 = loss_\n",
    "\n",
    "                if epoch % 20 == 0:\n",
    "\n",
    "                    mse_ = 0 \n",
    "\n",
    "                    for batch, (x_batch, y_batch) in enumerate(testloader):\n",
    "                        pred = model(x_batch, None)\n",
    "                        pred = F.relu(pred).squeeze()\n",
    "\n",
    "                        mse = mseloss(pred, y_batch)\n",
    "                        mse_ += mse.item()\n",
    "                        \n",
    "#                     print(len(testset))\n",
    "\n",
    "                    test_mse = mse_/len(testset)\n",
    "                    test_r = r2_score(y_batch.numpy(),pred.detach().numpy())\n",
    "\n",
    "                    print(f\"[epoch: {epoch:>3d}] Test MSE {test_mse:.4f} R2 score: {test_r:.3f} \")\n",
    "    return model\n",
    "\n",
    "    #         with open(out_dir+model_code+\"_regression_trpgen.csv\", \"a\") as f:\n",
    "    #             f.write(\"%s,%s,%s,%s,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f\\n\" % \\\n",
    "    #                 (model_run_date, model_type, zoomlevel, \"LR\", lr, w1, \n",
    "    #                   train_rmse, train_r, test_rmse, test_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:   0] Train MSE : 0.6495 R2 score: -10.921 \n",
      "[epoch:   0] Test MSE 0.6646 R2 score: -12.113 \n",
      "[epoch:  20] Train MSE : 0.5560 R2 score: -9.205 \n",
      "[epoch:  20] Test MSE 0.5561 R2 score: -9.973 \n",
      "[epoch:  40] Train MSE : 0.3196 R2 score: -4.867 \n",
      "[epoch:  40] Test MSE 0.3185 R2 score: -5.284 \n",
      "[epoch:  60] Train MSE : 0.1762 R2 score: -2.234 \n",
      "[epoch:  60] Test MSE 0.1752 R2 score: -2.457 \n",
      "[epoch:  80] Train MSE : 0.1045 R2 score: -0.918 \n",
      "[epoch:  80] Test MSE 0.1029 R2 score: -1.031 \n",
      "[epoch: 100] Train MSE : 0.0732 R2 score: -0.344 \n",
      "[epoch: 100] Test MSE 0.0708 R2 score: -0.396 \n",
      "[epoch: 120] Train MSE : 0.0615 R2 score: -0.128 \n",
      "[epoch: 120] Test MSE 0.0581 R2 score: -0.146 \n",
      "[epoch: 140] Train MSE : 0.0576 R2 score: -0.057 \n",
      "[epoch: 140] Test MSE 0.0536 R2 score: -0.057 \n",
      "[epoch: 160] Train MSE : 0.0564 R2 score: -0.036 \n",
      "[epoch: 160] Test MSE 0.0520 R2 score: -0.026 \n",
      "[epoch: 180] Train MSE : 0.0560 R2 score: -0.028 \n",
      "[epoch: 180] Test MSE 0.0514 R2 score: -0.013 \n",
      "[epoch: 200] Train MSE : 0.0558 R2 score: -0.023 \n",
      "[epoch: 200] Test MSE 0.0510 R2 score: -0.006 \n",
      "[epoch: 220] Train MSE : 0.0555 R2 score: -0.019 \n",
      "[epoch: 220] Test MSE 0.0507 R2 score: -0.001 \n",
      "[epoch: 240] Train MSE : 0.0553 R2 score: -0.014 \n",
      "[epoch: 240] Test MSE 0.0505 R2 score: 0.004 \n",
      "[epoch: 260] Train MSE : 0.0550 R2 score: -0.010 \n",
      "[epoch: 260] Test MSE 0.0502 R2 score: 0.009 \n",
      "[epoch: 280] Train MSE : 0.0547 R2 score: -0.005 \n",
      "[epoch: 280] Test MSE 0.0500 R2 score: 0.014 \n",
      "[epoch: 300] Train MSE : 0.0545 R2 score: 0.000 \n",
      "[epoch: 300] Test MSE 0.0497 R2 score: 0.019 \n",
      "[epoch: 320] Train MSE : 0.0542 R2 score: 0.005 \n",
      "[epoch: 320] Test MSE 0.0495 R2 score: 0.024 \n",
      "[epoch: 340] Train MSE : 0.0539 R2 score: 0.011 \n",
      "[epoch: 340] Test MSE 0.0492 R2 score: 0.029 \n",
      "[epoch: 360] Train MSE : 0.0536 R2 score: 0.016 \n",
      "[epoch: 360] Test MSE 0.0489 R2 score: 0.035 \n",
      "[epoch: 380] Train MSE : 0.0533 R2 score: 0.021 \n",
      "[epoch: 380] Test MSE 0.0486 R2 score: 0.040 \n",
      "[epoch: 400] Train MSE : 0.0530 R2 score: 0.027 \n",
      "[epoch: 400] Test MSE 0.0484 R2 score: 0.046 \n",
      "[epoch: 420] Train MSE : 0.0527 R2 score: 0.032 \n",
      "[epoch: 420] Test MSE 0.0481 R2 score: 0.051 \n",
      "[epoch: 440] Train MSE : 0.0524 R2 score: 0.038 \n",
      "[epoch: 440] Test MSE 0.0478 R2 score: 0.057 \n",
      "[epoch: 460] Train MSE : 0.0521 R2 score: 0.044 \n",
      "[epoch: 460] Test MSE 0.0475 R2 score: 0.063 \n",
      "[epoch: 480] Train MSE : 0.0518 R2 score: 0.049 \n",
      "[epoch: 480] Test MSE 0.0472 R2 score: 0.069 \n",
      "[epoch: 500] Train MSE : 0.0515 R2 score: 0.055 \n",
      "[epoch: 500] Test MSE 0.0469 R2 score: 0.074 \n",
      "[epoch: 520] Train MSE : 0.0512 R2 score: 0.061 \n",
      "[epoch: 520] Test MSE 0.0466 R2 score: 0.080 \n",
      "[epoch: 540] Train MSE : 0.0508 R2 score: 0.067 \n",
      "[epoch: 540] Test MSE 0.0463 R2 score: 0.086 \n",
      "[epoch: 560] Train MSE : 0.0505 R2 score: 0.073 \n",
      "[epoch: 560] Test MSE 0.0460 R2 score: 0.092 \n",
      "[epoch: 580] Train MSE : 0.0502 R2 score: 0.078 \n",
      "[epoch: 580] Test MSE 0.0457 R2 score: 0.098 \n",
      "[epoch: 600] Train MSE : 0.0499 R2 score: 0.084 \n",
      "[epoch: 600] Test MSE 0.0454 R2 score: 0.104 \n",
      "[epoch: 620] Train MSE : 0.0496 R2 score: 0.090 \n",
      "[epoch: 620] Test MSE 0.0451 R2 score: 0.110 \n",
      "[epoch: 640] Train MSE : 0.0493 R2 score: 0.096 \n",
      "[epoch: 640] Test MSE 0.0448 R2 score: 0.116 \n",
      "[epoch: 660] Train MSE : 0.0489 R2 score: 0.102 \n",
      "[epoch: 660] Test MSE 0.0445 R2 score: 0.122 \n",
      "[epoch: 680] Train MSE : 0.0486 R2 score: 0.108 \n",
      "[epoch: 680] Test MSE 0.0442 R2 score: 0.128 \n",
      "[epoch: 700] Train MSE : 0.0483 R2 score: 0.113 \n",
      "[epoch: 700] Test MSE 0.0439 R2 score: 0.133 \n",
      "[epoch: 720] Train MSE : 0.0480 R2 score: 0.119 \n",
      "[epoch: 720] Test MSE 0.0436 R2 score: 0.139 \n",
      "[epoch: 740] Train MSE : 0.0477 R2 score: 0.125 \n",
      "[epoch: 740] Test MSE 0.0433 R2 score: 0.145 \n",
      "[epoch: 760] Train MSE : 0.0474 R2 score: 0.131 \n",
      "[epoch: 760] Test MSE 0.0430 R2 score: 0.151 \n",
      "[epoch: 780] Train MSE : 0.0471 R2 score: 0.136 \n",
      "[epoch: 780] Test MSE 0.0427 R2 score: 0.157 \n",
      "[epoch: 800] Train MSE : 0.0467 R2 score: 0.142 \n",
      "[epoch: 800] Test MSE 0.0425 R2 score: 0.162 \n",
      "[epoch: 820] Train MSE : 0.0464 R2 score: 0.148 \n",
      "[epoch: 820] Test MSE 0.0422 R2 score: 0.168 \n",
      "[epoch: 840] Train MSE : 0.0461 R2 score: 0.153 \n",
      "[epoch: 840] Test MSE 0.0419 R2 score: 0.174 \n",
      "[epoch: 860] Train MSE : 0.0458 R2 score: 0.159 \n",
      "[epoch: 860] Test MSE 0.0416 R2 score: 0.179 \n",
      "[epoch: 880] Train MSE : 0.0455 R2 score: 0.164 \n",
      "[epoch: 880] Test MSE 0.0413 R2 score: 0.185 \n",
      "[epoch: 900] Train MSE : 0.0452 R2 score: 0.170 \n",
      "[epoch: 900] Test MSE 0.0410 R2 score: 0.190 \n",
      "[epoch: 920] Train MSE : 0.0450 R2 score: 0.175 \n",
      "[epoch: 920] Test MSE 0.0408 R2 score: 0.196 \n",
      "[epoch: 940] Train MSE : 0.0447 R2 score: 0.180 \n",
      "[epoch: 940] Test MSE 0.0405 R2 score: 0.201 \n",
      "[epoch: 960] Train MSE : 0.0444 R2 score: 0.185 \n",
      "[epoch: 960] Test MSE 0.0402 R2 score: 0.206 \n",
      "[epoch: 980] Train MSE : 0.0441 R2 score: 0.191 \n",
      "[epoch: 980] Test MSE 0.0400 R2 score: 0.212 \n"
     ]
    }
   ],
   "source": [
    "model = pytorch_lr(w1_list=[0.0003], lr_list=[0.001], x1=x_train, x2=x_test, y1=auto_train, y2=auto_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:   0] Train MSE : 0.0146 R2 score: -0.502 \n",
      "[epoch:   0] Test MSE 0.0839 R2 score: -7.997 \n",
      "[epoch:  20] Train MSE : 0.0146 R2 score: -0.502 \n",
      "[epoch:  20] Test MSE 0.0141 R2 score: -0.513 \n",
      "[epoch:  40] Train MSE : 0.0146 R2 score: -0.502 \n",
      "Early stopping at epoch 40\n",
      "0.01458367667407261 0.014583678022975463 0.014583679371878315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LR(\n",
       "  (embed): Linear(in_features=2048, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_lr(w1_list=[0.00001], lr_list=[0.5], x1=x_train, x2=x_test, y1=pt_train, y2=pt_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), out_dir+\"image_weights.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MNL for Mode Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader and model definition\n",
    "\n",
    "trainset = SurveyDataset(torch.tensor(x_train,  dtype=torch.float), torch.tensor(y_train, dtype=torch.float))\n",
    "trainloader = DataLoader(trainset, batch_size=len(trainset), shuffle=False)\n",
    "\n",
    "testset = SurveyDataset(torch.tensor(x_test, dtype=torch.float), torch.tensor(y_test, dtype=torch.float))\n",
    "testloader = DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "\n",
    "kldivloss = nn.KLDivLoss(reduction='sum')\n",
    "mseloss = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_train = np.sum(np.power(y_train - np.mean(y_train, axis=0), 2), axis=0)\n",
    "sst_test = np.sum(np.power(y_test - np.mean(y_test, axis=0), 2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mnl_torch(lr_list, wd_list):\n",
    "    \n",
    "    for (lr, wd) in itertools.product(lr_list, wd_list):\n",
    "        \n",
    "        print(f\"[lr: {lr:.4f}, wd: {wd:3.2e}]\")\n",
    "\n",
    "        # model setup\n",
    "        model = mnl.MNL(n_alts=4, n_features=x_train.shape[-1])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "#         print(optimizer)\n",
    "        # model training\n",
    "\n",
    "        ref1 = 0\n",
    "        ref2 = 0\n",
    "\n",
    "        for epoch in range(5000):\n",
    "\n",
    "            kl_ = 0\n",
    "            mse_ = 0\n",
    "            mse1_ = 0\n",
    "            mse2_ = 0\n",
    "            mse3_ = 0\n",
    "            mse4_ = 0\n",
    "\n",
    "            for batch, (x_batch, y_batch) in enumerate(trainloader):\n",
    "                \n",
    "                # Compute prediction and loss\n",
    "                util = model(x_batch)\n",
    "                probs = torch.log(nn.functional.softmax(util, dim=1))\n",
    "                kl = kldivloss(probs, y_batch)\n",
    "        #         kl = kldivloss(torch.log(util), y_batch)\n",
    "                kl_ += kl.item()\n",
    "\n",
    "                mse = mseloss(torch.exp(probs), y_batch)\n",
    "        #         mse = mseloss(util, y_batch)\n",
    "                mse_ += mse.sum().item()\n",
    "                mse1_ += mse[:,0].sum().item()\n",
    "                mse2_ += mse[:,1].sum().item()\n",
    "                mse3_ += mse[:,2].sum().item()\n",
    "                mse4_ += mse[:,3].sum().item()\n",
    "                mse = mse.sum()\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                kl.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_kl = kl_/len(trainset)\n",
    "            train_mse = np.sqrt(mse_/len(trainset))\n",
    "            train_mse1 = np.sqrt(mse1_/len(trainset))\n",
    "            train_mse2 = np.sqrt(mse2_/len(trainset))\n",
    "            train_mse3 = np.sqrt(mse3_/len(trainset))\n",
    "            train_mse4 = np.sqrt(mse4_/len(trainset))\n",
    "\n",
    "            train_r1 = 1-mse1_/sst_train[0]\n",
    "            train_r2 = 1-mse2_/sst_train[1]\n",
    "            train_r3 = 1-mse3_/sst_train[2]\n",
    "            train_r4 = 1-mse4_/sst_train[3]\n",
    "\n",
    "            loss_ = train_kl\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "\n",
    "                kl_ = 0\n",
    "                mse_ = 0 \n",
    "                mse1_ = 0\n",
    "                mse2_ = 0\n",
    "                mse3_ = 0\n",
    "                mse4_ = 0\n",
    "\n",
    "                for batch, (x_batch, y_batch) in enumerate(testloader):\n",
    "                    \n",
    "                    util = model(x_batch)\n",
    "                    probs = torch.log(nn.functional.softmax(util,dim=1))\n",
    "                    kl = kldivloss(probs, y_batch)\n",
    "            #         kl = kldivloss(torch.log(util), y_batch)\n",
    "                    kl_ += kl.item()\n",
    "\n",
    "                    mse = mseloss(torch.exp(probs), y_batch)\n",
    "            #         mse = mseloss(util, y_batch)\n",
    "                    mse_ += mse.sum().item()\n",
    "                    mse1_ += mse[:,0].sum().item()\n",
    "                    mse2_ += mse[:,1].sum().item()\n",
    "                    mse3_ += mse[:,2].sum().item()\n",
    "                    mse4_ += mse[:,3].sum().item()\n",
    "\n",
    "                test_kl = kl_/len(testset)\n",
    "                test_mse = np.sqrt(mse_/len(testset))\n",
    "                test_mse1 = np.sqrt(mse1_/len(testset))\n",
    "                test_mse2 = np.sqrt(mse2_/len(testset))\n",
    "                test_mse3 = np.sqrt(mse3_/len(testset))\n",
    "                test_mse4 = np.sqrt(mse4_/len(testset))\n",
    "\n",
    "                r1 = r2_score(y_batch.numpy()[:,0],torch.exp(probs).detach().numpy()[:,0])\n",
    "                r2 = r2_score(y_batch.numpy()[:,1],torch.exp(probs).detach().numpy()[:,1])\n",
    "                r3 = r2_score(y_batch.numpy()[:,2],torch.exp(probs).detach().numpy()[:,2])\n",
    "                r4 = r2_score(y_batch.numpy()[:,3],torch.exp(probs).detach().numpy()[:,3])\n",
    "\n",
    "                if epoch >= 40:\n",
    "#                     if (np.abs(loss_ - ref1)/ref1<0.001) & (np.abs(loss_ - ref2)/ref2<0.001):\n",
    "#                         print(\"Early stopping at epoch\", epoch)\n",
    "#                         break\n",
    "#                     if (ref1 < loss_) & (ref1 < ref2):\n",
    "#                         print(\"Diverging. stop.\")\n",
    "#                         break\n",
    "                    if loss_ < best:\n",
    "                        best = loss_\n",
    "                        best_epoch = epoch\n",
    "                        output = (train_kl, train_mse, train_mse1, train_mse2, train_mse3, train_mse4,\n",
    "                                  test_kl, test_mse, test_mse1, test_mse2, test_mse3, test_mse4,\n",
    "                                  train_r1, train_r2, train_r3, train_r4, r1, r2, r3, r4)\n",
    "                else:\n",
    "                    best = loss_\n",
    "                    best_epoch = epoch\n",
    "                    output = (train_kl, train_mse, train_mse1, train_mse2, train_mse3, train_mse4,\n",
    "                                  test_kl, test_mse, test_mse1, test_mse2, test_mse3, test_mse4,\n",
    "                                  train_r1, train_r2, train_r3, train_r4, r1, r2, r3, r4)\n",
    "                ref2 = ref1\n",
    "                ref1 = loss_\n",
    "\n",
    "#             if epoch % 20 == 0:\n",
    "\n",
    "#                 print(f\"[epoch: {epoch:>3d}] Train KL loss: {train_kl:.3f} RMSE {train_mse:.3f}\")\n",
    "#                    # {train_mse1:.3f} {train_mse2:.3f} {train_mse3:.3f} {train_mse4:.3f}\")\n",
    "#                 print(f\"\\t\\t\\t\\t\\t\\t Train R2 score: {train_r1:.3f} {train_r2:.3f} {train_r3:.3f} {train_r4:.3f} \")\n",
    "#                 print(f\"[epoch: {epoch:>3d}] Test KL loss: {kl_/len(testset):.3f} RMSE {np.sqrt(mse_/len(testset)):.3f}\")\n",
    "#                    #     {np.sqrt(mse1_/len(testset)):.3f} {np.sqrt(mse2_/len(testset)):.3f} {np.sqrt(mse3_/len(testset)):.3f} {np.sqrt(mse4_/len(testset)):.3f}\")\n",
    "#                 print(f\"\\t\\t\\t\\t\\t\\t Test R2 score: {r1:.3f} {r2:.3f} {r3:.3f} {r4:.3f} \")\n",
    "\n",
    "#                 print(f\"[epoch: {epoch:>3d}] Train KL loss: {train_kl:.3f} Train R2 score: {train_r1:.3f} {train_r2:.3f} {train_r3:.3f} {train_r4:.3f} \")\n",
    "#                 print(f\"[epoch: {epoch:>3d}] Test KL loss: {kl_/len(testset):.3f} Test R2 score: {r1:.3f} {r2:.3f} {r3:.3f} {r4:.3f} \")\n",
    "\n",
    "#         with open(out_dir+\"BA_mode_choice.csv\", \"a\") as f:\n",
    "#             f.write(\"%s,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f,%.4f\\n\" % \n",
    "#                     ((\"MNL\",lr, wd)+output))\n",
    "\n",
    "        print(f\"[epoch: {best_epoch:>3d}] Train KL loss: {output[0]:.3f} Train R2 score: {output[12]:.3f} {output[13]:.3f} {output[14]:.3f} {output[15]:.3f} \")\n",
    "        print(f\"[epoch: {best_epoch:>3d}] Test KL loss: {output[6]:.3f} Test R2 score: {output[16]:.3f} {output[17]:.3f} {output[18]:.3f} {output[19]:.3f} \")\n",
    "        print()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lr: 0.0005, wd: 0.00e+00]\n",
      "[epoch: 4985] Train KL loss: 0.095 Train R2 score: 0.712 0.770 0.238 0.716 \n",
      "[epoch: 4985] Test KL loss: 0.110 Test R2 score: 0.572 0.680 -0.267 0.458 \n",
      "\n",
      "[lr: 0.0005, wd: 1.00e-07]\n",
      "[epoch: 4995] Train KL loss: 0.096 Train R2 score: 0.698 0.763 0.271 0.707 \n",
      "[epoch: 4995] Test KL loss: 0.110 Test R2 score: 0.575 0.682 -0.271 0.462 \n",
      "\n",
      "[lr: 0.0005, wd: 1.00e-06]\n",
      "[epoch: 4995] Train KL loss: 0.096 Train R2 score: 0.706 0.765 0.232 0.706 \n",
      "[epoch: 4995] Test KL loss: 0.110 Test R2 score: 0.573 0.685 -0.263 0.465 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = mnl_torch(lr_list=[0.0005], wd_list=[0, 1e-7, 1e-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnl_torch(lr_list=[0.005], wd_list=[1e-5, 1e-4, 1e-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnl_torch(lr_list=[0.005], wd_list=[1e-2, 1e-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnl_torch(lr_list=[0.005], wd_list=[1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnl_torch(lr_list=[0.005], wd_list=[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lr: 0.0005, wd: 2.00e+00]\n",
      "[epoch: 4995] Train KL loss: 0.093 Train R2 score: 0.722 0.774 0.195 0.683 \n",
      "[epoch: 4995] Test KL loss: 0.098 Test R2 score: 0.601 0.729 -0.263 0.550 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = mnl_torch(lr_list=[0.0005], wd_list=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), out_dir+\"ac4_sae_22021402.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Substitution Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {0: \"Active\", 1:\"Auto\", 3:\"Public Transit\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mnl.MNL(n_alts=4, n_features=x_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = torch.load(out_dir+\"ac4_sae_220502.pt\")\n",
    "model.load_state_dict(saved);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNL(\n",
       "  (beta): Linear(in_features=2048, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_ct = \"17_31_842100\"\n",
    "# target_ct = \"17_31_839100\" # 17_31_802300\n",
    "target_ct = \"17_31_839100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_train = np.array(unique_ct)[~train_test_index].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17_43_846515'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_train[1192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "591"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_train.index(target_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = np.array(unique_ct)[~train_test_index].tolist().index(select_ct)\n",
    "target = np.array(unique_ct)[~train_test_index].tolist().index(target_ct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "591"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06543985, 0.86598945, 0.01792632, 0.05064437])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[591]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87705916, 0.2264165 , 0.48505932, ..., 0.3346961 , 0.7416978 ,\n",
       "        0.653571  ],\n",
       "       [0.6863482 , 0.33222264, 0.4920233 , ..., 0.26621485, 0.7509318 ,\n",
       "        0.4335001 ],\n",
       "       [0.8488263 , 0.24462366, 0.45612377, ..., 0.3412737 , 0.7206503 ,\n",
       "        0.5076797 ],\n",
       "       ...,\n",
       "       [0.7737112 , 0.6777059 , 0.4930932 , ..., 0.39824638, 0.37632185,\n",
       "        0.40257534],\n",
       "       [0.79223454, 0.82337284, 0.5619807 , ..., 0.39771435, 0.42390347,\n",
       "        0.37690142],\n",
       "       [0.7629267 , 0.32102448, 0.5110604 , ..., 0.29693416, 0.6177071 ,\n",
       "        0.6276804 ]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "util = model(torch.tensor(x_train))\n",
    "util = torch.log(F.softmax(util))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0932, dtype=torch.float64, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kldivloss(util,torch.tensor(y_train))/len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in np.linspace(0, 1, 6):\n",
    "    temp = x_train[select].copy() * (1-alpha) + x_train[target].copy() * (alpha)\n",
    "\n",
    "    if alpha == 0:\n",
    "        test_in = x_train[select].copy().reshape(1, -1)\n",
    "    else:\n",
    "        test_in = np.concatenate([test_in, temp.reshape(1,-1)], axis=0)\n",
    "\n",
    "test_in = torch.tensor(test_in, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/jtl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "util = model(torch.tensor(test_in))\n",
    "probs = F.softmax(util).detach().numpy()\n",
    "welfare = np.log(np.sum(np.exp(util.detach().numpy()), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.6346313, 1.5712262, 1.537279 , 1.5320381, 1.5534903, 1.5987531],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "welfare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.6346313, 1.7078968, 1.8118355, 1.942305 , 2.094722 , 2.2646515],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "welfare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26730236, 0.5195505 , 0.04384274, 0.16930443],\n",
       "       [0.22736342, 0.60420567, 0.0430031 , 0.12542772],\n",
       "       [0.18755025, 0.68142915, 0.04090541, 0.0901152 ],\n",
       "       [0.15065801, 0.7484014 , 0.03789131, 0.06304928],\n",
       "       [0.11839546, 0.8041122 , 0.03433735, 0.04315493],\n",
       "       [0.0914266 , 0.8489716 , 0.03057653, 0.02902521]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(3,3))\n",
    "for i in [0,1,3]:\n",
    "    ax.plot(np.linspace(0,1.01,test_in.shape[0]), probs[:,i], label=label[i])\n",
    "# ax2 = ax.twinx()\n",
    "# ax2.plot(np.linspace(0,1.01,test_in.shape[0]), welfare, '--', c='k', label=\"welfare\")\n",
    "\n",
    "#     print(probs[:,i])\n",
    "ax.set_ylabel(\"Probability of Alternatives\")\n",
    "ax.set_xlabel(\"% Transition\")\n",
    "ax.legend()\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_ylim([0,1])\n",
    "ax.grid()\n",
    "plt.show()\n",
    "# fig.savefig(out_dir+\"/substitution_curves/\"+select_ct+\"_\"+target_ct+\".png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26258385, 0.46862403, 0.41123223, ..., 0.88273513, 0.66203994,\n",
       "        0.7422787 ],\n",
       "       [0.3279127 , 0.3692272 , 0.43320718, ..., 0.88564366, 0.71464527,\n",
       "        0.7797907 ]], dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[[2,3],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_train.index(\"17_31_81600\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = model(torch.tensor(x_train[[345,298,789,766,780],:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = model(torch.tensor(x_train[[591,345,298,789,766,780],:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1097, 0.6140, 0.1345, 0.1417],\n",
       "        [0.2185, 0.5266, 0.1155, 0.1394],\n",
       "        [0.1302, 0.6635, 0.0859, 0.1204],\n",
       "        [0.1810, 0.5821, 0.0995, 0.1373],\n",
       "        [0.4175, 0.3875, 0.0949, 0.1001],\n",
       "        [0.2329, 0.5157, 0.0960, 0.1554]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0336,  0.4847, -1.6340, -0.0597],\n",
       "        [ 0.6245,  0.5732, -1.4503,  0.3164],\n",
       "        [ 0.3936,  0.7518, -1.6667,  0.2015],\n",
       "        [ 0.7723,  0.6468, -1.7494,  0.0114],\n",
       "        [ 0.8743,  0.3601, -1.4041, -0.0361]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = util.detach().numpy()\n",
    "util = np.log(np.sum(np.exp(util), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.7615857, 2.1936176], dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.7177628, 1.6578373, 1.6124746, 1.6600351, 1.6175466],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
