{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"models/\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import cross_validate, GroupKFold\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "from dataloader import SurveyDataset, load_aggregate_travel_behavior, load_demo_v1\n",
    "from M1_util_train_test import load_model, test\n",
    "import linear_reg\n",
    "import mnl\n",
    "from setup import out_dir, data_dir, image_dir, model_dir, proj_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_version = '1571'\n",
    "\n",
    "model_type = 'AE'\n",
    "sampling = 's'\n",
    "\n",
    "zoomlevel = 'zoom15'\n",
    "\n",
    "variable_names = ['active','auto','mas','pt', 'trpgen']\n",
    "\n",
    "demo_variables = ['tot_population','pct25_34yrs','pct35_50yrs','pctover65yrs',\n",
    "         'pctwhite_alone','pct_nonwhite','pctblack_alone',\n",
    "         'pct_col_grad','avg_tt_to_work','inc_per_capita']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Load Model Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proj_dir+\"latent_space/2023-07-04T17-20-36_sae_kl_f32_eplast.pkl\", \"rb\") as f:\n",
    "    encoder_output = pkl.load(f)\n",
    "    im = pkl.load(f)\n",
    "    ct = pkl.load(f)\n",
    "    sup_true_list = pkl.load(f)\n",
    "    sup_list = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Embeddings\n",
    "unique_ct = list(set(ct))\n",
    "unique_ct.sort()\n",
    "ct = np.array(ct)\n",
    "aggregate_embeddings = []\n",
    "for i in unique_ct:\n",
    "#     aggregate_embeddings.append(np.mean(sup_list[ct == i], axis=0))\n",
    "    aggregate_embeddings.append(np.mean(encoder_output[ct == i], axis=0))\n",
    "aggregate_embeddings = np.array(aggregate_embeddings)\n",
    "\n",
    "x = aggregate_embeddings\n",
    "\n",
    "x = x.reshape(x.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1588, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Trip Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"MyDailyTravel/origin_trip_behavior.csv\"\n",
    "df_pivot = load_aggregate_travel_behavior(file, data_version)\n",
    "y_ct = df_pivot['geoid'].to_list()\n",
    "y = df_pivot[variable_names].to_numpy()[:,:4]\n",
    "\n",
    "groups = df_pivot['train_test']\n",
    "group_split = GroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(x) != len(y):\n",
    "    x_mask = [True if c in y_ct else False for c in unique_ct]\n",
    "    x = x[x_mask,:]\n",
    "    unique_ct = list(np.array(unique_ct)[x_mask])\n",
    "    y_mask = [True if c in unique_ct else False for c in y_ct]\n",
    "    y = y[y_mask,:]\n",
    "    y_ct = list(np.array(y_ct)[y_mask])\n",
    "\n",
    "x = x[[y_ct.index(val) for val in unique_ct],:]\n",
    "unique_ct = list(np.array(unique_ct)[np.array([y_ct.index(val) for val in unique_ct])])\n",
    "for xc,yc in zip(unique_ct, y_ct):\n",
    "    assert xc == yc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1571\n"
     ]
    }
   ],
   "source": [
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Auto Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 \t Train score: 0.5790 \t Cross val score: 0.5664 \t Nonzero coef: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.430e+01, tolerance: 6.768e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.388e+01, tolerance: 6.596e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.419e+01, tolerance: 6.769e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.471e+01, tolerance: 7.111e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.448e+01, tolerance: 6.760e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "for a in (1e-3)*np.array([0]):#3,4,5,6,7,8,9,10,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    cross_results = cross_validate(lasso, x, y[:,y_index], cv=group_split, groups=groups, scoring='r2', return_train_score=True, return_estimator=True)\n",
    "    nz = 0\n",
    "    for m in cross_results['estimator']:\n",
    "        nz += sum(m.coef_ != 0)\n",
    "    nz /= 5\n",
    "    \n",
    "    print(\"Parameter: %.2e \\t Train score: %.4f \\t Cross val score: %.4f \\t Nonzero coef: %d\" % \n",
    "          (a, cross_results['train_score'].mean(), cross_results['test_score'].mean(), nz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 \t Train score: 0.4621 \t Cross val score: 0.4427 \t Nonzero coef: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.148e+00, tolerance: 1.168e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.007e+00, tolerance: 1.130e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.224e+00, tolerance: 1.210e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.481e+00, tolerance: 1.286e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.484e+00, tolerance: 1.281e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "for a in (1e-3)*np.array([0]):#3,4,5,6,7,8,9]):\n",
    "# for a in (1e-4)*np.array([10,15,20,25,30,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    cross_results = cross_validate(lasso, x, y[:,y_index], cv=group_split, groups=groups, scoring='r2', return_train_score=True, return_estimator=True)\n",
    "    nz = 0\n",
    "    for m in cross_results['estimator']:\n",
    "        nz += sum(m.coef_ != 0)\n",
    "    nz /= 5\n",
    "    \n",
    "    print(\"Parameter: %.2e \\t Train score: %.4f \\t Cross val score: %.4f \\t Nonzero coef: %d\" % \n",
    "          (a, cross_results['train_score'].mean(), cross_results['test_score'].mean(), nz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.00e+00 \t Train score: 0.4721 \t Cross val score: 0.4568 \t Nonzero coef: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.933e+00, tolerance: 3.383e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.393e+00, tolerance: 3.240e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.817e+00, tolerance: 3.355e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.989e+00, tolerance: 3.441e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/jtl/anaconda3/envs/qingyi/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.972e+00, tolerance: 3.289e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "for a in (1e-3)*np.array([0]):#3,4,5,6,7,8,9,10,20,50]):\n",
    "    lasso = linear_model.Lasso(alpha=a)\n",
    "    cross_results = cross_validate(lasso, x, y[:,y_index], cv=group_split, groups=groups, scoring='r2', return_train_score=True, return_estimator=True)\n",
    "    nz = 0\n",
    "    for m in cross_results['estimator']:\n",
    "        nz += sum(m.coef_ != 0)\n",
    "    nz /= 5\n",
    "    \n",
    "    print(\"Parameter: %.2e \\t Train score: %.4f \\t Cross val score: %.4f \\t Nonzero coef: %d\" % \n",
    "          (a, cross_results['train_score'].mean(), cross_results['test_score'].mean(), nz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MNL for Mode Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lr: 5.00e-03, wd: 1.00e-04]\n",
      "Early stopping at epoch 470\n",
      "[epoch: 465] Train KL loss: 0.139 Train R2 score: 0.477 0.590 -0.057 0.429 \n",
      "[epoch: 465] Test KL loss: 0.145 Test R2 score: 0.474 0.581 -0.038 0.437 \n",
      "\n",
      "[lr: 5.00e-03, wd: 1.00e-05]\n",
      "[epoch: 995] Train KL loss: 0.145 Train R2 score: 0.469 0.558 -0.019 0.312 \n",
      "[epoch: 995] Test KL loss: 0.150 Test R2 score: 0.443 0.555 -0.007 0.359 \n",
      "\n",
      "[lr: 5.00e-03, wd: 0.00e+00]\n",
      "Early stopping at epoch 855\n",
      "[epoch: 850] Train KL loss: 0.145 Train R2 score: 0.447 0.560 -0.156 0.452 \n",
      "[epoch: 850] Test KL loss: 0.151 Test R2 score: 0.419 0.541 -0.077 0.483 \n",
      "\n",
      "[lr: 5.00e-03, wd: 1.00e-04]\n",
      "Early stopping at epoch 615\n",
      "[epoch: 610] Train KL loss: 0.138 Train R2 score: 0.482 0.592 -0.073 0.500 \n",
      "[epoch: 610] Test KL loss: 0.152 Test R2 score: 0.428 0.577 -0.063 0.423 \n",
      "\n",
      "[lr: 5.00e-03, wd: 1.00e-05]\n",
      "Early stopping at epoch 725\n",
      "[epoch: 720] Train KL loss: 0.141 Train R2 score: 0.485 0.581 -0.164 0.487 \n",
      "[epoch: 720] Test KL loss: 0.156 Test R2 score: 0.429 0.561 -0.158 0.424 \n",
      "\n",
      "[lr: 5.00e-03, wd: 0.00e+00]\n",
      "Early stopping at epoch 665\n",
      "[epoch: 660] Train KL loss: 0.141 Train R2 score: 0.487 0.574 -0.023 0.331 \n",
      "[epoch: 660] Test KL loss: 0.154 Test R2 score: 0.438 0.557 -0.041 0.298 \n",
      "\n",
      "[lr: 5.00e-03, wd: 1.00e-04]\n",
      "Early stopping at epoch 645\n",
      "[epoch: 640] Train KL loss: 0.141 Train R2 score: 0.473 0.591 -0.022 0.476 \n",
      "[epoch: 640] Test KL loss: 0.135 Test R2 score: 0.430 0.540 -0.083 0.485 \n",
      "\n",
      "[lr: 5.00e-03, wd: 1.00e-05]\n",
      "Early stopping at epoch 440\n",
      "[epoch: 435] Train KL loss: 0.141 Train R2 score: 0.477 0.601 -0.047 0.462 \n",
      "[epoch: 435] Test KL loss: 0.134 Test R2 score: 0.448 0.560 -0.064 0.469 \n",
      "\n",
      "[lr: 5.00e-03, wd: 0.00e+00]\n",
      "Early stopping at epoch 600\n",
      "[epoch: 595] Train KL loss: 0.141 Train R2 score: 0.487 0.596 -0.069 0.469 \n",
      "[epoch: 595] Test KL loss: 0.137 Test R2 score: 0.446 0.542 -0.135 0.485 \n",
      "\n",
      "[lr: 5.00e-03, wd: 1.00e-04]\n",
      "Diverging. stop.\n",
      "[epoch:  45] Train KL loss: 0.253 Train R2 score: 0.099 0.068 -0.929 0.176 \n",
      "[epoch:  45] Test KL loss: 0.238 Test R2 score: 0.094 0.106 -1.268 0.180 \n",
      "\n",
      "[lr: 5.00e-03, wd: 1.00e-05]\n",
      "Early stopping at epoch 495\n",
      "[epoch: 490] Train KL loss: 0.142 Train R2 score: 0.460 0.587 -0.050 0.443 \n",
      "[epoch: 490] Test KL loss: 0.133 Test R2 score: 0.527 0.617 -0.081 0.457 \n",
      "\n",
      "[lr: 5.00e-03, wd: 0.00e+00]\n",
      "Early stopping at epoch 715\n",
      "[epoch: 710] Train KL loss: 0.144 Train R2 score: 0.451 0.561 -0.072 0.447 \n",
      "[epoch: 710] Test KL loss: 0.134 Test R2 score: 0.513 0.613 -0.078 0.441 \n",
      "\n",
      "[lr: 5.00e-03, wd: 1.00e-04]\n",
      "Diverging. stop.\n",
      "[epoch:  35] Train KL loss: 0.344 Train R2 score: 0.077 -0.226 -11.441 0.168 \n",
      "[epoch:  35] Test KL loss: 0.360 Test R2 score: -0.023 -0.330 -11.178 0.085 \n",
      "\n",
      "[lr: 5.00e-03, wd: 1.00e-05]\n",
      "Diverging. stop.\n",
      "[epoch:  40] Train KL loss: 0.185 Train R2 score: 0.178 0.405 -0.298 0.380 \n",
      "[epoch:  40] Test KL loss: 0.196 Test R2 score: 0.068 0.348 -0.286 0.411 \n",
      "\n",
      "[lr: 5.00e-03, wd: 0.00e+00]\n",
      "Early stopping at epoch 550\n",
      "[epoch: 545] Train KL loss: 0.137 Train R2 score: 0.485 0.596 -0.046 0.482 \n",
      "[epoch: 545] Test KL loss: 0.145 Test R2 score: 0.446 0.586 -0.081 0.426 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from util_aggregate_models import mnl_torch\n",
    "\n",
    "# dataloader and model definition\n",
    "\n",
    "lr_list = [5e-3]\n",
    "wd_list = [1e-4, 1e-5,0]\n",
    "\n",
    "results = {}\n",
    "for i in range(5):\n",
    "    \n",
    "    train_filter = groups != i\n",
    "    test_filter = groups == i\n",
    "    x_train = x[train_filter]\n",
    "    y_train = y[train_filter]\n",
    "    x_test = x[test_filter]\n",
    "    y_test = y[test_filter]\n",
    "    sst_train = np.sum(np.power(y_train - np.mean(y_train, axis=0), 2), axis=0)\n",
    "    sst_test = np.sum(np.power(y_test - np.mean(y_test, axis=0), 2), axis=0)\n",
    "    \n",
    "    trainset = SurveyDataset(torch.tensor(x_train,  dtype=torch.float), torch.tensor(y_train, dtype=torch.float))\n",
    "    trainloader = DataLoader(trainset, batch_size=len(trainset), shuffle=False)\n",
    "\n",
    "    testset = SurveyDataset(torch.tensor(x_test, dtype=torch.float), torch.tensor(y_test, dtype=torch.float))\n",
    "    testloader = DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "\n",
    "    ret_dict = mnl_torch(trainloader, testloader, x_train.shape[-1], sst_train, sst_test, lr_list=lr_list, wd_list=wd_list)\n",
    "    \n",
    "    results[i] = ret_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>train_kl_loss</th>\n",
       "      <th>test_kl_loss</th>\n",
       "      <th>train_r2_auto</th>\n",
       "      <th>train_r2_active</th>\n",
       "      <th>train_r2_pt</th>\n",
       "      <th>test_r2_auto</th>\n",
       "      <th>test_r2_active</th>\n",
       "      <th>test_r2_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.5773</td>\n",
       "      <td>0.4713</td>\n",
       "      <td>0.4361</td>\n",
       "      <td>0.5678</td>\n",
       "      <td>0.4527</td>\n",
       "      <td>0.4265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1508</td>\n",
       "      <td>0.1537</td>\n",
       "      <td>0.5464</td>\n",
       "      <td>0.4140</td>\n",
       "      <td>0.4168</td>\n",
       "      <td>0.5280</td>\n",
       "      <td>0.3830</td>\n",
       "      <td>0.4239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2029</td>\n",
       "      <td>0.2059</td>\n",
       "      <td>0.3231</td>\n",
       "      <td>0.3216</td>\n",
       "      <td>0.3498</td>\n",
       "      <td>0.2949</td>\n",
       "      <td>0.2806</td>\n",
       "      <td>0.3222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate  weight_decay  train_kl_loss  test_kl_loss  train_r2_auto  \\\n",
       "2          0.005        0.0000         0.1417        0.1442         0.5773   \n",
       "1          0.005        0.0000         0.1508        0.1537         0.5464   \n",
       "0          0.005        0.0001         0.2029        0.2059         0.3231   \n",
       "\n",
       "   train_r2_active  train_r2_pt  test_r2_auto  test_r2_active  test_r2_pt  \n",
       "2           0.4713       0.4361        0.5678          0.4527      0.4265  \n",
       "1           0.4140       0.4168        0.5280          0.3830      0.4239  \n",
       "0           0.3216       0.3498        0.2949          0.2806      0.3222  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = []\n",
    "\n",
    "for (lr, wd) in itertools.product(lr_list, wd_list):\n",
    "\n",
    "    new = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        new2 = []\n",
    "        for k,v in results[i][(lr,wd)].items():\n",
    "            new2.append(results[i][(lr,wd)][k]) \n",
    "        new.append(new2)\n",
    "        \n",
    "    new = np.array(new) \n",
    "    \n",
    "    df.append([lr] + [wd] + list(np.mean(new, axis=0)))\n",
    "\n",
    "pd.DataFrame(np.array(df), columns = ['learning_rate','weight_decay','train_kl_loss','test_kl_loss','train_r2_auto','train_r2_active','train_r2_pt',\n",
    "                                     'test_r2_auto','test_r2_active','test_r2_pt']).sort_values(by='test_kl_loss').round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qingyi",
   "language": "python",
   "name": "qingyi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
